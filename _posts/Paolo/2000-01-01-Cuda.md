---
title: CUDA, Calcolo Parallelo, UNIMIB 
categories: italiano
tags: [Paolo, Cuda, Calcolo Parallelo]
maths: 1
comments_id: 16
date: 2021-07-15T00:00:00.000Z
thumbnail: /images/posts/cuda.png
bigimg:
---

* **[CUDA a.a. 2020-2021:]({{ site.baseurl }}/assets/cuda_61.pdf)**
Anche queste sono diapositive "estese". Il numero totale di diapositive e' meno di 90, pero' il numero totale di pagine del documento pdf e' circa 380. Faccio
apparire un punto per volta nelle liste, e faccio apparire un'immagine per volta quando voglio fare piccole animazioni.  Ho migliorato l'impaginazione rispetto
 all'anno scorso e  ho aggiunto un po' di disegni (dato che ho trovato un modo migliore per impostare i passaggi da una slide all'altra senza dover salvare una per una tutte le diapositive, 
ma inserendo un unico pdf! eureka). CUDA sta diventando sempre piu' user friendly e quindi una buona parte delle raccomandazioni che erano 
necessarie per le prime schede grafiche stanno diventando obsolete. 

* **[CUDA a.a. 2019-2020:]({{ site.baseurl }}/assets/cuda_52.pdf)**
queste diapositive sono estese, ovvero contengono tutte le immagini e le "animazioni" (per esempio i punti di una lista possono comparire uno dopo l'altro).
Rispetto all'anno precedente, sono state fatte varie modifiche, sia di tipo strutturale (alcuni argomenti sono stati modificati e spostati) e sono state
aggiunte nuove immagini e "animazioni".

* **[CUDA a.a. 2018-2019:]({{ site.baseurl }}/assets/cuda_35.pdf)** queste diapositive sono in formato "handout", ovvero le
 animazioni del singolo frame sono condensate nell'ultima immagine del frame, e le liste appaiono tutte insieme (non una per volta).
D'altra parte la quantita' di memoria richiesta e' minore, e sono piu' facilmente studiabili.


I link qui sopra si riferiscono alle note che ho scritto su **CUDA**. Sono state usate durante il corso di "Sistemi di Calcolo Parallelo", presso il
Dipartimento di Informatica, Sistemisitca e Comunicazione dell'Universita' degli Studi di Milano-Bicocca.
In alcuni casi possono esserci dei richiami ad altre parti del corso, in particolare
a  **[MPI]({{ site.baseurl }}{% link _posts/Paolo/2000-01-01-MPI.md %})**
 e **[OpenMP]({{ site.baseurl }}{% link _posts/Paolo/2000-01-01-OpenMP.md %})**.




`A chi si rivolge CUDA?`

*  a chi ha una scheda grafica NVIDIA! 

*  ... e vuole velocizzare dei calcoli che richiedono molte volte la stessa operazione ma su dati differenti.

In una GPU infatti, ci sono centinaia (migliaia) di Aritmetic Logic Units (ALU) che possono eseguire operazioni in modo concorrente,
 ma ci sono poche unita' di controllo che gestiscono le ALU. Per questo motivo le 
ALU devono lavorare in **lockstep** ovvero un gruppo (warp) di ALU deve eseguire la stessa operazione ... su dati differenti.

Questo approccio e' una specie di versione piu' complessa rispetto a  **[OpenMP]({{ site.baseurl }}{% link _posts/Paolo/2000-01-01-OpenMP.md %})**
 dove molti thread possono accedere ad aree di memoria condivise.

Il punto chiave da comprendere quando si vogliono velocizzare 
dei calcoli tramite CUDA e' spesso legato alla memoria (o meglio alle memorie).
Avendo a disposizione molte unita' di processazione, il problema puo' diventare quello di portare i dati 
dalla memoria al chip. Se eseguire un calcolo puo' richidere un solo ciclo di clock, 
la **latenza** necessaria per portare il dato sulla ALU ne puo' richiedere centinaia!

Le GPU sono progettate per "nascondere" la latenza con un *trucco*:
mentre un warp aspetta la memoria, un altro warp puo' essere attivato a costo zero ed eseguire operazioni. 
Nelle normali CPU, questo passaggio (**context switch**), puo' essere molto costoso e come risultato
una CPU puo' passare molto tempo in stato di *idle*. Con una GPU e' invece possibile ottenere 
delle occupancy molto alte.

Dal punto di vista del programmatore, invece, 
per ottenere un codice che sfrutti al meglio le caratteristiche di CUDA 
e' necessario conoscere bene come si relazionano le strutture fisiche (CUDA core,
Streaming Multiprocessor,...) e le strutture logiche (thread, warp, blocco e griglia).

E' altrsi' necessario sfruttare i vari tipi di memoria: registri, shared memory, cache, 
Constant, Texture e Global.

Una buona applicazione di CUDA puo' velocizzare notevolmente un calcolo, ma ha una
curva di apprendimento piu' ripida rispetto ad OpenMP (e probabilmente anche di MPI).









