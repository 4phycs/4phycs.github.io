---
title: "Pytorch 6 Input Output"
published: true
categories: italiano
tags:
  - Python
  - DeepLearning
  - Neural Nets
  - Cuda
maths: 1
comments_id: 23
toc: 1
related_ok: 1
---

{% include toc.html %}

Indice Globale degli argomenti tra i vari post:
{% include TOC_globale_pytorch.md %}


# I/O Saving and Loading Models
**[Video-lezione](https://www.youtube.com/watch?v=9L9jEOwRrCg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=17&ab_channel=PythonEngineer)** 
di Python Engineer sul soggetto.

Un modello viene salvato come un dictionary.

i 3 metodi da ricordare:
1. `torch.save(arg, PATH)` posso salvare tensor, model o dictiionary  (e posso usare Pickle)
2. `torch.load(PATH)`
3. `model.load_state_dict(arg)`
 
Ci sono 2 modi per salvare un modello: il modo `lazy` e modo `raccomandato` 

1. lazy: usando `torch.save(arg, PATH)` e poi carico il modello con `model =torch.load(PATH)`
A questo punto si usa:<br>
`model.eval()` in questo modo si entra in modalita' evaluation (da controllare)

Il difetto del metodo lazy e' che i dati "serializzati" (intende compressi con pickle) seguono esattamente la classe e la struttura di quando sono salvati.

2. modo raccomandato: basta salvare **SOLO** i parametri del modello stesso: <br>
`torch.save(model.state_dict(), PATH)` 

A questo punto devo creare un nuovo modello, e poi importare i parametri: <br>
`model= Model(*args, **kwargs)`<br>
`model.load_state_dict(torch.load(PATH))` <br>
`model.eval()`<br>    
    

- i modelli vengono salvati in file che hanno come estensione `pth`

Salviamo anche i checkpoint:<br>
un check point e' un dizionario di dizionari!
- i parametri del modello sono un dizionario
- i parametri dell'optimizer sono un dizionario
 -....
 
 Quindi quello che facciamo e' un dizionario in cui la prima key e' "model" e associamo
 il dizionario del modello, poi la chiave "optimizer" e il dizionario dell'ottimizzatore.
 Alla fine quando abbiamo bisogno usiamo checkpoint["model"] e lui mi restituisce il dizionario
 con i parametri del modello (o se ho scritto optimizer, quelli dell'ottimizzatore).







```python
######## LAZY method #################

import torch
import torch.nn as nn

class Model(nn.Module):  # eredita un oggetto nn.Module
    def __init__(self , n_input_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(n_input_features, 1)

    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred
        
model =  Model(n_input_features=6)


######## SALVO IL MODELLO ############
FILE = "model.pth"       # solitamente i modelli hanno come estensione pth ?
torch.save(model, FILE)     

######## CARICO IL MODELLO ###########
model = torch.load(FILE)
model.eval()

for param in model.parameters():
    print(param)
```

    Parameter containing:
    tensor([[-0.3695,  0.2621,  0.0619, -0.2925, -0.3088, -0.2284]],
           requires_grad=True)
    Parameter containing:
    tensor([-0.2680], requires_grad=True)



```python
########## Metodo raccomandato ###########

import torch
import torch.nn as nn

class Model(nn.Module):  # eredita un oggetto nn.Module
    def __init__(self , n_input_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(n_input_features, 1)

    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred
        
model =  Model(n_input_features=6)
#for param in model.parameters():
#    print('Prima',param)

######## SALVO IL MODELLO ############
FILE = "model-raccomandato.pth"       # solitamente i modelli hanno come estensione pth ?
torch.save(model.state_dict(), FILE)  # salvo solo i parametri del modello   

######## Prima devo definire un modello #####
loaded_model = Model(n_input_features=6)        # creo un modello con la stessa struttura
loaded_model.load_state_dict(torch.load(FILE))  # carico i parametri
loaded_model.eval()                             # setto il modello in eval mode.


#for param in loaded_model.parameters():
#    print('Dopo', param)
```




    Model(
      (linear): Linear(in_features=6, out_features=1, bias=True)
    )




```python
############# Check point  ####################

import torch
import torch.nn as nn

class Model(nn.Module):  # eredita un oggetto nn.Module
    def __init__(self , n_input_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(n_input_features, 1)

    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred
        
model =  Model(n_input_features=6)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)

print(optimizer.state_dict())

####### CHECK point ######

checkpoint  = {         # e' un dizionario
    "epoch":90,         # per esempio siamo alla epoca 90
    "model_state": model.state_dict(),
    "optim_state": optimizer.state_dict()
}

torch.save(checkpoint, "checkpoint.pth")

#  a questo punto posso caricare:

loaded_checkpoint  =torch.load("checkpoint.pth")
epoch  =loaded_checkpoint["epoch"] 
model = Model(n_input_features=6)

optimizer = torch.optim.SGD(model.parameters(), lr=0) # mettiamo 0 e poi carichiamo la corretta lr
model.load_state_dict(checkpoint["model_state"])
optimizer.load_state_dict(checkpoint["optim_state"])

print(optimizer.state_dict())




```

    {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}
    {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}


## Salvare modelli da GPU
Minuto 16 circa.

Gli esempi sopra fungono se tutt il modello e' tenuto sulla CPU (sia per load che train, validation).

**map_location**
argomento di `load_state_dict(PATH, map_location=device)`

non e' chiaro cosa intenda per *save on GPU* (io salvo su disco! magari intende che il modello e' sulla GPU e poi lo metto su disco)



```python
import torch
import torch.nn as nn

# SAVE sulla GPU, load sulla CPU

device = torch.device("cuda")
model.to(device)
torch.save(model.state_dict(), PATH)


device =torch.device("cpu")
model = Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))

#####################################

# SAVE sulla GPU, load sulla GPU

device = torch.device("cuda")
model.to(device)
torch.save(model.state_dict(), PATH)


device =torch.device("cpu")
model = Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.to(device)                      


#####################################

# SAVE sulla CPU, load sulla GPU


torch.save(model.state_dict(), PATH)


device =torch.device("cuda")
model = Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location="cuda:0")  # 0 e' per la GPU zerp
model.to(device)                        
                      
                      
                      
                      
```

