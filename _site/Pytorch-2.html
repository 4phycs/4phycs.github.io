<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href="https://fonts.googleapis.com/css?family=Maven+Pro:400,500&amp;subset=latin-ext,vietnamese" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Dancing+Script:400,700&amp;subset=vietnamese" rel="stylesheet">
  <meta name="google-site-verification" content="8zqeFQNuNAWS7ye6oN69hdEeYC_RsDyAlhht79xtAQo" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/assets/res/banner.png" />

  

  <title>
    
      Pytorch 2 Chainrule Autograd | 4Phycs
    
  </title>

  

  <!-- page's cover -->
  
    <meta property="og:image" content="http://localhost:4000/images/defaultCoverPost.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1234">
    <meta property="og:image:height" content="592">
  

  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  

  <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.png">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="stylesheet" href="/assets/css/thi_scss.css">

  
    
      <link rel="stylesheet" href="/assets/css/post.css">
    
  

  

  <link rel="stylesheet" href="/assets/css/syntax.css">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
  <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
  
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Pytorch 2 Chainrule Autograd" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="tocIn this post" />
<meta property="og:description" content="tocIn this post" />
<link rel="canonical" href="http://localhost:4000/Pytorch-2" />
<meta property="og:url" content="http://localhost:4000/Pytorch-2" />
<meta property="og:site_name" content="4Phycs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-15T00:00:00+02:00" />
<meta name="google-site-verification" content="" />
<script type="application/ld+json">
{"headline":"Pytorch 2 Chainrule Autograd","dateModified":"2021-10-15T00:00:00+02:00","datePublished":"2021-10-15T00:00:00+02:00","description":"tocIn this post","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Pytorch-2"},"@type":"BlogPosting","url":"http://localhost:4000/Pytorch-2","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
	<header>
  
    <nav class="top-nav light-blue darken-4">
  <div class="nav-wrapper">
    <div class="container">
      <a class="page-title font-title" href="/">4Phycs</a>
      <ul id="nav-mobile" class="right hide-on-med-and-down">
        <li><a href="/categories">Ita Eng</a></li>
        <li><a href="/tags">Tags</a></li>
        <li><a href="/me">Me</a></li>
        <li><a href="/about">About</a></li>
        <li><a href="/contact">Contact</a></li>
      </ul>
    </div>
  </div>
</nav>

<div class="container">
  <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
    <i class="material-icons">menu</i>
  </a>
</div>
<div id="slide-out" class="side-nav fixed">
  <div>
    <div class="userView thi-userView">
      <div class="background"></div>
        <a href="/">
          <img style="display:inherit;" class="circle z-depth-2" src="/assets/res/user.png">
        </a>
      <span style="font-size: larger;" class="white-text name">Paolo Avogadro</span>
      <span class="white-text email"><a style="color: #bdbdbd;" href="http://"></a></span>
    </div>
  </div>
  <div style="padding: 10px;">
    <form action="/search" method="get">
      <input class="search-sidebar" type="search" name="q"  placeholder="search something?" autofocus>
      <input type="submit" value="Search" style="display: none;">
    </form>
  </div>
  <div id="toc-bar">
    <div class="toc-bar-title">
      In this post
    </div>
    <ol id="toc-sidebar">
  <li><a href="#chainrule-e-autograd">ChainRule e Autograd</a>
    <ol>
      <li><a href="#introduzione-ai-grafi-computazionali">Introduzione ai grafi computazionali</a></li>
      <li><a href="#stop-tracking">Stop tracking</a></li>
    </ol>
  </li>
  <li><a href="#backpropagation">Backpropagation</a></li>
  <li><a href="#discesa-del-gradiente-manuale">Discesa del gradiente Manuale</a></li>
  <li><a href="#discesa-del-gradiente-autograd">Discesa del gradiente Autograd</a></li>
</ol>

  </div>
</div>
  
</header>
<main>
  <div class="container">
    <div id="post-info">
      <h3>Pytorch 2 Chainrule Autograd</h3>
      <span>
        Posted on
        <span style="display: initial;" class="cat-class">15/10/2021</span>,
        in
        
          
          
            <a class="cat-class cat-commas" href="/categories#italiano">Italiano</a>.
          
        
        <span class="reading-time" title="Estimated read time">
  
  
  <font size="2"> Reading time: 18 mins </font>
  
</span>

      </span>
    </div>

    <div class="divider"></div>
    <div class="row thi-post">
      <div class="col s12">
        <div id="toc">
  <div class="toc-title">
    <i class="material-icons mat-icon">toc</i><span>In this post</span>
  </div>
  <div id="full">
<ul id="markdown-toc">
  <li><a href="#chainrule-e-autograd" id="markdown-toc-chainrule-e-autograd">ChainRule e Autograd</a>    <ul>
      <li><a href="#introduzione-ai-grafi-computazionali" id="markdown-toc-introduzione-ai-grafi-computazionali">Introduzione ai grafi computazionali</a></li>
      <li><a href="#stop-tracking" id="markdown-toc-stop-tracking">Stop tracking</a></li>
    </ul>
  </li>
  <li><a href="#backpropagation" id="markdown-toc-backpropagation">Backpropagation</a></li>
  <li><a href="#discesa-del-gradiente-manuale" id="markdown-toc-discesa-del-gradiente-manuale">Discesa del gradiente Manuale</a></li>
  <li><a href="#discesa-del-gradiente-autograd" id="markdown-toc-discesa-del-gradiente-autograd">Discesa del gradiente Autograd</a></li>
</ul>

  </div>
</div>

<p>Indice Globale degli argomenti tra i vari post:</p>

<p> 1.1.                                    <strong><a href="/Pytorch-1">Indice degli argomenti</a></strong>.<br />
 1.1.                                      <strong><a href="/Pytorch-1">Introduzione e fonti</a></strong>.<br />
 1.1.                                  <strong><a href="/Pytorch-1">Lingo - Gergo utilizzato</a></strong>.<br />
 1.2.                                        <strong><a href="/Pytorch-1">Tensori in Pytorch</a></strong>.<br />
 2.1.                                      <strong><a href="/Pytorch-2">Chainrule e Autograd</a></strong>.<br />
 2.2.                                           <strong><a href="/Pytorch-2">Backpropagation</a></strong>.<br />
 3.1.                                          <strong><a href="/Pytorch-3">Loss e optimizer</a></strong>.<br />
 3.2.                                        <strong><a href="/Pytorch-3">Modelli di Pytorch</a></strong>.<br />
 3.3.                                      <strong><a href="/Pytorch-3">Dataset e Dataloader</a></strong>.<br />
 3.4.                                       <strong><a href="/Pytorch-3">Dataset Transforms</a></strong>.<br />
 3.5.                              <strong><a href="/Pytorch-3">Softmax e Cross-Entropy Loss</a></strong>.<br />
 3.6.                                       <strong><a href="/Pytorch-3">Activation Function</a></strong>.<br />
 4.1.                                <strong><a href="/Pytorch-4">Feed Forward Neural Network</a></strong>.<br />
 5.1.                               <strong><a href="/Pytorch-5">Convolutional Neural Network</a></strong>.<br />
 5.2.                                          <strong><a href="/Pytorch-5">Transfer Learning</a></strong>.<br />
 5.3.                                                <strong><a href="/Pytorch-5">Tensorboard</a></strong>.<br />
 6.1.                              <strong><a href="/Pytorch-6">I/O Saving and Loading Models</a></strong>.<br />
 7.1.                                  <strong><a href="/Pytorch-7">Recurrent Neural Networks</a></strong>.<br />
 7.2.                                            <strong><a href="/Pytorch-7">RNN, GRU e LSTM</a></strong>.<br />
 8.1.                                          <strong><a href="/Pytorch-8">Pytorch Lightning</a></strong>.<br />
 8.2.                                               <strong><a href="/Pytorch-8">LR Scheduler</a></strong>.<br />
 9.1.                                                <strong><a href="/Pytorch-9">Autoencoder</a></strong>.</p>

<h1 id="chainrule-e-autograd">ChainRule e Autograd</h1>
<p>Il pacchetto <code class="language-plaintext highlighter-rouge">Autograd</code> fornisce differenziazione automatica per le operazioni (funzioni) sui tensori:<br />
<strong>requires_grad=True</strong> <br />
Immagina un <strong>tensore</strong> come una semplice variabile (multidimensionale) che entra in un grafo computazionale. Alla fine del grafo ho uno scalare (in genere) e voglio sapere come dipende questo scalare dal un particolare tensore, allora devo usare Autograd.
<script type="math/tex">\displaystyle
L(z(x)): R^n \rightarrow R</script></p>

<h2 id="introduzione-ai-grafi-computazionali">Introduzione ai grafi computazionali</h2>
<p>Per esempio:</p>
<ul>
  <li>costruisco un tensore x  (1D con 3 ingressi random)</li>
  <li>costruisco un tensore y funzione di x: <strong>y=x+2</strong></li>
  <li>costruisco un tensore z funzione di y: <strong>z= 3y$^2$</strong></li>
  <li>ATTENTO il gradiente si puo’ calcolare solo se alla fine si hanno dei valori SCALARI (altrimenti ho un numero di gradienti pari alle componenti del vettore). La logica e’ chiara, alla fine io voglio vedere come varia una funzione di LOSS rispetto ai parametri che metto nella rete neurale. La LOSS e’ una funzione scalare e quindi non sono state implementate delle variazioni per funzioni vettoriali.</li>
  <li>calcolo quindi il valore medio di <strong>u=&lt;z&gt;</strong> (per avere uno scalare)</li>
</ul>

<script type="math/tex; mode=display">{\bf x}=
\left(
\begin{eqnarray}
 x_1 \\
 x_2 \\
 x_3
\end{eqnarray}
\right)</script>

<p><script type="math/tex">{\bf y}=
\left(
\begin{eqnarray}
 y_1 \\
 y_2 \\
 y_3
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 x_1+2 \\
 x_2+2 \\
 x_3+2
\end{eqnarray}
\right)</script></p>

<p><script type="math/tex">{\bf z}
=
\left(
\begin{eqnarray}
 3y_1^2 \\
 3y_2^2 \\
 3y_3^2
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 3(x_1+2)^2 \\
 3(x_2+2)^2 \\
 3(x_3+2)^2
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 3(x_1^2+4x_1+4) \\
 3(x_2^2+4x_2+4) \\
 3(x_3^2+4x_3+4)
\end{eqnarray}
\right)</script></p>

<script type="math/tex; mode=display">\displaystyle
u = \frac{1}{3}\sum_{i=1}^3z_i = \frac{1}{3} \sum 3y_i^2  =  \sum y_i^2 \mbox{  Derivata parziale}  \Rightarrow  \frac{\partial u}{\partial y_k} = 2 y_k</script>

<p>Se voglio conoscere la dipendenza di ${\bf u}$ da parte di ${\bf x}$, dal punto di vista matematico devo calcolare la derivata parziale di u rispetto a x:</p>

<p><script type="math/tex">\displaystyle \frac{ \partial u } {\partial x_j} = \frac{\partial u}{\partial y_i} \frac{\partial y_i}{\partial x_j}</script> (indici ripetuti sono sommati)</p>

<script type="math/tex; mode=display">\displaystyle
 \frac{\partial u}{\partial y_k} = 2 y_k = 2(x_k+2)</script>

<script type="math/tex; mode=display">\displaystyle
 \frac{\partial y_k}{\partial x_j} = \delta_{kj}</script>

<p>Quindi:
<script type="math/tex">\displaystyle \frac{ \partial u } {\partial x_k} = 2(x_k+2)</script></p>

<p>Se il valore del tensore ${\bf x_0} = (1, 1, 1)$, alora il gradiente rispetto alla variaibile x della funzione u e’ un vettore che vale:</p>

<p><script type="math/tex">\nabla_x u |_{x_0} = 
=
\left(
\begin{eqnarray}
 2(x_1+2) \\
 2(x_2+2) \\
 2(x_3+2)
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 6 \\
 6 \\ 
 6 
\end{eqnarray}
\right)</script></p>

<p>Pensala cosi’: C’e’ una funzione di molte variabili che vengono combinate passo passo. Queste variabili pensale come proprio i pesi della rete neurale. Alla fine noi vogliamo minimizzare la <strong>LOSS</strong>. Quindi prendiamo il gradiente per trovare la pendenza massima e scendiamo lungo il gradiente di queste variabili con piccoli passi, sperando di raggiungere un buon minimo (occhio che la cosa non e’ garantita banalmente in quanto non siamo in un caso semplice di un solo massimo, potremmo finire in un minimo locale!).</p>

<p><strong>Attenzione</strong></p>

<p>Quando si fa il <strong>.backward()</strong> i valori dei gradienti vengono ACCUMULATI nell’attributo <strong>.grad</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>   <span class="c1">#  x = [x_1, x_2, x_3] = [1, 1, 1]
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>                               <span class="c1">#  y = [y_1, y_2, y_3]      
</span>
<span class="c1">##### Occhio y e' funzione di x, che ha requires_grad=True. Quindi ha come attributo grad_fn
</span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>                           <span class="c1">#  z = [3 y_1^2, 3 y_2^2, 3 y_3^2] Lavorano sul singolo ingresso!
</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                            <span class="c1">#  zmean = 1/3 ( 3 y_1^2 +  3 y_2^2 + 3 y_3^2 )      calcolo la media
</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                            <span class="c1"># back propagation 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>                           <span class="c1"># dz/dx = dz/ dy * dy/dx CALCOLATO nel valore corrente delle x
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;AddBackward0 object at 0x000001EE446B2BB0&gt;
tensor([6., 6., 6.])
</code></pre></div></div>

<p>Se l’output non e’ uno scalare si devono specificare gli argomenti per il <em>metodo</em> <strong>.backward()</strong>, non mi e’ chiaro come questi argomenti vengano usati.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#x = torch.randn(3, requires_grad=True)     # tensore 1D
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>     <span class="c1"># tensore 1D
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>                                  <span class="c1"># altro tensore 1D
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>                        
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>                              <span class="c1"># y * y * y * ... * y (10 volte +1 del passo precedente)  = x * 2**11
</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>         <span class="c1"># qui ho specificato che voglio il gradiente rispetto a ... v? non chiaro forse fa derivata direzionale
</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([2252.8000, 2252.8000, 2252.8000], grad_fn=&lt;MulBackward0&gt;)
torch.Size([3])
tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])
</code></pre></div></div>

<h2 id="stop-tracking">Stop tracking</h2>
<ul>
  <li>Supponiamo di volere fare un’update dei pesi durante il loop del training.</li>
  <li>questo implica fare delle nuove funzioni sui pesi (le update), e quindi quando si fa  la back propagation si rischia che questa tenga conto anche delle update! Bisogna quindi dire al TENSORE di non tenere conto delle update. Ovvero si deve dire al TENSORE che deve essere tracciato <strong>solo</strong> lungo il <strong>network computazionale</strong></li>
</ul>

<p>(<strong>non del tutto chiaro devo fare esperimenti</strong>)</p>

<ul>
  <li><strong>x.requires_grad_(False)</strong>  (nota l’underscore <strong>_</strong> finale per INPLACE)</li>
  <li><strong>x.detach()</strong></li>
  <li>wrap in <strong>with torch.no_grad():</strong></li>
</ul>

<p>Se si usa il metodo <strong>.zero_()</strong> questo riempie il gradiente prima di un nuovo passo di ottimizzazione.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>         <span class="c1"># qui NON accendiamo il requires_grad
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>        <span class="c1"># e appunto se controlliamo da': False
</span><span class="n">b</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>       <span class="c1"># costruisco un nuovo Tensore b
</span><span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>              <span class="c1"># e per qusto non c'e' l'attributo grad_fn che indica che c'e' una gradiente
</span>
<span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>        <span class="c1"># accendiamo INPLACE(_) il gradiente  
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>        <span class="c1"># ora il risultato e' True
</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>             <span class="c1"># creiamo uno scalare b con sum() fa la somma del Tensore. 
</span><span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>              <span class="c1">#  
</span>
<span class="c1"># .detach(): get a new Tensor with the same content but no gradient computation:
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="c1"># wrap in 'with torch.no_grad():'
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span> <span class="c1"># qui ho fatto un'altra funzione con x ma non contribuisce al gradiente!
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>False
None
True
&lt;SumBackward0 object at 0x000001A0C4C411F0&gt;
True
False
True
False
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -------------
# backward() accumulates the gradient for this tensor into .grad attribute.
# !!! We need to be careful during optimization !!!
# Use .zero_() to empty the gradients before a new optimization step!
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># just a dummy example
</span>    <span class="n">model_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">model_output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

    <span class="c1"># optimize model, i.e. adjust weights...
</span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>                 <span class="c1"># quando faccio l'ottimizzazione dei valori del tensore
</span>        <span class="n">weights</span> <span class="o">-=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span>     <span class="c1"># non voglio che facciano parte del grafo computazionale!  
</span>
    <span class="c1"># this is important! It affects the final weights &amp; output
</span>    <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>   <span class="c1"># se non azzeri c'e' accumulo (?)
</span>
<span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>

<span class="c1"># Optimizer has zero_grad() method
# optimizer = torch.optim.SGD([weights], lr=0.1)
# During training:
# optimizer.step()
# optimizer.zero_grad()
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)
tensor(4.8000, grad_fn=&lt;SumBackward0&gt;)
</code></pre></div></div>

<h1 id="backpropagation">Backpropagation</h1>
<p>un esempio semplice di backpropagation.</p>
<ul>
  <li>costruisco un tensore 0D x=1  (e’ il <code class="language-plaintext highlighter-rouge">predictor</code>)</li>
  <li>costruisco un tensore 0D y=2  (e’ la funzione obiettivo)</li>
  <li>costruisco un tensore 0D w=1  (sono i pesi che voglio ottimizzare)</li>
  <li>calcolo le y_predicted =w*x</li>
  <li>calcolo la LOSS (y_predicted-y)$^2$  (tutto questo e’ il forward pass)</li>
  <li>calcolo la BACKPROPAGATION (stando attento a non farla entrare nel grafo computazionale)</li>
  <li>azzero i gradienti e ripeto varie epoche</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>                          <span class="c1"># costruisco un tensore 0D (1 oggetto): i predictors
</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>                          <span class="c1"># un altro tensore 0D:                  la risposta ESATTA
</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>      <span class="c1"># questo tensore ha accesa la condizione requires_grad, i PESI
</span>
<span class="c1"># FORWARD PASS
</span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># costruisco un grafo computazionale, ora y =w*x, la risposta CALCOLATA 
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># la funzione di LOSS 
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># BACKWARD PASS dLoss/dw                       # calcolo la dipendenza della LOSS in funzione dei PESI
</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                               
<span class="c1">#print(w.grad)
</span>
<span class="c1"># A questo punto voglio fare una update dei PESI per cercare di fare predizioni migliori
</span>
<span class="c1"># 
# l'update dei PESI NON deve entrare nel grafo computazionale
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>      <span class="c1"># mi muovo lungo la direzione di massima crescita... al negativo di un passetto
</span>
<span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>              <span class="c1"># NON dimenticare di azzerare i gradienti 
</span>

<span class="c1"># FORWARD PASS
</span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># nuovo forward pass 
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># nuova funzione di LOSS 
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1">############# faccio ora un ciclo ################
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>      <span class="c1"># mi muovo lungo la direzione di massima crescita... al negativo di un passetto
</span>
    <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>              <span class="c1"># AZZERO i gradienti
</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># nuovo forward pass 
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># nuova funzione di LOSS 
</span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                                <span class="c1"># backward! se non lo faccio il gradiente e' stato azzerato! 
</span>    <span class="k">if</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="o">==</span><span class="n">epoch</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(1., grad_fn=&lt;PowBackward0&gt;)
tensor(0.9604, grad_fn=&lt;PowBackward0&gt;)
tensor(0.9604, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.6412, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.4281, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.2858, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.1908, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.1274, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0850, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0568, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0379, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0253, grad_fn=&lt;PowBackward0&gt;) 99
</code></pre></div></div>

<h1 id="discesa-del-gradiente-manuale">Discesa del gradiente Manuale</h1>
<p>proviamo ora con un esempio 1D (prima era 0D) con una regressione lineare.</p>

<ul>
  <li>i vari passi vengono calcolati MANUALMENTE senza usare Torch</li>
  <li>costruisco una funzione che fa il forward pass</li>
  <li>costruisco una funzione che fa il backward pass</li>
  <li>calcolo il gradiente (senza autograd)</li>
</ul>

<p>La cosa interessante e’ che qui ho un array in ingresso.
Prendo tutti i valori dell’array di ingresso e con essi faccio il training tutti insieme (calcolo infatti la LOSS su tutti).
Poi il passo forward lo faccio su uno scalare! per vedere se la predizione funziona</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="c1"># Regressione Lineare 
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># PREDICTORS
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># OBIETTIVO
</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.0</span>                                        <span class="c1"># pesi (ma non e' un tensore...) 
</span>
<span class="c1"># MODEL OUTPUT 
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                               <span class="c1"># FORWARD PASS
</span>
<span class="c1"># LOSS MSE
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                           <span class="c1"># LOSS MSE  
</span>    <span class="k">return</span> <span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>            <span class="c1"># uso un metodo dei tensori .mean()
</span>
<span class="c1"># J = MSE = 1/N * (w*x - y)**2
# dJ/dw = 1/N * 2x(w*x - y)
</span><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                    <span class="c1"># calcolo il gradiente  
</span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione prima del training: f(5) = {forward(5):.3f}'</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>               <span class="c1"># FORWARD
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>               <span class="c1"># LOSS
</span>    
    <span class="n">dw</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>       <span class="c1"># GRADIENTE (senza autograd) 
</span>    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span>           <span class="c1"># UPDATE   
</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}'</span><span class="p">)</span>
     
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione dopo il training: f(5) = {forward(5):.3f}'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prediction before training: f(5) = 0.000
epoch 1: w = 1.200, loss = 30.00000000
epoch 3: w = 1.872, loss = 0.76800019
epoch 5: w = 1.980, loss = 0.01966083
epoch 7: w = 1.997, loss = 0.00050332
epoch 9: w = 1.999, loss = 0.00001288
epoch 11: w = 2.000, loss = 0.00000033
epoch 13: w = 2.000, loss = 0.00000001
epoch 15: w = 2.000, loss = 0.00000000
epoch 17: w = 2.000, loss = 0.00000000
epoch 19: w = 2.000, loss = 0.00000000
Prediction after training: f(5) = 10.000
</code></pre></div></div>

<h1 id="discesa-del-gradiente-autograd">Discesa del gradiente Autograd</h1>
<p>come il punto precedente ma usando Autograd</p>

<ul>
  <li><strong>ATTENZIONE</strong> il print non legge bene il formato dei “tensori”, devo usare il metodo <strong>.item()</strong> per ottenere il valore.</li>
  <li><strong>.backward()</strong> va fatto sulla loss</li>
  <li><strong>.grad</strong>  e’ automaticamente ottenuto come parametro del tensore (per esempio dei pesi)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Regressione Lineare 
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># PREDICTORS
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># OBIETTIVO
</span>
<span class="c1">#Nota che posso vedere sia dal punto di vista spaziale che temporale.
# dal punto di vista temporale passo alla mia rete neurale un predictor per volta
# (ma non e' manco piu' un vettore).
# dal punto di vista spaziale, passo tutti i predictor e ottengo tutti gli obiettivi.
</span>

<span class="c1"># trasformo in TENSORI (avrei potuto direttamente usare torch.tensor(), ma cosi' uso from_numpy())
</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                        <span class="c1"># autograd non serve
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>                        <span class="c1"># neanche qui 
</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>    <span class="c1"># pesi: accendo Autograd  
</span>
<span class="c1"># MODEL OUTPUT 
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                               <span class="c1"># FORWARD PASS
</span>
<span class="c1"># LOSS MSE
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                           <span class="c1"># LOSS MSE  
</span>    <span class="k">return</span> <span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>            <span class="c1"># mean() e' un metodo dei tensori
</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione prima del training: f(5) = {forward(5).item():.3f}'</span><span class="p">)</span>

<span class="c1"># Parametri del Training
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>               <span class="c1"># FORWARD
</span>    <span class="n">LOSS</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>            <span class="c1"># LOSS
</span>    <span class="n">LOSS</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                   <span class="c1"># BACKPROPAGATION (Autograd) 
</span>    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>   <span class="c1"># UPDATE   
</span>    <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>                    <span class="c1"># AZZERO i gradienti
</span>        
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1}: w = {w.item():.3f}, loss = {LOSS.item():.8f}'</span><span class="p">)</span>
        <span class="c1">#print(w)    
#print(f'Predizione dopo il training: f(5) = {forward(5):.3f}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predizione prima del training: f(5) = 0.000
epoch 1: w = 0.300, loss = 30.00000000
epoch 11: w = 1.665, loss = 1.16278565
epoch 21: w = 1.934, loss = 0.04506890
epoch 31: w = 1.987, loss = 0.00174685
epoch 41: w = 1.997, loss = 0.00006770
</code></pre></div></div>


      </div>
    </div>

    <div class="tag-list">
      
      
      
      <a class="tag-chip" href="/tags#python_cap"><div class="chip z-depth-1">Python</div></a>
      
      
      
      <a class="tag-chip" href="/tags#deeplearning_cap"><div class="chip z-depth-1">DeepLearning</div></a>
      
      
      
      <a class="tag-chip" href="/tags#neural-nets_cap"><div class="chip z-depth-1">Neural Nets</div></a>
      
      
      
      <a class="tag-chip" href="/tags#cuda_cap"><div class="chip z-depth-1">Cuda</div></a>
      
    </div>
    
          <style type="text/css">
.related-posts{
  border: 3px dotted #2e87e7;
  border-radius: 5px;
  margin: 20px 0;
  padding: 10px 10px 0 10px;
}
.related-posts span{
  font-size: 130%;
  font-weight: 500;
  color: #2e87e7;
}
.related-posts ul{
  margin-top: 5px!important;
}
.thi-icon{
  float: left;
  line-height: inherit;
  margin-right: 5px;
  margin-left: 2px;
  color: #2e87e7;
}
</style>

<div class="related-posts">
  <i class="material-icons thi-icon">grade</i><span>You might also like ...</span>
  
  <ul>
    
    
      
      
        
      
        
      
        
      
        
      
    
      
      
        
          <li>
            <a href="/Pytorch-1">
              Pytorch 1 Inizio, Tensori
            </a>
            <small>12 Nov 2021</small>
          </li>
          
          
    
      
      
        
          <li>
            <a href="/Seaborn">
              Seaborn - appunti
            </a>
            <small>08 Nov 2021</small>
          </li>
          
          
    
      
      
        
      
        
          <li>
            <a href="/Matplotlib">
              Matplotlib - appunti
            </a>
            <small>07 Nov 2021</small>
          </li>
          
          
    
      
      
        
          <li>
            <a href="/Pytorch-7">
              Pytorch 7 Recurrent Neural Network
            </a>
            <small>17 Oct 2021</small>
          </li>
          
          
    
      
        

    
    
  </ul>
</div>

    
    		
			<script src="  https://unpkg.com/showdown/dist/showdown.min.js"></script>
<script>
const GH_API_URL = 'https://api.github.com/repos/4phycs/4phycs.github.io/issues/18/comments';

let request = new XMLHttpRequest();
request.open( 'GET', GH_API_URL, true );
request.onload = function() {
	if ( this.status >= 200 && this.status < 400 ) {
		let response = JSON.parse( this.response );

		for ( var i = 0; i < response.length; i++ ) {
			document.getElementById( 'gh-comments-list' ).appendChild( createCommentEl( response[ i ] ) );
		}

		if ( 0 === response.length ) {
			document.getElementById( 'no-comments-found' ).style.display = 'block';
		}
	} else {
		console.error( this );
	}
};

function createCommentEl( response ) {
	let user = document.createElement( 'a' );
	user.setAttribute( 'href', response.user.url.replace( 'api.github.com/users', 'github.com' ) );
	user.classList.add( 'user' );

	let userAvatar = document.createElement( 'img' );
	userAvatar.classList.add( 'avatar' );
	userAvatar.setAttribute( 'src', response.user.avatar_url );

	user.appendChild( userAvatar );

	let commentLink = document.createElement( 'a' );
	commentLink.setAttribute( 'href', response.html_url );
	commentLink.classList.add( 'comment-url' );
	commentLink.innerHTML = '#' + response.id + ' - ' + response.created_at;

	let commentContents = document.createElement( 'div' );
	commentContents.classList.add( 'comment-content' );
	commentContents.innerHTML = response.body;
	// Progressive enhancement.
	if ( window.showdown ) {
		let converter = new showdown.Converter();
		commentContents.innerHTML = converter.makeHtml( response.body );
	}

	let comment = document.createElement( 'li' );
	comment.setAttribute( 'data-created', response.created_at );
	comment.setAttribute( 'data-author-avatar', response.user.avatar_url );
	comment.setAttribute( 'data-user-url', response.user.url );

	comment.appendChild( user );
	comment.appendChild( commentContents );
	comment.appendChild( commentLink );

	return comment;
}
request.send();
</script>

<hr>

<div class="github-comments">
	<h2>Comments</h2>
	<ul id="gh-comments-list"></ul>
	<div class="buttonArea">
	  <a target="_blank" href="https://github.com/4phycs/4phycs.github.io/issues/18"class="button">Add comment (via Github)</a>
	</div>
</div>


		
 
  </div>
</main>

	<script src="/assets/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript">
  jQuery(document).ready(function($){
    // browser window scroll (in pixels) after which the "back to top" link is shown
    var offset = 300,
      //browser window scroll (in pixels) after which the "back to top" link opacity is reduced
      offset_opacity = 1200,
      //duration of the top scrolling animation (in ms)
      scroll_top_duration = 700,
      //grab the "back to top" link
      $back_to_top = $('.cd-top');

    //hide or show the "back to top" link
    $(window).scroll(function(){
      ( $(this).scrollTop() > offset ) ? $back_to_top.addClass('cd-is-visible') : $back_to_top.removeClass('cd-is-visible cd-fade-out');
      // if( $(this).scrollTop() > offset_opacity ) { 
      //  $back_to_top.addClass('cd-fade-out');
      // }
    });

    //smooth scroll to top
    $back_to_top.on('click', function(event){
      event.preventDefault();
      $('body,html').animate({
        scrollTop: 0 ,
        }, scroll_top_duration
      );
    });

  });
</script>
<style type="text/css">
.cd-top {
  display: inline-block;
  height: 50px;
  width: 50px;
  position: fixed;
  bottom: 2%;
  right: 2%;
  border-radius: 40px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
  /* image replacement properties */
  overflow: hidden;
  text-indent: 100%;
  white-space: nowrap;
  background: #bbb url(/images/cd-top-arrow.svg) no-repeat center 50%;
  visibility: hidden;
  opacity: 0;
  -webkit-transition: opacity .3s 0s, visibility 0s .3s;
  -moz-transition: opacity .3s 0s, visibility 0s .3s;
  transition: opacity .3s 0s, visibility 0s .3s;
}
.cd-top.cd-is-visible, .cd-top.cd-fade-out, .no-touch .cd-top:hover {
  -webkit-transition: opacity .3s 0s, visibility 0s 0s;
  -moz-transition: opacity .3s 0s, visibility 0s 0s;
  transition: opacity .3s 0s, visibility 0s 0s;
}
.cd-top.cd-is-visible {
  /* the button becomes visible */
  visibility: visible;
  opacity: 1;
}
.cd-top.cd-fade-out {
  /* if the user keeps scrolling down, the button is out of focus and becomes less visible */
  opacity: .5;
}
.no-touch .cd-top:hover {
  background-color: #e86256;
  opacity: 1;
}
</style>

<a href="#0" class="cd-top">Top</a>
	<footer class="page-footer light-blue accent-4">  
  <div class="footer-copyright">
    <div class="container text-white">
     <a href="">4Phycs</a> &#xA9; 2022 Inherited from <a href="https://shawnteoh.github.io/matjek/">MatJeck</a>.
    </div>
  </div>
</footer>

<script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>


  
    <script src="/assets/js/post.js"></script>
  





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
  (window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>

<script src="/assets/js/main.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158698892-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158698892-1');
</script>

</body>
</html>
