<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href="https://fonts.googleapis.com/css?family=Maven+Pro:400,500&amp;subset=latin-ext,vietnamese" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Dancing+Script:400,700&amp;subset=vietnamese" rel="stylesheet">
  <meta name="google-site-verification" content="8zqeFQNuNAWS7ye6oN69hdEeYC_RsDyAlhht79xtAQo" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/assets/res/banner.png" />

  

  <title>
    
      Pytorch 7 Recurrent Neural Network | 4Phycs
    
  </title>

  

  <!-- page's cover -->
  
    <meta property="og:image" content="http://localhost:4000/images/defaultCoverPost.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1234">
    <meta property="og:image:height" content="592">
  

  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  

  <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.png">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="stylesheet" href="/assets/css/thi_scss.css">

  
    
      <link rel="stylesheet" href="/assets/css/post.css">
    
  

  

  <link rel="stylesheet" href="/assets/css/syntax.css">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
  <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
  
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Pytorch 7 Recurrent Neural Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="tocIn this post" />
<meta property="og:description" content="tocIn this post" />
<link rel="canonical" href="http://localhost:4000/Pytorch-7" />
<meta property="og:url" content="http://localhost:4000/Pytorch-7" />
<meta property="og:site_name" content="4Phycs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-17T00:00:00+02:00" />
<meta name="google-site-verification" content="" />
<script type="application/ld+json">
{"headline":"Pytorch 7 Recurrent Neural Network","dateModified":"2021-10-17T00:00:00+02:00","datePublished":"2021-10-17T00:00:00+02:00","description":"tocIn this post","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Pytorch-7"},"@type":"BlogPosting","url":"http://localhost:4000/Pytorch-7","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
	<header>
  
    <nav class="top-nav light-blue darken-4">
  <div class="nav-wrapper">
    <div class="container">
      <a class="page-title font-title" href="/">4Phycs</a>
      <ul id="nav-mobile" class="right hide-on-med-and-down">
        <li><a href="/tags">Tags</a></li>
        <li><a href="/categories">Ita-Eng</a></li>
        <li><a href="/me">Me</a></li>
        <li><a href="/about">About</a></li>
        <li><a href="/contact">Contact</a></li>
      </ul>
    </div>
  </div>
</nav>

<div class="container">
  <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
    <i class="material-icons">menu</i>
  </a>
</div>
<div id="slide-out" class="side-nav fixed">
  <div>
    <div class="userView thi-userView">
      <div class="background"></div>
        <a href="/">
          <img style="display:inherit;" class="circle z-depth-2" src="/assets/res/user.png">
        </a>
      <span style="font-size: larger;" class="white-text name">Paolo Avogadro</span>
      <span class="white-text email"><a style="color: #bdbdbd;" href="http://"></a></span>
    </div>
  </div>
  <div style="padding: 10px;">
    <form action="/search" method="get">
      <input class="search-sidebar" type="search" name="q"  placeholder="search something?" autofocus>
      <input type="submit" value="Search" style="display: none;">
    </form>
  </div>
  <div id="toc-bar">
    <div class="toc-bar-title">
      In this post
    </div>
    <ol id="toc-sidebar">
  <li><a href="#recurrent-neural-network">Recurrent Neural Network</a></li>
  <li><a href="#rnn-gru-e-lstm">RNN, GRU e LSTM</a>
    <ol>
      <li><a href="#il-codice">Il Codice</a></li>
    </ol>
  </li>
</ol>

  </div>
</div>
  
</header>
<main>
  <div class="container">
    <div id="post-info">
      <h3>Pytorch 7 Recurrent Neural Network</h3>
      <span>
        Posted on
        <span style="display: initial;" class="cat-class">17/10/2021</span>,
        in
        
          
          
            <a class="cat-class cat-commas" href="/categories#italiano">Italiano</a>.
          
        
        <span class="reading-time" title="Estimated read time">
  
  
  <font size="2"> Reading time: 27 mins </font>
  
</span>

      </span>
    </div>

    <div class="divider"></div>
    <div class="row thi-post">
      <div class="col s12">
        <div id="toc">
  <div class="toc-title">
    <i class="material-icons mat-icon">toc</i><span>In this post</span>
  </div>
  <div id="full">
<ul id="markdown-toc">
  <li><a href="#recurrent-neural-network" id="markdown-toc-recurrent-neural-network">Recurrent Neural Network</a></li>
  <li><a href="#rnn-gru-e-lstm" id="markdown-toc-rnn-gru-e-lstm">RNN, GRU e LSTM</a>    <ul>
      <li><a href="#il-codice" id="markdown-toc-il-codice">Il Codice</a></li>
    </ul>
  </li>
</ul>

  </div>
</div>

<p>Indice Globale degli argomenti tra i vari post:
 1.1.                                    <strong><a href="/Pytorch-1">Indice degli argomenti</a></strong>.<br />
 1.2.                                      <strong><a href="/Pytorch-1">Introduzione e fonti</a></strong>.<br />
 1.6.                                  <strong><a href="/Pytorch-1">Lingo - Gergo utilizzato</a></strong>.<br />
 2.                                        <strong><a href="/Pytorch-1">Tensori in Pytorch</a></strong>.</p>
<ul>
  <li><strong><a href="/Pytorch-2">Grafo Computazionale e Calcolo dei Gradienti con Autograd</a></strong>.</li>
  <li><strong><a href="/Pytorch-2">Backpropagation introduzione</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Loss e optimizer</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Dataset e Dataloader</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Modelli di Pytorch</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Dataset Transforms</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Softmax</a></strong>.</li>
  <li><strong><a href="/Pytorch-3">Activation Function</a></strong>.</li>
  <li><strong><a href="/Pytorch-4">Feed Forward Neural Network</a></strong>.</li>
  <li><strong><a href="/Pytorch-5">Convolutional Neural Network</a></strong>.</li>
  <li><strong><a href="/Pytorch-5">Transfer Learning</a></strong>.</li>
  <li><strong><a href="/Pytorch-5">Tensorboard</a></strong>.</li>
  <li><strong><a href="/Pytorch-6">I/O Saving and Loading Models</a></strong>.</li>
  <li><strong><a href="/Pytorch-7">Recurrent Neural Networks</a></strong>.</li>
  <li><strong><a href="/Pytorch-7">RNN, GRU e LSTM</a></strong>.</li>
  <li><strong><a href="/Pytorch-8">Pytorch Lightning</a></strong>.</li>
  <li><strong><a href="/Pytorch-8">LR Scheduler</a></strong>.</li>
  <li><strong><a href="/Pytorch-9">Autoencoder</a></strong>.</li>
</ul>

<h1 id="recurrent-neural-network">Recurrent Neural Network</h1>

<p><a href="https://www.youtube.com/watch?v=WEV61GmmPrk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=19&amp;ab_channel=PythonEngineer">Video,</a>
 note e <a href="https://github.com/python-engineer/pytorch-examples">diapositive</a> di Python Engineer.</p>

<p><a href="https://www.youtube.com/watch?v=qjrad0V0uJE&amp;ab_channel=AlexanderAmini">Lezione</a>  del MIT sulle RNN
© Alexander Amini and Ava Soleimany  <br />
MIT 6.S191: Introduction to Deep Learning <br />
IntroToDeepLearning.com <br /></p>

<p><code class="language-plaintext highlighter-rouge">Attenzione</code> alla notazione: il <strong>hidden-tensor</strong> non e’ un <strong>hidden layer</strong> del modello! E’ un tensore che viene usato per la predizione ma non e’ l’input!</p>

<p><code class="language-plaintext highlighter-rouge">Nota</code> descrivo con qualche immagine in piu’ le RNN, LSTM e RCU nel prossimo capitolo.</p>

<p><code class="language-plaintext highlighter-rouge">Scopo</code>: costruire una rete neurale ricorrente RNN, che prenda una dopo l’altra le singole lettere di un nome (ogni lettera sara’ un input) e alla fine dell’ultima lettera dica a quale lingua appartiene il nome</p>

<ul>
  <li>usiamo batch di dimensione 1  qui (1 lettera)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">LOGICA</code></p>
<ul>
  <li>un nome e’ una sequenza di lettere.</li>
  <li>ogni lettera vine trasformata in un tensore-lettera che diventa <code class="language-plaintext highlighter-rouge">parte</code> dell’input della rete neurale.</li>
  <li>Perche’ ho scritto solo <code class="language-plaintext highlighter-rouge">parte</code> dell’input? Perche’ l’input e’ una concatenazione di un tensore-lettera e un tensore-hidden!</li>
  <li>la rete neurale restituisce in output a quale lingua appartiene la lettera … ma dato che in input c’e’ anche il tensore-hidden modificato dallo strato lineare (<strong>hidden tensor</strong>), c’e’ memoria delle lettere precedenti, quindi ha senso dire la lingua di appartenenza di una sola lettera: c’e’ comunque la memoria derivante dalle altre lettere ottenuta tramite il tensore hidden!</li>
  <li>come secondo output la rete neurale emette anche un nuovo tensore-hidden (che verra’ usato al passo successivo, concatenandolo al  tensore-lettera successivo)</li>
  <li>alla fine della parola mi deve dire di che lingua stiamo parlando.</li>
</ul>

<p>Le parole sono spezzate in modo da diventare sequenze di tensori, secondo la logica <strong>one-hot encoderd</strong>. Supponiamo di avere un alfabeto di 6 lettere: <code class="language-plaintext highlighter-rouge">a, e, i, o, u, l.</code>
    La parola <code class="language-plaintext highlighter-rouge">aiuola</code> diventa la seguente sequenza di tensori 1D contennti 6 ingressi:</p>

<p><strong>a</strong>= 
<script type="math/tex">\left( 
\begin{eqnarray}
   {\bf 1}  \\
   0  \\
   0  \\ 
   0  \\
   0  \\
   0  
\end{eqnarray} 
\right)</script>
, <strong>i</strong>= 
<script type="math/tex">\left( \begin{eqnarray}
   0  \\
   0  \\
   {\bf 1}  \\ 
   0  \\
   0  \\
   0  \\
   \end{eqnarray} 
\right)</script>
, <strong>u</strong>= 
<script type="math/tex">\left( 
\begin{eqnarray}
   0  \\
   0  \\
   0  \\ 
   0  \\
   {\bf 1}  \\
   0  \\
   \end{eqnarray} 
\right)</script> 
, <strong>o</strong>= 
<script type="math/tex">\left( 
\begin{eqnarray}
   0  \\
   0  \\
   0  \\ 
   {\bf 1}  \\
   0  \\
   0  \\
   \end{eqnarray} 
\right)</script> 
, <strong>l</strong>= 
<script type="math/tex">\left( 
\begin{eqnarray}
   0  \\
   0  \\
   0  \\ 
   0  \\
   0  \\
  {\bf 1}  \\
   \end{eqnarray} 
\right)</script> 
, <strong>a</strong>= 
<script type="math/tex">\left( \begin{eqnarray}
  {\bf 1}  \\
   0  \\
   0  \\ 
   0  \\
   0  \\
   0  \\
   \end{eqnarray} 
\right)</script></p>

<p>Quanto appena fatto e’ un tipo di <code class="language-plaintext highlighter-rouge">embedding</code>, ovvero una trasformazione da un dominio (quello delle lettere in questo caso) ad un 
formato matematico (di grandezza fissa) che puo’ essere processato dalle reti neurali. Chiaramente esistono molti tipi di embedding
differenti e possono avere un impatto importante sui risultati.</p>

<p>Il tensore hidden invece ha una dimensione che fissiamo noi (nell’esempio 128), al passo 0 viene inizializzato con tutti 0, 
ma ai passi successivi si popola perche’ lo strato lineare che costruisce le versioni successive del tensore hidden prende in ingresso sia il tensore combinato che il tensore della lettera. Per esempio se la lettera passata e’ la $a$ e il tensore-hidden e’:<br />
<strong>hidden</strong> = 
<script type="math/tex">\left( \begin{eqnarray}
  {\bf 0.3}  \\
   0.23  \\
   \dots  \\ 
   0.29  \\
   0.52  \\
   0.11  \\
   \end{eqnarray} 
\right)</script> 
<script type="math/tex">\begin{eqnarray}
   1  \\
   2  \\
   \dots  \\ 
   126  \\
   127  \\
   128  \\
   \end{eqnarray}</script></p>

<p>Allora il <strong>tensore-combinato</strong> = 
<script type="math/tex">\left(\begin{eqnarray}
     {\bf a}  \\
    {\bf hidden}  \\
   \end{eqnarray} 
\right)</script> =
<script type="math/tex">$\left( \begin{eqnarray}
   {\bf 1}  \\
   0  \\
   0  \\ 
   0  \\
   0  \\
   0  \\
  {\bf 0.3}  \\
   0.23  \\
   \dots  \\ 
   0.29  \\
   0.52  \\
   0.11  \\
   \end{eqnarray} 
\right)</script> 
(in questo esempio  il tensore combinato ha dimensione 6 + 128 (perche’ in questo esempio l’alfabeto per  l’one-hot encoring contiene solo <em>aeioul</em>, mentre la dimensione del tensore combinato nel video di Python Engineer e’ 57 +128, perche’ l’alfabeto da lui usato contiene tutte le maiuscole, le minuscole e alcuni segni di punteggiatura.</p>

<p><img src="/images/posts/pytorch/rnn-nomi-lingue.png" alt="cane" /></p>

<p><code class="language-plaintext highlighter-rouge">Osservazione</code>
Il tensore che viene dato in pasto alla fully connected layer e’ la versione concatenata di:</p>
<ul>
  <li>one-shot encoded  (che ha dim 57 nel nostro caso, le maiuscole, minuscole e qualche segno di punteggiatura</li>
  <li>hidden tensor (che ha dimensione 128, perche’ lo abbiamo scelto noi).</li>
</ul>

<p>Questo tensore concatenato viene dato in pasto anche ad un’altra fully connected layer in modo da mantenere la
memoria di quanto e’ successo.</p>

<p>Nel video del MIT si dice che per fare il training di una RNN si usano la cosiddetta: <code class="language-plaintext highlighter-rouge">BPTT</code> (Backpropagation through time)
occhio che nel nostro caso non mi pare che si faccia.  Nel dettaglio la backpropagation si fa sulla loss soltanto, e quindi l’hidden tensor e’ considerato come un input non come un peso. Per questo non mi e’ chiaro come venga aggiornato il tensore dei pesi che ho chiamato W2 nell’immagine.</p>

<p><code class="language-plaintext highlighter-rouge">PROBLEMI</code></p>

<ul>
  <li>
    <p>ho provato a mandare tutto sul device CUDA ma rallenta! sospetto che sia perche’ copio sul device di volta in volta.
Il tempo che impiega (su 5000 passi) e’ 23 s col device e 13 sulla cpu! Vediamo se riesco ad evitare di copiare le cose in GPU ogni passaggio per velocizzare il calcolo. Mettendo line_tensor e hidden prima ho guadagnao, ora sono 20 s (comunque piu’ lento che con la CPU)</p>
  </li>
  <li>
    <p>Che strano: rifacendolo andare qualche giorno successivo ci mette 307 secondi! (sulla CPU) e se provo a farlo andare sulla GPU dice che ci sono problemiperche’ alcune cose sono su CPU e altre su GPU.  Seguendo le indicazioni ho messo line_tensor.to(device) e ora sembra fungere linea 188.</p>
  </li>
  <li>
    <p>ho inserito tutto fino a quando fa “whole sequence/name” ma ottengo un errore non ben chiaro:
“IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)”. Errore trovato: avevo scritto 
<code class="language-plaintext highlighter-rouge">inout_tensor</code> invece che <code class="language-plaintext highlighter-rouge">input_tensor</code></p>
  </li>
  <li>
    <p>Il tensore in uscita ha dimensione 128 invece che 18 (il numero di lingue) e non capisco perche’! Chiaro avevo scritto: 
<code class="language-plaintext highlighter-rouge">self.i2o = nn.Linear(input_size + hidden_size , hidden_size)</code> che quindi mi dava come dimensione di uscita del layer lineare <code class="language-plaintext highlighter-rouge">hidden_size</code> invece che <code class="language-plaintext highlighter-rouge">output_size</code>!</p>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Note</code>:
Nella lezione si fa uso di funzioni di aiuto “helper functions” (io all’inizio capivo alpha-functions…). Python Engineer le mette in un modulo, mentre io le ho riscritte all’inizio del codice qui sotto</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dati https://download.pytorch.org/tutorial/data.zip
</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">glob</span>       <span class="c1"># ?
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="c1">#  Helper Functions
</span>
<span class="c1"># Alfabeto minuscolo e maiuscolo
</span><span class="n">ALL_LETTERS</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span> <span class="s">".,;''"</span>   <span class="c1"># insieme delle lettere e della punteggiatura usata
</span><span class="n">N_LETTERS</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ALL_LETTERS</span><span class="p">)</span>

<span class="c1"># converti un UNICODE in ASCII grazie a https://www.stackoverflow.com/a/518232/2809427
# in pratica trasforma le lettere accentate in lettere NON accentate
</span><span class="k">def</span> <span class="nf">unicode_to_ascii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span> <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s">'NFD'</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s">"Mn"</span> <span class="ow">and</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">ALL_LETTERS</span><span class="p">)</span>

<span class="c1"># costuisce un dizionario "category_lines" e una lista di nomi per le varie lingue
</span><span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
    <span class="n">category_lines</span> <span class="o">=</span> <span class="p">{}</span>     <span class="c1"># dizionario
</span>    <span class="n">all_categories</span> <span class="o">=</span> <span class="p">[]</span> 

    <span class="k">def</span> <span class="nf">find_files</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>    <span class="c1"># glob??? The glob module finds all the pathnames matching a specified pattern according to ...
</span>    
    <span class="c1"># leggi un file e spezzalo in linee
</span>    <span class="k">def</span> <span class="nf">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">unicode_to_ascii</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">find_files</span><span class="p">(</span><span class="s">'data/names/*.txt'</span><span class="p">):</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">all_categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
    
        <span class="n">lines</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
        <span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">]</span>  <span class="o">=</span><span class="n">lines</span>
        
    <span class="k">return</span> <span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span>
        
<span class="c1"># trova l'indice di posizione associato ad una lettera nalla parola
</span>
<span class="k">def</span> <span class="nf">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ALL_LETTERS</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">letter</span><span class="p">)</span>

<span class="c1"># trasforma una lettera in un tensore 1 x n letters (tensore riga)
</span>
<span class="k">def</span> <span class="nf">letter_to_tensor</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_LETTERS</span><span class="p">)</span>  
    <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="c1"># trasforma una linea in una &lt;line_length x 1 x n_letters&gt;
# o un array hon-hot letter
</span>
<span class="k">def</span> <span class="nf">line_to_tensor</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_LETTERS</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">letter</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
        <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="k">def</span> <span class="nf">random_training_example</span><span class="p">(</span><span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">random_choice</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="n">random_idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
    
    <span class="n">category</span> <span class="o">=</span> <span class="n">random_choice</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
    <span class="n">line</span> <span class="o">=</span><span class="n">random_choice</span><span class="p">(</span><span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">])</span>
    <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">all_categories</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">category</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span>
    <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">line_to_tensor</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span>


<span class="c1">###############################################################################
###############################################################################
###############################################################################
#print(ALL_LETTERS)
#category_lines , all_categories = load_data()
#print(category_lines['Italian'][:5])
#print(letter_to_tensor('J'))                # trasforma la lettera J (maiuscola) in un tensore one-hot encoding (sono 1D con 57 ingressi)
#print(line_to_tensor('Jones').size())       # Jones contiene 5 lettere, ognuna e' trasformata in un tensore 1D con 57 ingressi 
</span>
<span class="c1">#test = line_to_tensor('Jones')
#print(test)
##################  ok sopra funge correttamente   ###################
</span>

<span class="c1"># ho gia'importato torch sopra
</span><span class="kn">import</span> <span class="nn">torch.nn</span>  <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># from utils import ALL_LETTERS, N_LETTERS # qui non serve perche' fanno gia' parte di questo listato
# from load_data, letter_to_tensor, ...  
</span>
<span class="c1">#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>


<span class="c1"># esiste gia' un RNN in Torch, qui pero' lo creiamo da zero.
</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span> <span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>   <span class="c1"># ricorda che servono solo le dimensioni: dim tensore combinato, dim uscita
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span> <span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>   <span class="c1"># input 2 output
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># input ha dimensione 1,57, quindi softmax sulle colonne
</span>        
    
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden_tensor</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden_tensor</span> <span class="p">),</span><span class="mi">1</span><span class="p">)</span>   <span class="c1">#concatena input tensor e hidden tensor: nuova dimensione= input_size+hidden_size
</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>      <span class="c1"># qui spara fuori l'oggetto hidden, i pesi di questo layer non sono l'oggetto hidden!
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>      <span class="c1"># qui indica la guess riguardo la nazione
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>    <span class="c1"># qui usa la softmax per ottenere valori di probabilita'
</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>            <span class="c1"># restituisce sia l'output che l'hidden per il prossimo passo
</span>    
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>                             <span class="c1"># inizializza lo hidden_tensor alla dimensione hidden_size
</span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>


<span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>  <span class="c1"># chiave valore, nazione chiave, nome valore, e poi tutte le nazioni
</span><span class="n">n_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
<span class="c1">#print(n_categories)
</span>
<span class="n">n_hidden</span>  <span class="o">=</span><span class="mi">128</span>   <span class="c1"># selto da me
</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">N_LETTERS</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># qui ho inizializzato il modello, servono i parametri del costruttore
</span>


<span class="c1"># likelyhood di ogni nazione, vogliamo l'indice della categoria massima
</span><span class="k">def</span> <span class="nf">category_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">category_idx</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">all_categories</span><span class="p">[</span><span class="n">category_idx</span><span class="p">]</span>
    
<span class="c1">#print(category_from_output(output))
#print(output)
</span>
<span class="c1">####### facciamo il training ######
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>      <span class="c1"># negative log likelihood loss
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>         <span class="c1">#
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1">#  funzione helper che fa il training metto il tensore e la sua label
</span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">):</span>    

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                    <span class="c1"># azzero l'hidden tensor iniziale
</span>    <span class="c1">#        hidden = hidden.to(device)
</span>    <span class="c1">#line_tensor = line_tensor.to(device)
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span> 
        <span class="c1"># lunghezza del nome
</span>        <span class="c1">#l2d = line_tensor[i].to(device)
</span>        
<span class="c1">#        output, hidden = rnn(l2d, hidden)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>  <span class="c1"># inserisco i 2 tensori di input: lettera one-shot e hidden
</span>     
    <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">category_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1">#output = output.to(device) # inventato da me ma da comunque errore
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>




<span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">all_losses</span>  <span class="o">=</span> <span class="p">[]</span>
<span class="n">plot_steps</span><span class="p">,</span> <span class="n">print_steps</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span> <span class="mi">100000</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">100000</span>


<span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>     <span class="c1"># il momento d'inizio
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">random_training_example</span><span class="p">(</span><span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">)</span>
    
    <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">line_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">output</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="n">plot_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">all_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span> <span class="o">/</span><span class="n">plot_steps</span><span class="p">)</span>
        <span class="n">current_loss</span> <span class="o">=</span><span class="mi">0</span>
        
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="n">print_steps</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">category_from_output</span> <span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="s">"Corretto"</span> <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">category</span> <span class="k">else</span> <span class="n">f</span><span class="s">" Sbagliato ( {category})"</span>
        <span class="k">print</span> <span class="p">(</span><span class="n">f</span><span class="s">'{i} {i/n_iters*100} {loss:.4f} {line}/{guess}{correct}  '</span> <span class="p">)</span>
        
        
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="mi">100000</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Tempo impiegato '</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span><span class="n">since</span><span class="p">)</span>   
        
        
<span class="c1">#plt.figure()
#plt.plot(all_losses)
</span>
<span class="c1">#############  data una stringa in ingresso 
</span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">input_line</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\n</span><span class="s">&gt; {input_line}'</span><span class="p">)</span>                        <span class="c1"># la riscrive 
</span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="n">line_tensor</span>  <span class="o">=</span> <span class="n">line_to_tensor</span><span class="p">(</span><span class="n">input_line</span><span class="p">)</span>     <span class="c1"># trasforma in un tensore
</span>        
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>                    <span class="c1"># crea lo stato iniziale vuoto da dare in pasto
</span>        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>       <span class="c1"># gira sui tensori one-hot encoded
</span>            <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>   <span class="c1"># usa la rete neurale, uno dietro l'altro, cosi' hidden si aggiorna
</span>            
        <span class="n">guess</span> <span class="o">=</span> <span class="n">category_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>          <span class="c1"># Ottiene dal numero il nome della nazione
</span>        <span class="k">print</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>                                  <span class="c1"># stampa il nome della nazione 
</span>    
            
<span class="c1">########## qui si imparano molte cose #######
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">"Inserisci un nome (quit per terminare)"</span><span class="p">)</span>      <span class="c1"># scrive a video
</span>    <span class="k">if</span> <span class="n">sentence</span>  <span class="o">==</span> <span class="s">'quit'</span><span class="p">:</span>                     <span class="c1"># esci dal ciclo se scrivi quit
</span>        <span class="k">break</span>
    <span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>                           <span class="c1"># usa la rete neurale e scrivi la predizione
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>99999 99.99900000000001 1.3037 Woo/Chinese Sbagliato ( Korean)  
Tempo impiegato  319.30727434158325
Inserisci un nome (quit per terminare)quit
</code></pre></div></div>

<h1 id="rnn-gru-e-lstm">RNN, GRU e LSTM</h1>

<p>Il <a href="https://www.youtube.com/watch?v=0_PgWWmauHk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=20&amp;ab_channel=PythonEngineer">video</a>
di Python Enginner contiene un esempio di RNN. E’ basato fortemente sulle <a href="nn.RNN: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">note</a>
 di PyTorch.</p>

<p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">Recurrent Neural Networks cheatsheet</a> di Stanford
e’ molto completa e succinta! Probabilmente e’ la guida piu’ precisa, la matematica usa delle notazioni diverse dalle altre due 
(per esempio l’hidden tensor viene indicato con a), ma suppongo che sia la piu’ affidabile.</p>

<p>In questa <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">guida</a> 
illustrata delle LSTM e dell GRU si cerca di non mettere la matematica, ma ci sono delle gif animate.</p>

<p>In questa <a href="http://dprogrammer.org/rnn-lstm-gru">pagina</a> vengono indicate anche le formule associate ai vari passaggi;
   ma sospetto che le formule non corrispondano alle immagini (ha preso le immagini da qualche parte e le formule altrove). 
Di buono mette anche le formule per la backpropagation!</p>

<h2 id="il-codice">Il Codice</h2>

<p>Qui usiamo i moduli gia’ creati per Long Short Term Memory e per GRU. Si prende come punto di partenza il tutorial 13 di Python Engineer.</p>

<p>Invece che guardare a tutta una immagine per volta vogliamo prendere una <code class="language-plaintext highlighter-rouge">sequenza</code> di righe</p>

<p>Usiamo Architettura Many to 1 (molti input e un solo output)</p>

<p>Commentando e decommentando le parti con GRU e LSTM si ottengono tutte e 3 le architetture.
 In realta’ sono solo 2 le righe che vanno cambiate tra GRU e RNN e 3 per LSTM (si deve mettere anche la cella)!</p>

<p>Nel codice qui sotto si fa una sorta di estensione rispetto al lavoro fatto nel capitolo 13 (Feed Forward NN )</p>

<p><code class="language-plaintext highlighter-rouge">RNN</code> di torch.nn.RNN e’ una Elman: 
<script type="math/tex">\large
\displaystyle
h_t = \tanh\left( \frac{} {} 
  W_{ih} x_t +b_{ih}  + W_{hh} h_{t-1}+b_hh 
  \right)</script></p>
<ul>
  <li>$h_t$ hidden state al tempo $t$</li>
  <li>$h_{t-1}$ hidden state al tempo $t-1$ (eh… abbastanza ovvio)</li>
  <li>$x_t$ input al tempo $t$</li>
  <li>$W_{ih}$ sono i <strong>pesi</strong> che contribuiscono a $h_t$ dal tensore di input $x_t$</li>
  <li>$b_{ih}$ sono i <code class="language-plaintext highlighter-rouge">bias</code> che contribuiscono a $h_t$ dal tensore di input</li>
  <li>$W_{hh}$ sono i <strong>pesi</strong> che contribuiscono a</li>
  <li>$b_{hh}$ sono i <code class="language-plaintext highlighter-rouge">bias</code> (non chiaro perche’ vengano distinti rispetto a b_{ih}, alla fine sono delle costanti…</li>
  <li>$x_t$ vettore di input al tempo t</li>
  <li>$\hat{y}_t$ vettore di output al tempo t</li>
  <li>$y_t$ le label vere (ground truth) al tempo t</li>
</ul>

<script type="math/tex; mode=display">\large
\displaystyle
\hat y_t = 
  W_{hy} h_t +b_{hy}</script>

<p><code class="language-plaintext highlighter-rouge">Empirical Loss</code>:</p>
<ul>
  <li>Quando si ha una RNN si hanno diversi output per ognuno dei “tempi” t</li>
  <li>Si sommano i valori delle loss.</li>
</ul>

<p>ok a questo punto pero’ dovrebbero anche esserci i pesi per l’output, nella formula sopra vedo solo l’equazione per l’hidden state.</p>

<p>Vediamo un grafico per <strong>LSTM</strong>:
<img src="/images/posts/pytorch/LSTM.jpg" alt="LSTM" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="c1">#device = torch.device('cpu')
</span>
<span class="c1"># Hyper parametri 
#input_size = 28*28    #  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">128</span>   <span class="c1">#  scelto da me
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span>        <span class="c1"># singolo input e' la riga     RNN
</span><span class="n">sequence_length</span> <span class="o">=</span><span class="mi">28</span>    <span class="c1"># ci sono 28 righe             RNN
</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>         <span class="c1"># di default =1                RNN
</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># 
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">### nota che la dimensione dei sample e' la seguente
# 100  = numero di immagini nel batch (se non metti batch_size, di default vale 1)
#   1  = numero di canali solitamente i colori
#  28  =  numero di ingressi sull'asse delle x
#  28  = numero di ingressi sull'asse delle y
</span>        

<span class="c1">############  MODELLO ####################
</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>   
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>     <span class="c1"># RNN
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>   <span class="c1"># RNN
</span>        <span class="c1">#self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)  # RNN , l'ordine e' importante batch set first dimension RNN
</span>        <span class="c1">#self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # GRU 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># LSTM         
</span>        
        <span class="c1"># x -&gt; batch_size, seq, input_size
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="c1"># RNN questo e' per l'ultimo passo della sequenza per avere la classificazione
</span>       
    
    <span class="c1"># RNN da documentazione ora servono 2 input, uno e' lo stato e l'altro e' l'hidden state
</span>    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># RNN numero di layer, batch size, hidden size
</span>        
        <span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># LSTM initial cell  
</span>        
        
        <span class="c1">#out, _ = self.rnn(x, h0) # RNN restituisce 2 outputs, uno out e hidden state per step n
</span>        <span class="c1">#out, _ = self.gru(x, h0) # GRU restituisce 2 outputs, uno out e hidden state per step n       
</span>        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span><span class="n">c0</span><span class="p">))</span> <span class="c1"># LSTM restituisce 2 outputs, uno out e hidden state per step n       
</span>        
        <span class="c1"># RNN batch_size, sequence_length, hidden_size
</span>        
        <span class="c1"># RNN vogliamo l'hidden state dell'ultimo step
</span>        <span class="c1"># RNN out (N, 28, 28)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># RNN serve solo l'ultimo time step quindi metto -1 e tutte le feature dell'hidden size
</span>        <span class="c1"># RNN out(N, 128)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># RNN
</span>        <span class="k">return</span> <span class="n">out</span>
        
        

<span class="c1">############### ISTANZIO MODELLO, Loss, optimizer  ########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>  <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>                                         <span class="c1"># 
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>       <span class="c1"># 
</span><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>



<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                          <span class="c1">#   
</span>   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>   <span class="c1">#      
</span>        <span class="c1"># 100, 1, 28, 28  (batch, canali, x, y) forma del tensore images
</span>        <span class="c1"># 100, 28x28=784  forma voluta dall'hidden layer
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     <span class="c1"># Ora vogliamo solo righe e tante.
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
              
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">images</span><span class="p">)</span>        <span class="c1">#  non chiama il metodo forward: perche'?
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                 
        <span class="c1"># backward pass
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>    <span class="c1"># 
</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1"># 
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         <span class="c1"># 
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>        

            
<span class="c1">############ TEST LOOP e' identico a RNN e FeedForward! #################
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># numero di predizioni azzeccate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># ? 
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>               <span class="c1"># qui il modello e' gia' trainato!
</span>        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># prendo la classe che ha il valore massimo
</span>        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># numero di samples nel batch corrente (nell'ultimo sono diversi spesso)
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span> <span class="c1"># accuratezza in percentuale
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>            
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 1 / 2, step 100/600, loss= 0.9440
epoch 1 / 2, step 200/600, loss= 0.5394
epoch 1 / 2, step 300/600, loss= 0.3369
epoch 1 / 2, step 400/600, loss= 0.1903
epoch 1 / 2, step 500/600, loss= 0.2440
epoch 1 / 2, step 600/600, loss= 0.1913
epoch 2 / 2, step 100/600, loss= 0.1267
epoch 2 / 2, step 200/600, loss= 0.1024
epoch 2 / 2, step 300/600, loss= 0.0285
epoch 2 / 2, step 400/600, loss= 0.1911
epoch 2 / 2, step 500/600, loss= 0.1058
epoch 2 / 2, step 600/600, loss= 0.1007
accuracy =97.46
</code></pre></div></div>


      </div>
    </div>

    <div class="tag-list">
      
      
      
      <a class="tag-chip" href="/tags#python_cap"><div class="chip z-depth-1">Python</div></a>
      
      
      
      <a class="tag-chip" href="/tags#deeplearning_cap"><div class="chip z-depth-1">DeepLearning</div></a>
      
      
      
      <a class="tag-chip" href="/tags#neural-nets_cap"><div class="chip z-depth-1">Neural Nets</div></a>
      
      
      
      <a class="tag-chip" href="/tags#cuda_cap"><div class="chip z-depth-1">Cuda</div></a>
      
    </div>
    
          <style type="text/css">
.related-posts{
  border: 3px dotted #2e87e7;
  border-radius: 5px;
  margin: 20px 0;
  padding: 10px 10px 0 10px;
}
.related-posts span{
  font-size: 130%;
  font-weight: 500;
  color: #2e87e7;
}
.related-posts ul{
  margin-top: 5px!important;
}
.thi-icon{
  float: left;
  line-height: inherit;
  margin-right: 5px;
  margin-left: 2px;
  color: #2e87e7;
}
</style>

<div class="related-posts">
  <i class="material-icons thi-icon">grade</i><span>You might also like ...</span>
  
  <ul>
    
    
      
      
        
      
        
          <li>
            <a href="/Seaborn">
              Seaborn - appunti
            </a>
            <small>08 Nov 2021</small>
          </li>
          
          
    
      
      
        
      
        
          <li>
            <a href="/Matplotlib">
              Matplotlib - appunti
            </a>
            <small>07 Nov 2021</small>
          </li>
          
          
    
      
      
        
          <li>
            <a href="/Pytorch-1">
              Pytorch 1 Inizio, Tensori
            </a>
            <small>17 Oct 2021</small>
          </li>
          
          
    
      
      
        
          <li>
            <a href="/Pytorch-9">
              Pytorch 9 Autoencoder
            </a>
            <small>15 Oct 2021</small>
          </li>
          
          
    
      
        

    
    
  </ul>
</div>

    
    		
			<script src="  https://unpkg.com/showdown/dist/showdown.min.js"></script>
<script>
const GH_API_URL = 'https://api.github.com/repos/4phycs/4phycs.github.io/issues/24/comments';

let request = new XMLHttpRequest();
request.open( 'GET', GH_API_URL, true );
request.onload = function() {
	if ( this.status >= 200 && this.status < 400 ) {
		let response = JSON.parse( this.response );

		for ( var i = 0; i < response.length; i++ ) {
			document.getElementById( 'gh-comments-list' ).appendChild( createCommentEl( response[ i ] ) );
		}

		if ( 0 === response.length ) {
			document.getElementById( 'no-comments-found' ).style.display = 'block';
		}
	} else {
		console.error( this );
	}
};

function createCommentEl( response ) {
	let user = document.createElement( 'a' );
	user.setAttribute( 'href', response.user.url.replace( 'api.github.com/users', 'github.com' ) );
	user.classList.add( 'user' );

	let userAvatar = document.createElement( 'img' );
	userAvatar.classList.add( 'avatar' );
	userAvatar.setAttribute( 'src', response.user.avatar_url );

	user.appendChild( userAvatar );

	let commentLink = document.createElement( 'a' );
	commentLink.setAttribute( 'href', response.html_url );
	commentLink.classList.add( 'comment-url' );
	commentLink.innerHTML = '#' + response.id + ' - ' + response.created_at;

	let commentContents = document.createElement( 'div' );
	commentContents.classList.add( 'comment-content' );
	commentContents.innerHTML = response.body;
	// Progressive enhancement.
	if ( window.showdown ) {
		let converter = new showdown.Converter();
		commentContents.innerHTML = converter.makeHtml( response.body );
	}

	let comment = document.createElement( 'li' );
	comment.setAttribute( 'data-created', response.created_at );
	comment.setAttribute( 'data-author-avatar', response.user.avatar_url );
	comment.setAttribute( 'data-user-url', response.user.url );

	comment.appendChild( user );
	comment.appendChild( commentContents );
	comment.appendChild( commentLink );

	return comment;
}
request.send();
</script>

<hr>

<div class="github-comments">
	<h2>Comments</h2>
	<ul id="gh-comments-list"></ul>
	<div class="buttonArea">
	  <a target="_blank" href="https://github.com/4phycs/4phycs.github.io/issues/24"class="button">Add comment (via Github)</a>
	</div>
</div>


		
 
  </div>
</main>

	<script src="/assets/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript">
  jQuery(document).ready(function($){
    // browser window scroll (in pixels) after which the "back to top" link is shown
    var offset = 300,
      //browser window scroll (in pixels) after which the "back to top" link opacity is reduced
      offset_opacity = 1200,
      //duration of the top scrolling animation (in ms)
      scroll_top_duration = 700,
      //grab the "back to top" link
      $back_to_top = $('.cd-top');

    //hide or show the "back to top" link
    $(window).scroll(function(){
      ( $(this).scrollTop() > offset ) ? $back_to_top.addClass('cd-is-visible') : $back_to_top.removeClass('cd-is-visible cd-fade-out');
      // if( $(this).scrollTop() > offset_opacity ) { 
      //  $back_to_top.addClass('cd-fade-out');
      // }
    });

    //smooth scroll to top
    $back_to_top.on('click', function(event){
      event.preventDefault();
      $('body,html').animate({
        scrollTop: 0 ,
        }, scroll_top_duration
      );
    });

  });
</script>
<style type="text/css">
.cd-top {
  display: inline-block;
  height: 50px;
  width: 50px;
  position: fixed;
  bottom: 2%;
  right: 2%;
  border-radius: 40px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
  /* image replacement properties */
  overflow: hidden;
  text-indent: 100%;
  white-space: nowrap;
  background: #bbb url(/images/cd-top-arrow.svg) no-repeat center 50%;
  visibility: hidden;
  opacity: 0;
  -webkit-transition: opacity .3s 0s, visibility 0s .3s;
  -moz-transition: opacity .3s 0s, visibility 0s .3s;
  transition: opacity .3s 0s, visibility 0s .3s;
}
.cd-top.cd-is-visible, .cd-top.cd-fade-out, .no-touch .cd-top:hover {
  -webkit-transition: opacity .3s 0s, visibility 0s 0s;
  -moz-transition: opacity .3s 0s, visibility 0s 0s;
  transition: opacity .3s 0s, visibility 0s 0s;
}
.cd-top.cd-is-visible {
  /* the button becomes visible */
  visibility: visible;
  opacity: 1;
}
.cd-top.cd-fade-out {
  /* if the user keeps scrolling down, the button is out of focus and becomes less visible */
  opacity: .5;
}
.no-touch .cd-top:hover {
  background-color: #e86256;
  opacity: 1;
}
</style>

<a href="#0" class="cd-top">Top</a>
	<footer class="page-footer light-blue accent-4">  
  <div class="footer-copyright">
    <div class="container text-white">
     <a href="">4Phycs</a> &#xA9; 2021 Inherited from <a href="https://shawnteoh.github.io/matjek/">MatJeck</a>.
    </div>
  </div>
</footer>

<script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>


  
    <script src="/assets/js/post.js"></script>
  





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
  (window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>

<script src="/assets/js/main.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158698892-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158698892-1');
</script>

</body>
</html>
