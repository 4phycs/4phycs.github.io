<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href="https://fonts.googleapis.com/css?family=Maven+Pro:400,500&amp;subset=latin-ext,vietnamese" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Dancing+Script:400,700&amp;subset=vietnamese" rel="stylesheet">
  <meta name="google-site-verification" content="8zqeFQNuNAWS7ye6oN69hdEeYC_RsDyAlhht79xtAQo" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/assets/res/banner.png" />

  

  <title>
    
      DeepLearning con Pytorch | 4Phycs
    
  </title>

  

  <!-- page's cover -->
  
    <meta property="og:image" content="http://localhost:4000/images/defaultCoverPost.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1234">
    <meta property="og:image:height" content="592">
  

  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  

  <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.png">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="stylesheet" href="/assets/css/thi_scss.css">

  
    
      <link rel="stylesheet" href="/assets/css/post.css">
    
  

  

  <link rel="stylesheet" href="/assets/css/syntax.css">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
  <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
  
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="DeepLearning con Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PyTorch e Deep Learning" />
<meta property="og:description" content="PyTorch e Deep Learning" />
<link rel="canonical" href="http://localhost:4000/Pytorch-e-DeepLearning" />
<meta property="og:url" content="http://localhost:4000/Pytorch-e-DeepLearning" />
<meta property="og:site_name" content="4Phycs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-15T00:00:00+02:00" />
<meta name="google-site-verification" content="" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Pytorch-e-DeepLearning"},"description":"PyTorch e Deep Learning","@type":"BlogPosting","url":"http://localhost:4000/Pytorch-e-DeepLearning","headline":"DeepLearning con Pytorch","dateModified":"2021-10-15T00:00:00+02:00","datePublished":"2021-10-15T00:00:00+02:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
	<header>
  
    <nav class="top-nav light-blue darken-4">
  <div class="nav-wrapper">
    <div class="container">
      <a class="page-title font-title" href="/">4Phycs</a>
      <ul id="nav-mobile" class="right hide-on-med-and-down">
        <li><a href="/tags">Tags</a></li>
        <li><a href="/categories">Ita-Eng</a></li>
        <li><a href="/me">Me</a></li>
        <li><a href="/about">About</a></li>
        <li><a href="/contact">Contact</a></li>
      </ul>
    </div>
  </div>
</nav>

<div class="container">
  <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
    <i class="material-icons">menu</i>
  </a>
</div>
<div id="slide-out" class="side-nav fixed">
  <div>
    <div class="userView thi-userView">
      <div class="background"></div>
        <a href="/">
          <img style="display:inherit;" class="circle z-depth-2" src="/assets/res/user.png">
        </a>
      <span style="font-size: larger;" class="white-text name">Paolo Avogadro</span>
      <span class="white-text email"><a style="color: #bdbdbd;" href="http://"></a></span>
    </div>
  </div>
  <div style="padding: 10px;">
    <form action="/search" method="get">
      <input class="search-sidebar" type="search" name="q"  placeholder="search something?" autofocus>
      <input type="submit" value="Search" style="display: none;">
    </form>
  </div>
  <div id="toc-bar">
    <div class="toc-bar-title">
      In this post
    </div>
    <ol id="toc-sidebar">
  <li><a href="#pytorch-e-deep-learning">PyTorch e Deep Learning</a>
    <ol>
      <li><a href="#introduzione-a-queste-note">Introduzione a queste note</a></li>
      <li><a href="#disclaimer">Disclaimer</a></li>
      <li><a href="#indice-delle-lezioni-di-python-engineer">Indice delle lezioni di Python-Engineer</a></li>
      <li><a href="#altre-fonti-utili">Altre fonti utili</a></li>
      <li><a href="#lingo----utilia">Lingo -  Utilia</a></li>
      <li><a href="#domande">Domande</a></li>
      <li><a href="#pipeline">Pipeline</a></li>
      <li><a href="#overfitting">Overfitting</a></li>
    </ol>
  </li>
  <li><a href="#tensor-basics">Tensor Basics</a>
    <ol>
      <li><a href="#costruire-tensori-random-0-e-1-e-custom">Costruire tensori (Random, 0, e 1 e custom)</a></li>
      <li><a href="#attributimetodi-dei-tensori">Attributi/metodi dei tensori</a></li>
      <li><a href="#requires_gradtrue--operazioni-tra-tensori-e-slicing">requires_grad=True,  operazioni tra tensori e slicing</a></li>
      <li><a href="#importante-cambiare-la-forma-di-un-tensore-view">IMPORTANTE: Cambiare la forma di un tensore view()!</a></li>
      <li><a href="#reshape">Reshape</a></li>
      <li><a href="#numpy--tensori-e-gpu">Numpy,  Tensori e GPU</a></li>
      <li><a href="#funzioni-custom-sui-tensori">Funzioni Custom sui tensori:</a></li>
      <li><a href="#tensori-avanzato">Tensori avanzato</a></li>
    </ol>
  </li>
  <li><a href="#chainrule-e-autograd">ChainRule e Autograd</a>
    <ol>
      <li><a href="#introduzione-ai-grafi-computazionali">Introduzione ai grafi computazionali</a></li>
      <li><a href="#stop-tracking">Stop tracking</a></li>
    </ol>
  </li>
  <li><a href="#backpropagation">Backpropagation</a></li>
  <li><a href="#discesa-del-gradiente-manuale">Discesa del gradiente Manuale</a></li>
  <li><a href="#discesa-del-gradiente-autograd">Discesa del gradiente Autograd</a></li>
  <li><a href="#loss-e-optimizer-di-pytorch">LOSS e OPTIMIZER di PyTorch</a></li>
  <li><a href="#modelli-di-pytorch">Modelli di PyTorch</a></li>
  <li><a href="#dataset-e-dataloader">Dataset e DataLoader</a>
    <ol>
      <li><a href="#dataset">Dataset</a></li>
      <li><a href="#dataloader">DataLoader</a></li>
    </ol>
  </li>
  <li><a href="#dataset-transforms">Dataset Transforms</a></li>
  <li><a href="#softmax-e-cross-entropy-loss">Softmax e Cross-Entropy Loss</a>
    <ol>
      <li><a href="#cross-entropy-loss">Cross-Entropy Loss</a></li>
      <li><a href="#crossentropyloss">CrossEntropyLoss</a></li>
      <li><a href="#esempio-applicazione-di-softmax">Esempio Applicazione di Softmax</a></li>
    </ol>
  </li>
  <li><a href="#activation-function">Activation Function</a></li>
  <li><a href="#feed-forward-neural-network">Feed Forward Neural Network</a>
    <ol>
      <li><a href="#il-modello">il Modello</a></li>
      <li><a href="#criterion-e-optimizer">Criterion e Optimizer</a></li>
      <li><a href="#training-loop">Training loop</a></li>
      <li><a href="#test-loop">Test Loop</a></li>
      <li><a href="#prove-sul-modello">Prove sul modello</a></li>
      <li><a href="#prova-senza-funzioni-di-attivazione-e-un-modello-lineare">Prova senza funzioni di attivazione: e’ un modello lineare!</a></li>
    </ol>
  </li>
  <li><a href="#convolutional-neural-network">Convolutional Neural Network</a>
    <ol>
      <li><a href="#come-misurare-le-dimensioni-dei-tensori-in-uscita">Come misurare le dimensioni dei tensori in uscita</a></li>
    </ol>
  </li>
  <li><a href="#transfer-learning">Transfer Learning</a>
    <ol>
      <li><a href="#image-folder">Image Folder</a></li>
      <li><a href="#scheduler">Scheduler</a></li>
      <li><a href="#fine-tuninig">Fine Tuninig</a></li>
      <li><a href="#domande-1">Domande</a></li>
      <li><a href="#tempo">Tempo</a></li>
      <li><a href="#train-e-eval">Train() e Eval()</a></li>
    </ol>
  </li>
  <li><a href="#tensorboard">Tensorboard</a></li>
  <li><a href="#io-saving-and-loading-models">I/O Saving and Loading Models</a>
    <ol>
      <li><a href="#salvare-modelli-da-gpu">Salvare modelli da GPU</a></li>
    </ol>
  </li>
  <li><a href="#creare-e-deployare-un-modello-pytorch-con-flask">Creare e deployare un modello pytorch con Flask</a></li>
  <li><a href="#recurrent-neural-network">Recurrent Neural Network</a></li>
  <li><a href="#rnn-gru-e-lstm">RNN, GRU e LSTM</a></li>
  <li><a href="#pytorch-lightning">PyTorch Lightning</a></li>
  <li><a href="#lr-scheduler">LR Scheduler</a></li>
  <li><a href="#autoencoder">Autoencoder</a>
    <ol>
      <li><a href="#senza-convoluzioni">Senza convoluzioni</a></li>
      <li><a href="#con-convoluzioni">Con Convoluzioni</a></li>
      <li><a href="#my-playground">My Playground</a></li>
    </ol>
  </li>
</ol>
  </div>
</div>
  
</header>
<main>
  <div class="container">
    <div id="post-info">
      <h3>DeepLearning con Pytorch</h3>
      <span>
        Posted on
        <span style="display: initial;" class="cat-class">15/10/2021</span>,
        in
        
          
          
            <a class="cat-class cat-commas" href="/categories#italiano">italiano</a>.
          
        
        <span class="reading-time" title="Estimated read time">
  
  
  <font size="2"> Reading time: 206 mins </font>
  
</span>

      </span>
    </div>

    <div class="divider"></div>
    <div class="row thi-post">
      <div class="col s12">
        <h1 id="pytorch-e-deep-learning">PyTorch e Deep Learning</h1>

<h2 id="introduzione-a-queste-note">Introduzione a queste note</h2>

<p>Queste sono le mie note (Paolo Avogadro) su PyTorch e reti neurali in genere. La maggior parte del materiale e’ una traduzione delle  lezioni di <strong><a href="https://www.youtube.com/channel/UCbXgNpp0jedKWcQiULLbDTA">Python Engineer</a></strong> (Patrick Loeber):</p>

<p>Le lezioni di Python Engineer contengono dei riassunti di teoria, ma per capire a fondo il motivo di quello che viene fatto e’ bene avere una solida base di come funzionano le reti neurali. Per una buona introduzione teorica che permetta di capire meglio la logica dietro le scelte di programmazione e di modellazione delle reti neurali consiglio questo <strong><a href="https://www.youtube.com/watch?v=5tvmMX8r_OM&amp;ab_channel=AlexanderAmini">corso introduttivo</a></strong> del MIT.</p>

<p>In alcuni casi ho preso direttamente i codici di Patrick Loeber (forniti nei link alle sue lezioni), ma nella maggior parte dei casi li ho riscritti (sempre seguendo il video), quindi potrebbero esserci delle piccole differenze. Questo e’ anche dovuto al fatto che questi codici sono pensati per girare all’interno di un notebook, mentre Python Engineer usa Vstudio Code.  Per esempio, quando Patrick vuole mostrare alcuni grafici tramite Matplotlib deve lanciare dei comandi che non servono qui. 
Piu’ di una volta mi e’ capitato avere dei problemi con i codici. Spesso il motivo era una mia errata comprensione dei  video che dava luogo a degli errori non facilmente notabili,  nonostante questo segno questi errori (tra i commenti) perche’ sono utili esempi di cosa si puo’ sbagliare.</p>

<p>Il valore aggiunto portato da me riguarda principalmente 4 cose:</p>

<ol>
  <li>
    <p>Il lavoro di traduzione che mi obbliga a pensare mentre scrivo il codice. Alle volte, mantengo i termini inglesi perche’ mi consentono di ricordare le keyword e la sintassi.</p>
  </li>
  <li>
    <p>Ove lo ritengo utile aggiungo dei test e delle prove per capire meglio quello che sta succedendo.</p>
  </li>
  <li>
    <p>Ho inoltre aggiunto alcune mie considerazioni personali, utili per me per ricordare e capire meglio certe cose.</p>
  </li>
  <li>
    <p>Ho messo un contesto e una cornice iniziale di teorina e notazioni che leghi insieme le varie lezioni.</p>
  </li>
</ol>

<p>Queste note sono pensate per potere essere navigate tramite un indice interattivo. Nel Jupyter Notebook dove sono state scritte ho ottenuto l’indice tramite: <code class="language-plaintext highlighter-rouge">jupyter-navbar.</code> Ho semplicemente scaricato lo zip da https://github.com/shoval/jupyter-navbar e (dopo avere decompresso) ho fatto girare da Babun con python2.7 il file setup.py (questo perche’ lo sto facendo girare in Windows 10). In questo modo, a sinistra appare il panel con l’indice.</p>

<h2 id="disclaimer">Disclaimer</h2>
<p>Eventuali errori di queste note sono da attribuire solo a me. 
Sono appunti personali di cui non assicuro il funzionamento (o la pericolosita’).
Ci sono vari problemi di traduzine dal Jupyter notebook su cui sono gli originali e questi,
dato che in Markdown alcuni effetti non sono possibili e mancano delle immagini.</p>

<h2 id="indice-delle-lezioni-di-python-engineer">Indice delle lezioni di Python-Engineer</h2>
<p>I codici possono essere scaricati <strong><a href="https://github.com/python-engineer/pytorchTutorial">qui</a></strong>. Qui indico le lezioni di Python Engineer, la loro durata e i capitoli corrispondenti in questo notebook.</p>

<ol>
  <li><a href="https://www.youtube.com/watch?v=EMXfZB8FVUA&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;ab_channel=PythonEngineer">Istallazione</a>     5:45</li>
  <li><a href="https://www.youtube.com/watch?v=exaWOE8jvy8&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=2">Tensor Basics</a> 18:28</li>
  <li><a href="https://www.youtube.com/watch?v=DbeIqrwb_dE&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=3">Gradient Calculation con Autograd</a> 15:54</li>
  <li><a href="https://www.youtube.com/watch?v=3Kb0QS6z7WA&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=4">Backpropagation - teorie ed esempi</a> 13:13   (molto ben fatto)</li>
  <li><a href="https://www.youtube.com/watch?v=E-I2DNVzQLg&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=5">Gradient Descent con Autograd e Backpropagation</a> 17:31</li>
  <li><a href="https://www.youtube.com/watch?v=VVDHU_TWwUg&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=6">Training Pipeline: Model, Loss, e Optimizer</a>  14:16</li>
  <li><a href="https://www.youtube.com/watch?v=YAJ5XBwlN4o&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=7">Regressione Lineare</a>   12:11</li>
  <li><a href="https://www.youtube.com/watch?v=OGpQxIkR4ao&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=8">Regressione Logistica</a> 18:22</li>
  <li><a href="https://www.youtube.com/watch?v=PXOzkkB5eH0&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=9">Dataset e DataLoader - Batch Training</a> 15:27   (importante da rivedere)</li>
  <li><a href="https://www.youtube.com/watch?v=X_QOZEko5uE&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=10">Dataset Transforms</a>  10:43</li>
  <li><a href="https://www.youtube.com/watch?v=7q7E91pHoW4&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=11">Softmax e Cross Entropy</a> 18:17</li>
  <li><a href="https://www.youtube.com/watch?v=3t9lZM7SS7k&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=12">Activation Functions</a> 10:00</li>
  <li><a href="https://www.youtube.com/watch?v=oPhxf2fXHkQ&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=13">Feed-Forward Neural Network</a> 21:34</li>
  <li><a href="https://www.youtube.com/watch?v=pDdP0TFzsoQ&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=14">Convolutional Neural Network</a> 22:07</li>
  <li><a href="https://www.youtube.com/watch?v=K0lWSB2QoIQ&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=15">Transfer Learning</a> 14:55</li>
  <li><a href="https://www.youtube.com/watch?v=VJW9wU-1n18&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=16">How to use TensorBoard</a> 25:41</li>
  <li><a href="https://www.youtube.com/watch?v=9L9jEOwRrCg&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=17">Saving and loading Models</a> 18:24</li>
  <li><a href="https://www.youtube.com/watch?v=bA7-DEtYCNM&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=18">Create and Deploy A Deep Learning App - PyTorch Model Deployment with Flask</a>  41:52</li>
  <li><a href="https://www.youtube.com/watch?v=WEV61GmmPrk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=19">RNN Tutorial- Name Classification Using a Recurrent</a> 38:57</li>
  <li><a href="https://www.youtube.com/watch?v=0_PgWWmauHk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=20">RNN &amp; LSTM &amp; GRU Recurrent Neural Nets</a> 15:52</li>
  <li><a href="https://www.youtube.com/watch?v=Hgg8Xy6IRig&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=21">Lightning Tutorial Lightweight PyTorch Wrapper for ML</a> 28:02</li>
  <li><a href="https://www.youtube.com/watch?v=81NJgoR5RfY&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=22">LR Scheduler - Adjust the learning Rate for Better Results</a> 13:29</li>
</ol>

<h2 id="altre-fonti-utili">Altre fonti utili</h2>
<p>Un <strong><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">articolo</a></strong> interssante (suggerito proprio da Python Engineer) sulle RNN e’ quello di Andrej Karpathy (ora a Tesla).</p>

<p>Altri appunt utili possono essere trovati <strong><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-1/">qui</a></strong></p>

<p>Altro <strong><a href="https://www.cs.toronto.edu/~lczhang/360/">corso</a>* molto interessante (da cui Python Engineer ha preso spunto, per esempio per l’</strong>autoencoder**.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<h2 id="lingo----utilia">Lingo -  Utilia</h2>
<p>Qui metto un po’ di keyword che possono risultare utili:</p>

<ul>
  <li><strong>super()</strong>  metodo che viene usato quando si costruisce una classe ereditandola da un’altra e consente di usare i metodi della classe genitore.</li>
  <li><strong>tensor</strong>   e’ una matrice con in aggiunta dei metodi che sono propri di PyTorch, e’ il tipo fondamentale di PyTorch (il capitolo Tensor Basics e’ fatto proprio per dare una introduzione)</li>
  <li><strong>_</strong> Un metodo il cui nome termina con un underscore vuole dire che lavora <strong>inplace</strong></li>
  <li><strong>Dataset</strong>  e’ una classe di <em>torch.utils.data</em> dove viene messo il dataset che serve alla rete neurale</li>
  <li><strong>DataLoader</strong> e’ una classe di <em>torch.utils.data</em>, serve per dividere il dataset in batch da dare in pasto alla rete</li>
  <li><strong>epoch</strong> <br /> un passo forward e un backward di <strong>TUTTI</strong> i campioni del training</li>
  <li><strong>batch_size</strong> numero di campioni di training in un forward/backward pass</li>
  <li><strong>numero di iterazioni</strong> numero di passi, dove in ogni passo(forward/backward) si usa un batch di campioni ( di dimensione “batch_size”)</li>
  <li><strong>criterion</strong> e’ il <em>criterio</em> (la funzione) che usiamo per generare la Loss (per esempio, cross_entropy).Nota che la loss e’ il singolo valore ottenuto (in genere), mentre il criterio e’ il tipo di funzione che, ottenuti come argomenti i valori predetti e quelli corretti fornisce il valore. In queste note uso in modo “liberale” i termini loss e criterion, normalmente questo non dovrebbe creare problemi.</li>
  <li><strong>learning rate</strong> in generale si ottimizzano i pesi della rete usando una tecnica tipo <strong>discesa del gradiente</strong>. La loss e’ una funzione da $R^n \rightarrow R$ (dove n e’ il numero di pesi usati). Se calcolo il gradiente allora conosco la massima pendenza e mi posso muovere lungo quella direzione per cercare il minimo (ma nel verso opposto). Di quanto mi muovo? la grandezza di questo passo verso il possibile minimo e’ data dal learning rate. Il prolema di come variare il learning rate e’ fondamentale per poter ottenere delle buone convergenze.</li>
  <li><strong>Grafo computazionale</strong>, immagina di mettere tutte le operazioni fatte per ottenere i risultati della rete neurale. In pratica stai costruendo una funzione $R^n \rightarrow R^m$. Questa funzione puo’ essere vista come una serire di passi, ognuno indipendente dall’altro. Per esempio se hai una rete neurale con vari hidden layer, ogni passaggo ad un layer e’ diverso, ci sono poi delle funzioni non lineari applicate ecc. La tua funzione Loss e’ quindi una funzione di funzione:F(x) = f(g(h(x)))  (ho messo solo 3 funzioni per esempio ma sono in genere di piu’). Quando vorrai calcolare il gradiente rispetto ai parametri dovrai usare una chain rule e spesso questo viene visualizzato come un grafo con vari passi.</li>
</ul>

<h2 id="domande">Domande</h2>

<ul>
  <li>viene piu’ volte consigliato di non fare fare la somma quando si fa backpropagation perche’ viene fatta in automatico.
Ci sono vari modi per evitare questo (devo indicare quali sono i video).</li>
</ul>

<h2 id="pipeline">Pipeline</h2>
<p>Lo scopo e’ costruire un codice tramite Pytorch che impari a fare qualcosa. Qui sotto indico la pipeline (la serie di passi) che serve per ottenere questo risultato. Attenzione: con il wrapper <code class="language-plaintext highlighter-rouge">Pytorch-Lightning</code>, alcuni di questi passi possono essere saltati e diventa quindi piu’ semplice ottenre un modello funzionante.</p>

<ul>
  <li>si importano i dati in <code class="language-plaintext highlighter-rouge">dataset</code> (sia per il training che per il test)</li>
  <li>i dataset vengono trasformati per migliorarne le caratteristiche tramite delle funzioni <code class="language-plaintext highlighter-rouge">transformations</code> (per esempio si possono normalizzare le informazioni)</li>
  <li>si costruisce i <code class="language-plaintext highlighter-rouge">dataloader</code> per dare al modello dei batch (sia per train che per test)</li>
  <li>si eredita un <code class="language-plaintext highlighter-rouge">nn.Model</code> (ricordati di mettere il <em>super</em>) dove vengono inseriti i vari strati della rete nella funzione <code class="language-plaintext highlighter-rouge">__init__</code>.</li>
  <li>nel modello si inserisce anche un metodo <code class="language-plaintext highlighter-rouge">forward</code> (ATTENTO il nome deve porprio essere <code class="language-plaintext highlighter-rouge">forward</code>, si sta facendo un overload di un metodo gia’ esistente in nn.Model!) dove vengono proprio implementati i passi uno dopo l’altro. Questo e’ il cosiddetto <code class="language-plaintext highlighter-rouge">grafo computazionale</code></li>
  <li>nota che il modello ereditato e’ “callable” ovvero posso usarlo come una funzione a cui do in pasto qualcosa… in pratica i batch di dati.</li>
  <li>si istanzia il modello passando solo <code class="language-plaintext highlighter-rouge">pochi parametri</code> come: dimensione input, dimensione hidden e dimensione output!</li>
  <li>Quando si chiama l’istanza del nostro modello personalizzato inserendo un batch, viene chiamata la funzone <code class="language-plaintext highlighter-rouge">forward</code> a cui e’ passato il batch.</li>
  <li>si fa un ciclo esterno sulle Epoche (ogni epoca e’ divisa in batch)</li>
  <li>si fa un ciclo interno su tutti i batch (infornate) di ogni epoca.</li>
  <li>si istanzia una funzione chiamata <code class="language-plaintext highlighter-rouge">criterion</code> (spesso chiamiamo l’istanza proprio <code class="language-plaintext highlighter-rouge">criterion</code>) che viene usata per ottenre la loss. Il criterio e’ la forma generale della loss (per esempio cross-entropy), mentre la Loss e’ l’istanza particolare associata al criterio.</li>
  <li>la <code class="language-plaintext highlighter-rouge">loss</code> e’ una funzione sia delle predizioni $\hat y$ che dei valori noti $y$. Si usano delle notazioni che rimandano con precisione alle funzioni e gli stimatori, per esempio l’input e’ dato dalla $x$, l’output e’ dato dalla $\hat{y}$ (questa scrittura assomiglia a quella di uno stimatore di un’osservabile di una distribuzione)
Il risultato di applicare il criterion a questi dati produce un numero (la loss) che quantifica la qualita’ della predizione. Nota che a questo punto ho una fun</li>
  <li><code class="language-plaintext highlighter-rouge">empirical loss</code> e’ la <strong>media</strong> delle varie loss ottenute da un batch, in pratica quindi la discesa del gradiente viene fatta sull’empirical loss</li>
  <li><code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> serve per evitare che tutte le azioni (per esempio l’optimizer) vengno considerate parte del grafo computazionale</li>
  <li><code class="language-plaintext highlighter-rouge">loss.backward()</code>    fa la backpropagation in modo da ottenere i gradienti rispetto ai pesi (stiamo cercando un minimo rispetto della loss dove le variabili sono i pesi)</li>
  <li><code class="language-plaintext highlighter-rouge">optimizer.step()</code>   e’ il modo in cui ci si muove (con un passo di grandezza <code class="language-plaintext highlighter-rouge">learning_rate</code>) sul landscape dato dalla loss per cercare il minimo, per esempio usando la tecnica chiamata SGD (stochastic gradient descent).</li>
</ul>

<p>Una osservazione sui batch (supportata dalla prima lecture del MIT intorno al min 47).
Cosa significa dare in pasto un batch alla mia (feed forward) rete neurale?
Immagina la rete neurale smplicemente come una funzione di:</p>
<ul>
  <li>${\bf x_i}$ : sono gli input, per esempio i pixel di una foto. Nel seguito supporro’ che sia una sola variabile</li>
  <li>${\bf w}$ i <em>pesi</em>,  anche qui per semplicita’ si ha un  solo peso.</li>
</ul>

<p>Con i valori in uscita (output) e i valori veri (noti nel training set) otteniamo una funzione di Loss.
Questa funzione e’ $\displaystyle L = f(x,w)$</p>

<p>A questo punto pensa al modello piu’ semplice del mondo in cui ho un solo peso $w$.  in questo caso $L = x \cdot w^2$ (nota che i pesi possono entrare in modo non lineare). Se passo 2 vettori di input diversi (per esempio 2 immagini) allora ho 2 funzioni di loss diverse:</p>
<ul>
  <li>$L(x_1,w) = x_1 \cdot w^2 +x_1\cdot w$  (una quadratica in funzione di y, dove $x_1$ e’ un prametro)</li>
  <li>$L(x_2,w) = x_2 \cdot  w^2 +x_2\cdot w$</li>
</ul>

<p>Noi ora cambiamo prospettiva, dato che vogliamo minimizzare la loss in funzione dei pesi, considero ora:</p>
<ul>
  <li>$x_i$ sono i parametri</li>
  <li>w  sono le variabili</li>
</ul>

<p>La <code class="language-plaintext highlighter-rouge">average loss</code>: $L=L_1 +L_2$ e’ ora una funzione di $y$.
Per trovare il minimo, uno dei modi piu’ interessanti e’ muoversi nella direzione di massima pendenza (gradient) verso valori piu’ bassi (basta ricordare le utilissime note di Valentina di analisi 2 sulle approssimazioni lineari di funzioni da $R^n \rightarrow R$. In questo caso calcolo la derivata parziale della loss rispetto al peso: $\frac{\partial}{\partial w}$. Se i pesi sono tanti, allora calcolo il gradiente e ottengo quindi una direzione verso cui muovermi.</p>

<p>In questo esempio banale le loss sono due parabole centrate in zero e non e’ interessante, il minimo si ottiene mettendo $w=0$. Se prendiamo un caso appena piu’ complicato, dove la loss e’ la somma di due parabole non centrate in zero otteniamo una curva con vari minimi.  Nota che il profilo della funzinoe <code class="language-plaintext highlighter-rouge">Loss empirica</code> non e’ identico a quello della loss del singolo imput e noi siamo intressati ad un minimo globale per tutto il batch.</p>

<h2 id="overfitting">Overfitting</h2>

<p>Come evitare l’overfitting durange la fase di training? I casi reali si riferiscono a delle funzioni che sono multidimensionali con una dimensionalita’ enorme (migliaia o centinaia di migliaia di parametri). Questo implica che la superficie su cui facciamo la minimizzazione, data dalla Loss avra’ molti minimi locali. Noi siamo interessati ad un minimo globale che non abbia una forte dipendenza dagli input iniziali, ma vada bene per un vasto range di casi.</p>

<p>Per evitare l’overfitting per esempio ci sono delle regolarizzazioni:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Regularization 1</code>: metodo <strong>dropout</strong>, si spengono random dei neuroni (guarda 5.3)
https://www.youtube.com/watch?v=5tvmMX8r_OM&amp;t=1008s&amp;ab_channel=AlexanderAmini
In ogni iterazione si fanno dei dropout differenti (scelti in modo random) in modo da ottenere diversi percorsi “neuronali”. Avendo spento alcuni dei neuroni diventa anche piu’ facile fare il training in quanto il numero di parametri per la backpropagation diminiuisce.
-<code class="language-plaintext highlighter-rouge">Regularization 2</code>: <strong>Early Stopping</strong>, fermare il fit dei parametri prima che raggiunga il minimio. Se si guardano le curve che indicano l’errore del training e della validation, al crescere delle iterazioni tendono a scendere. La curva del training pero’ continua a scendere anche quando la validazione ha smesso di scendere. A quel punto sto overfittando.</li>
</ul>

<h1 id="tensor-basics">Tensor Basics</h1>
<p>Gli oggetti chiave di PyTorch sono i <strong>tensor</strong>.</p>

<ul>
  <li>un tensor non e’ un <strong>tensore</strong> della matematica (funzionale lineare, con proprieta’ di trasformazione)!</li>
  <li>un tensor e’ una matrice di dimensione variabile (1D, 2D, 3D, …)</li>
  <li>un tensor ha associati dei metodi particolari che servono durante il training di un neural network</li>
  <li>le convenzioni sono simili a quelle di Numpy (per esempio riguardo lo <strong>slicing</strong>)</li>
  <li>E’ spesso utile cambiare la <strong>forma</strong> del tensore (per esempio con il metodo <strong>.view()</strong>)</li>
  <li>E’ spesso utile cambiare il <strong>tipo</strong> degli oggetti contenuti nel tensore <strong>dtype=torch.float16</strong>. Nota che Torch ha i suoi tipi.</li>
</ul>

<p>comandi utili:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># scalar   NON inizializzato
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>       <span class="c1"># vector, 1D
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>     <span class="c1"># matrice 2D con 2 righe e 3 colonne
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>   <span class="c1"># matrice 3D 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># matrice 4D 
</span></code></pre></div></div>

<h2 id="costruire-tensori-random-0-e-1-e-custom">Costruire tensori (Random, 0, e 1 e custom)</h2>
<ul>
  <li><strong>torch.empty(5,3)</strong> per avere un tensore <strong>NON</strong> inizializzato</li>
  <li><strong>torch.rand(5,3)</strong> per costruire un tensore pieno di numeri <strong>Random</strong> U (0,1)</li>
  <li><strong>torch.zeros(5,3)</strong>  per avere un tensore pieno di <strong>0</strong></li>
  <li><strong>torch.ones(5,3)</strong> per avere un tensore pieno di <strong>1</strong></li>
  <li><strong>torch.tensor([1,2,3])</strong> per creare un tensore, a partire da una <code class="language-plaintext highlighter-rouge">lista</code></li>
  <li><strong>x.size()</strong> ci dice la forma del tensore (numero di righe, colonne, ecc)</li>
  <li><strong>dtype=torch.float16</strong> per esempio  <strong>float32</strong> default</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>                       <span class="c1"># numeri RANDOM intervallo [0,1]
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>                      <span class="c1"># zeri
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>                       <span class="c1"># gli ingressi sono tutti uno
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="c1"># specifico il tipo
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># con questa scrittura si inizializza il tensore inserendo i valori in una lista. Questo
</span>                            <span class="c1"># determina automaticamente anche il numero di righe e colonne
</span></code></pre></div></div>

<h2 id="attributimetodi-dei-tensori">Attributi/metodi dei tensori</h2>
<ul>
  <li>x.size()           numero di <strong>righe</strong> e <strong>colonne</strong></li>
  <li>x.dtype            <strong>tipo</strong> degli oggetti contenuti</li>
  <li>x[1]                    fai uno <strong>slicing</strong> ottieni un <strong>TENSORE</strong></li>
  <li>x[1].item()        <strong>estrai</strong> un valore: otteni un <strong>FLOAT</strong> (o quello che e’ il tipo degli oggetti nel tensore)</li>
  <li>x.mean()           e’ un metodo che calcola la media di TUTTI gli ingressi (non importa se il tensore e’ 2D, ottieni uno scalare)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="p">)</span>         <span class="c1"># dimmi il numero di righe e colonne  
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="p">)</span>          <span class="c1"># dimmi il tipo degli oggetti contenuti 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>              <span class="c1"># questo e' un TENSORE
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>       <span class="c1"># questo e' un FLOAT
</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>        <span class="c1"># infatti...
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([2])
torch.float32
tensor(3.)
3.0





float
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">g</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(3.7500) torch.Size([2, 4])
</code></pre></div></div>

<h2 id="requires_gradtrue--operazioni-tra-tensori-e-slicing">requires_grad=True,  operazioni tra tensori e slicing</h2>

<ul>
  <li><strong>requires_grad=True</strong> se si inserisce questo argomento nella creazione di un tensore, allora, durante il processo di ottimizzazione PyTorch calcolera’ il gradiente (derivata parziale rispetto a questo tensore). Nota che questa opzione e’ accesa di default</li>
</ul>

<p>Le <code class="language-plaintext highlighter-rouge">operazioni</code> di base tra tensori sono ottenute con dei metodi indicati con <strong>3</strong> lettere, <strong>minuscole</strong>:  <br /></p>
<ul>
  <li>torch.sub(x,y)     oppure       - (fa la sottrazione tra x e y e la restituisce)</li>
  <li>torch.add()         oppure       +</li>
  <li>torch.div()         oppure       /</li>
  <li>torch.mul()         oppure       *</li>
  <li><strong>requires_grad=True</strong> se voglio che PyTorch calcoli il gradiente (derivata parziale) rispetto a questo tensore rispetto al grafo computazionale.</li>
  <li>tutti i casi in cui il metodo finisce con un underscore _ lavorano <strong>INPLACE</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># costruiamo 2 tensori random 2D
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># costruiamo 2 tensori random 2D
</span>
<span class="c1"># ADDIZIONI 
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                   <span class="c1"># INPLACE   &lt;=====================
</span>

<span class="c1"># SOTTRAZIONI
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># MOLTIPLICAZIONI
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># DIVISIONI
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Slicing restituisce dei sotto-tensori (ma sempre di tipo tensor)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># tutte le righe, colonna 0
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>  <span class="c1"># riga 1, tutte le colonne 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>   <span class="c1"># elemento  1, 1
</span>
<span class="c1"># Se voglio ottenere il valore di un ingresso devo usare il metodo .item() (vale per 1 solo valore)
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.7356, 0.5790, 0.3409],
        [0.1011, 0.1175, 0.8874],
        [0.5814, 0.6688, 0.1503],
        [0.7797, 0.6233, 0.0940],
        [0.5445, 0.1418, 0.8160]])
tensor([0.7356, 0.1011, 0.5814, 0.7797, 0.5445])
tensor([0.1011, 0.1175, 0.8874])
tensor(0.1175)
0.11745560169219971
</code></pre></div></div>

<h2 id="importante-cambiare-la-forma-di-un-tensore-view">IMPORTANTE: Cambiare la forma di un tensore view()!</h2>
<p>con il metodo <strong>view</strong> si cambia la forma di un tensore. Questo e’ particolarmente utile quando si fanno per esempio le reti convoluzionali. In una fully connected layer io vedo l’ingresso come un verttore 1D. Se faccio la convoluzione devo ridare una forma 2D.</p>

<ul>
  <li>con <strong>view(3,4)</strong> cambio la forma. Semplicemente si mette il numero di righe (3 nell’esempio) e colonne(4 nell’esempio) voluto!</li>
  <li><strong>NON</strong> lavora inplace</li>
  <li>il valore <strong>-1</strong> e’ un jolly: torch automaticamente determina il numero di righe (per esempio) se io scrivo solo il numero di colonne. Per esempio <strong>x.view(-1,8)</strong> allora torch mettera’ come numero di colonne, il numero di ingressi diviso per 8.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>                 <span class="c1"># costruisco un tensore2D: 4x4 
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>                        <span class="c1"># lo trasformo in 1D: 16x1
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>                     <span class="c1"># Voglio ora un tensore2D, con 8 colonne a partire da quello di prima 
</span>                                      <span class="c1"># il -1 indica che in questa dimensione sceglie torch AUTOMATICAMENTE!
</span><span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>    
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([2, 2, 2, 2])
</code></pre></div></div>

<h2 id="reshape">Reshape</h2>
<p>questo e’ un altro metodo che serve per modificare la forma di un tensore. 
Reshape <strong>puo’</strong> resituire sia una <strong>copy</strong> che una <strong>view</strong></p>

<h2 id="numpy--tensori-e-gpu">Numpy,  Tensori e GPU</h2>
<p>vediamo come passare da un <code class="language-plaintext highlighter-rouge">ndarray</code> (Numpy) ad un tensore e viceversa.</p>

<ul>
  <li><strong>.numpy()</strong> e’ un <strong>metodo di torch</strong> per trasformare un tensor $\rightarrow$ ndarray (di numpy). <strong>ATTENZIONE</strong> usando questo metodo cosi’: <em>b = torch.numpy(a)</em>
si ottiene b, che e’ un ndarray, ma i suoi valori sono presi da a. NON e’ un oggetto completamente nuovo. Se modifico “a”, anche “b” cambia!</li>
  <li><strong>.from_numpy()</strong> e’ un <strong>metodo di torch</strong> per trasformare un ndarray $\rightarrow$ tensor</li>
  <li><strong>attenzione</strong> se il tensore e’ sulla GPU e lo trasformo in NUMPY anche il trasformato resta sulla GPU</li>
  <li>esiste un oggetto che dice dove deve essere il tensore: <strong>device=torch.device(“cuda”)</strong>, nota che il nome scelto in questo caso serve a ricordarci che l’argomento perche’ e’ identico</li>
  <li><strong>.to()</strong> per muovere un tensore da un posto all’altro basta il metodo: <strong>x=x.to(device)</strong></li>
  <li>numpy non e’ in grado di gestire tensori sulla GPU</li>
  <li><strong>torch.cuda.is_available()</strong> per sapere se CUDA e’ disponibile, e’ BOOL</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Numpy
# Convertiamo un TENSORE in un ndarray di Numpy
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>    <span class="c1"># a e' un TENSORE (Torch)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>        <span class="c1"># b e' un ndarray (Numpy)  (occhio che  .numpy e' un metodo dei TENSORI)
</span>
<span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>            <span class="c1"># cambiamo a
</span><span class="k">print</span><span class="p">(</span><span class="s">'a='</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>            
<span class="k">print</span><span class="p">(</span><span class="s">'b='</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>        <span class="c1"># anche b e' cambiato &lt;================ ATTENTO
</span>


<span class="c1"># numpy to torch with .from_numpy(x)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c1"># again be careful when modifying
</span><span class="n">a</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c1"># by default all tensors are created on the CPU,
# but you can also move them to the GPU (only if it's available )
</span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>          <span class="c1"># a CUDA device object
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Cuda e' disponibile!!</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># directly create a tensor on GPU
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># or just use strings ``.to("cuda")``
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c1"># z = z.numpy() # not possible because numpy cannot handle GPU tenors
</span>    <span class="c1"># move to CPU again
</span>    <span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>       <span class="c1"># ``.to`` can also change dtype together!
</span>    <span class="c1"># z = z.numpy()
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a= tensor([2., 2., 2., 2., 2.])
b= [2. 2. 2. 2. 2.]
[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
Cuda e' disponibile!!
</code></pre></div></div>

<h2 id="funzioni-custom-sui-tensori">Funzioni Custom sui tensori:</h2>
<p>guarda la risposta di  msd15213 al link:
<strong><a href="https://stackoverflow.com/questions/46509039/pytorch-define-custom-function">link</a></strong></p>

<p>molto meglio <strong><a href="https://stackoverflow.com/questions/55765234/pytorch-custom-activation-functions">questo</a></strong></p>

<h2 id="tensori-avanzato">Tensori avanzato</h2>
<ul>
  <li>un tensore puo’ essere <strong><a href="https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092">contiguous</a></strong></li>
</ul>

<h1 id="chainrule-e-autograd">ChainRule e Autograd</h1>
<p>Il pacchetto <code class="language-plaintext highlighter-rouge">Autograd</code> fornisce differenziazione automatica per le operazioni (funzioni) sui tensori:<br />
<strong>requires_grad=True</strong> <br />
Immagina un <strong>tensore</strong> come una semplice variabile (multidimensionale) che entra in un grafo computazionale. Alla fine del grafo ho uno scalare (in genere) e voglio sapere come dipende questo scalare dal un particolare tensore, allora devo usare Autograd.
<script type="math/tex">\displaystyle
L(z(x)): R^n \rightarrow R</script></p>

<h2 id="introduzione-ai-grafi-computazionali">Introduzione ai grafi computazionali</h2>
<p>Per esempio:</p>
<ul>
  <li>costruisco un tensore x  (1D con 3 ingressi random)</li>
  <li>costruisco un tensore y funzione di x: <strong>y=x+2</strong></li>
  <li>costruisco un tensore z funzione di y: <strong>z= 3y$^2$</strong></li>
  <li>ATTENTO il gradiente si puo’ calcolare solo se alla fine si hanno dei valori SCALARI (altrimenti ho un numero di gradienti pari alle componenti del vettore). La logica e’ chiara, alla fine io voglio vedere come varia una funzione di LOSS rispetto ai parametri che metto nella rete neurale. La LOSS e’ una funzione scalare e quindi non sono state implementate delle variazioni per funzioni vettoriali.</li>
  <li>calcolo quindi il valore medio di <strong>u=&lt;z&gt;</strong> (per avere uno scalare)</li>
</ul>

<script type="math/tex; mode=display">{\bf x}=
\left(
\begin{eqnarray}
 x_1 \\
 x_2 \\
 x_3
\end{eqnarray}
\right)</script>

<p><script type="math/tex">{\bf y}=
\left(
\begin{eqnarray}
 y_1 \\
 y_2 \\
 y_3
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 x_1+2 \\
 x_2+2 \\
 x_3+2
\end{eqnarray}
\right)</script></p>

<p><script type="math/tex">{\bf z}
=
\left(
\begin{eqnarray}
 3y_1^2 \\
 3y_2^2 \\
 3y_3^2
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 3(x_1+2)^2 \\
 3(x_2+2)^2 \\
 3(x_3+2)^2
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 3(x_1^2+4x_1+4) \\
 3(x_2^2+4x_2+4) \\
 3(x_3^2+4x_3+4)
\end{eqnarray}
\right)</script></p>

<script type="math/tex; mode=display">\displaystyle
u = \frac{1}{3}\sum_{i=1}^3z_i = \frac{1}{3} \sum 3y_i^2  =  \sum y_i^2 \mbox{  Derivata parziale}  \Rightarrow  \frac{\partial u}{\partial y_k} = 2 y_k</script>

<p>Se voglio conoscere la dipendenza di ${\bf u}$ da parte di ${\bf x}$, dal punto di vista matematico devo calcolare la derivata parziale di u rispetto a x:</p>

<p><script type="math/tex">\displaystyle \frac{ \partial u } {\partial x_j} = \frac{\partial u}{\partial y_i} \frac{\partial y_i}{\partial x_j}</script> (indici ripetuti sono sommati)</p>

<script type="math/tex; mode=display">\displaystyle
 \frac{\partial u}{\partial y_k} = 2 y_k = 2(x_k+2)</script>

<script type="math/tex; mode=display">\displaystyle
 \frac{\partial y_k}{\partial x_j} = \delta_{kj}</script>

<p>Quindi:
<script type="math/tex">\displaystyle \frac{ \partial u } {\partial x_k} = 2(x_k+2)</script></p>

<p>Se il valore del tensore ${\bf x_0} = (1, 1, 1)$, alora il gradiente rispetto alla variaibile x della funzione u e’ un vettore che vale:</p>

<p><script type="math/tex">\nabla_x u |_{x_0} = 
=
\left(
\begin{eqnarray}
 2(x_1+2) \\
 2(x_2+2) \\
 2(x_3+2)
\end{eqnarray}
\right)</script>
<script type="math/tex">=
\left(
\begin{eqnarray}
 6 \\
 6 \\ 
 6 
\end{eqnarray}
\right)</script></p>

<p>Pensala cosi’: C’e’ una funzione di molte variabili che vengono combinate passo passo. Queste variabili pensale come proprio i pesi della rete neurale. Alla fine noi vogliamo minimizzare la <strong>LOSS</strong>. Quindi prendiamo il gradiente per trovare la pendenza massima e scendiamo lungo il gradiente di queste variabili con piccoli passi, sperando di raggiungere un buon minimo (occhio che la cosa non e’ garantita banalmente in quanto non siamo in un caso semplice di un solo massimo, potremmo finire in un minimo locale!).</p>

<p><strong>Attenzione</strong></p>

<p>Quando si fa il <strong>.backward()</strong> i valori dei gradienti vengono ACCUMULATI nell’attributo <strong>.grad</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>   <span class="c1">#  x = [x_1, x_2, x_3] = [1, 1, 1]
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>                               <span class="c1">#  y = [y_1, y_2, y_3]      
</span>
<span class="c1">##### Occhio y e' funzione di x, che ha requires_grad=True. Quindi ha come attributo grad_fn
</span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>                           <span class="c1">#  z = [3 y_1^2, 3 y_2^2, 3 y_3^2] Lavorano sul singolo ingresso!
</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                            <span class="c1">#  zmean = 1/3 ( 3 y_1^2 +  3 y_2^2 + 3 y_3^2 )      calcolo la media
</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                            <span class="c1"># back propagation 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>                           <span class="c1"># dz/dx = dz/ dy * dy/dx CALCOLATO nel valore corrente delle x
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;AddBackward0 object at 0x000001EE446B2BB0&gt;
tensor([6., 6., 6.])
</code></pre></div></div>

<p>Se l’output non e’ uno scalare si devono specificare gli argomenti per il <em>metodo</em> <strong>.backward()</strong>, non mi e’ chiaro come questi argomenti vengano usati.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#x = torch.randn(3, requires_grad=True)     # tensore 1D
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>     <span class="c1"># tensore 1D
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>                                  <span class="c1"># altro tensore 1D
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>                        
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>                              <span class="c1"># y * y * y * ... * y (10 volte +1 del passo precedente)  = x * 2**11
</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>         <span class="c1"># qui ho specificato che voglio il gradiente rispetto a ... v? non chiaro forse fa derivata direzionale
</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([2252.8000, 2252.8000, 2252.8000], grad_fn=&lt;MulBackward0&gt;)
torch.Size([3])
tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])
</code></pre></div></div>

<h2 id="stop-tracking">Stop tracking</h2>
<ul>
  <li>Supponiamo di volere fare un’update dei pesi durante il loop del training.</li>
  <li>questo implica fare delle nuove funzioni sui pesi (le update), e quindi quando si fa  la back propagation si rischia che questa tenga conto anche delle update! Bisogna quindi dire al TENSORE di non tenere conto delle update. Ovvero si deve dire al TENSORE che deve essere tracciato <strong>solo</strong> lungo il <strong>network computazionale</strong></li>
</ul>

<p>(<strong>non del tutto chiaro devo fare esperimenti</strong>)</p>

<ul>
  <li><strong>x.requires_grad_(False)</strong>  (nota l’underscore <strong>_</strong> finale per INPLACE)</li>
  <li><strong>x.detach()</strong></li>
  <li>wrap in <strong>with torch.no_grad():</strong></li>
</ul>

<p>Se si usa il metodo <strong>.zero_()</strong> questo riempie il gradiente prima di un nuovo passo di ottimizzazione.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>         <span class="c1"># qui NON accendiamo il requires_grad
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>        <span class="c1"># e appunto se controlliamo da': False
</span><span class="n">b</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>       <span class="c1"># costruisco un nuovo Tensore b
</span><span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>              <span class="c1"># e per qusto non c'e' l'attributo grad_fn che indica che c'e' una gradiente
</span>
<span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>        <span class="c1"># accendiamo INPLACE(_) il gradiente  
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>        <span class="c1"># ora il risultato e' True
</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>             <span class="c1"># creiamo uno scalare b con sum() fa la somma del Tensore. 
</span><span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>              <span class="c1">#  
</span>
<span class="c1"># .detach(): get a new Tensor with the same content but no gradient computation:
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="c1"># wrap in 'with torch.no_grad():'
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span> <span class="c1"># qui ho fatto un'altra funzione con x ma non contribuisce al gradiente!
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>False
None
True
&lt;SumBackward0 object at 0x000001A0C4C411F0&gt;
True
False
True
False
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -------------
# backward() accumulates the gradient for this tensor into .grad attribute.
# !!! We need to be careful during optimization !!!
# Use .zero_() to empty the gradients before a new optimization step!
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># just a dummy example
</span>    <span class="n">model_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">model_output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

    <span class="c1"># optimize model, i.e. adjust weights...
</span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>                 <span class="c1"># quando faccio l'ottimizzazione dei valori del tensore
</span>        <span class="n">weights</span> <span class="o">-=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span>     <span class="c1"># non voglio che facciano parte del grafo computazionale!  
</span>
    <span class="c1"># this is important! It affects the final weights &amp; output
</span>    <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>   <span class="c1"># se non azzeri c'e' accumulo (?)
</span>
<span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>

<span class="c1"># Optimizer has zero_grad() method
# optimizer = torch.optim.SGD([weights], lr=0.1)
# During training:
# optimizer.step()
# optimizer.zero_grad()
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)
tensor(4.8000, grad_fn=&lt;SumBackward0&gt;)
</code></pre></div></div>

<h1 id="backpropagation">Backpropagation</h1>
<p>un esempio semplice di backpropagation.</p>
<ul>
  <li>costruisco un tensore 0D x=1  (e’ il <code class="language-plaintext highlighter-rouge">predictor</code>)</li>
  <li>costruisco un tensore 0D y=2  (e’ la funzione obiettivo)</li>
  <li>costruisco un tensore 0D w=1  (sono i pesi che voglio ottimizzare)</li>
  <li>calcolo le y_predicted =w*x</li>
  <li>calcolo la LOSS (y_predicted-y)$^2$  (tutto questo e’ il forward pass)</li>
  <li>calcolo la BACKPROPAGATION (stando attento a non farla entrare nel grafo computazionale)</li>
  <li>azzero i gradienti e ripeto varie epoche</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>                          <span class="c1"># costruisco un tensore 0D (1 oggetto): i predictors
</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>                          <span class="c1"># un altro tensore 0D:                  la risposta ESATTA
</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>      <span class="c1"># questo tensore ha accesa la condizione requires_grad, i PESI
</span>
<span class="c1"># FORWARD PASS
</span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># costruisco un grafo computazionale, ora y =w*x, la risposta CALCOLATA 
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># la funzione di LOSS 
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># BACKWARD PASS dLoss/dw                       # calcolo la dipendenza della LOSS in funzione dei PESI
</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                               
<span class="c1">#print(w.grad)
</span>
<span class="c1"># A questo punto voglio fare una update dei PESI per cercare di fare predizioni migliori
</span>
<span class="c1"># 
# l'update dei PESI NON deve entrare nel grafo computazionale
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>      <span class="c1"># mi muovo lungo la direzione di massima crescita... al negativo di un passetto
</span>
<span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>              <span class="c1"># NON dimenticare di azzerare i gradienti 
</span>

<span class="c1"># FORWARD PASS
</span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># nuovo forward pass 
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># nuova funzione di LOSS 
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1">############# faccio ora un ciclo ################
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>      <span class="c1"># mi muovo lungo la direzione di massima crescita... al negativo di un passetto
</span>
    <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>              <span class="c1"># AZZERO i gradienti
</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                            <span class="c1"># nuovo forward pass 
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># nuova funzione di LOSS 
</span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                                <span class="c1"># backward! se non lo faccio il gradiente e' stato azzerato! 
</span>    <span class="k">if</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="o">==</span><span class="n">epoch</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(1., grad_fn=&lt;PowBackward0&gt;)
tensor(0.9604, grad_fn=&lt;PowBackward0&gt;)
tensor(0.9604, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.6412, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.4281, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.2858, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.1908, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.1274, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0850, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0568, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0379, grad_fn=&lt;PowBackward0&gt;) 99
tensor(0.0253, grad_fn=&lt;PowBackward0&gt;) 99
</code></pre></div></div>

<h1 id="discesa-del-gradiente-manuale">Discesa del gradiente Manuale</h1>
<p>proviamo ora con un esempio 1D (prima era 0D) con una regressione lineare.</p>

<ul>
  <li>i vari passi vengono calcolati MANUALMENTE senza usare Torch</li>
  <li>costruisco una funzione che fa il forward pass</li>
  <li>costruisco una funzione che fa il backward pass</li>
  <li>calcolo il gradiente (senza autograd)</li>
</ul>

<p>La cosa interessante e’ che qui ho un array in ingresso.
Prendo tutti i valori dell’array di ingresso e con essi faccio il training tutti insieme (calcolo infatti la LOSS su tutti).
Poi il passo forward lo faccio su uno scalare! per vedere se la predizione funziona</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="c1"># Regressione Lineare 
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># PREDICTORS
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># OBIETTIVO
</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.0</span>                                        <span class="c1"># pesi (ma non e' un tensore...) 
</span>
<span class="c1"># MODEL OUTPUT 
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                               <span class="c1"># FORWARD PASS
</span>
<span class="c1"># LOSS MSE
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                           <span class="c1"># LOSS MSE  
</span>    <span class="k">return</span> <span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>            <span class="c1"># uso un metodo dei tensori .mean()
</span>
<span class="c1"># J = MSE = 1/N * (w*x - y)**2
# dJ/dw = 1/N * 2x(w*x - y)
</span><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                    <span class="c1"># calcolo il gradiente  
</span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione prima del training: f(5) = {forward(5):.3f}'</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>               <span class="c1"># FORWARD
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>               <span class="c1"># LOSS
</span>    
    <span class="n">dw</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>       <span class="c1"># GRADIENTE (senza autograd) 
</span>    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span>           <span class="c1"># UPDATE   
</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}'</span><span class="p">)</span>
     
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione dopo il training: f(5) = {forward(5):.3f}'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prediction before training: f(5) = 0.000
epoch 1: w = 1.200, loss = 30.00000000
epoch 3: w = 1.872, loss = 0.76800019
epoch 5: w = 1.980, loss = 0.01966083
epoch 7: w = 1.997, loss = 0.00050332
epoch 9: w = 1.999, loss = 0.00001288
epoch 11: w = 2.000, loss = 0.00000033
epoch 13: w = 2.000, loss = 0.00000001
epoch 15: w = 2.000, loss = 0.00000000
epoch 17: w = 2.000, loss = 0.00000000
epoch 19: w = 2.000, loss = 0.00000000
Prediction after training: f(5) = 10.000
</code></pre></div></div>

<h1 id="discesa-del-gradiente-autograd">Discesa del gradiente Autograd</h1>
<p>come il punto precedente ma usando Autograd</p>

<ul>
  <li><strong>ATTENZIONE</strong> il print non legge bene il formato dei “tensori”, devo usare il metodo <strong>.item()</strong> per ottenere il valore.</li>
  <li><strong>.backward()</strong> va fatto sulla loss</li>
  <li><strong>.grad</strong>  e’ automaticamente ottenuto come parametro del tensore (per esempio dei pesi)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Regressione Lineare 
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># PREDICTORS
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>   <span class="c1"># OBIETTIVO
</span>
<span class="c1">#Nota che posso vedere sia dal punto di vista spaziale che temporale.
# dal punto di vista temporale passo alla mia rete neurale un predictor per volta
# (ma non e' manco piu' un vettore).
# dal punto di vista spaziale, passo tutti i predictor e ottengo tutti gli obiettivi.
</span>

<span class="c1"># trasformo in TENSORI (avrei potuto direttamente usare torch.tensor(), ma cosi' uso from_numpy())
</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                        <span class="c1"># autograd non serve
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>                        <span class="c1"># neanche qui 
</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>    <span class="c1"># pesi: accendo Autograd  
</span>
<span class="c1"># MODEL OUTPUT 
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>                               <span class="c1"># FORWARD PASS
</span>
<span class="c1"># LOSS MSE
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>                           <span class="c1"># LOSS MSE  
</span>    <span class="k">return</span> <span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>            <span class="c1"># mean() e' un metodo dei tensori
</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Predizione prima del training: f(5) = {forward(5).item():.3f}'</span><span class="p">)</span>

<span class="c1"># Parametri del Training
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>               <span class="c1"># FORWARD
</span>    <span class="n">LOSS</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>            <span class="c1"># LOSS
</span>    <span class="n">LOSS</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                   <span class="c1"># BACKPROPAGATION (Autograd) 
</span>    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>   <span class="c1"># UPDATE   
</span>    <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>                    <span class="c1"># AZZERO i gradienti
</span>        
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1}: w = {w.item():.3f}, loss = {LOSS.item():.8f}'</span><span class="p">)</span>
        <span class="c1">#print(w)    
#print(f'Predizione dopo il training: f(5) = {forward(5):.3f}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predizione prima del training: f(5) = 0.000
epoch 1: w = 0.300, loss = 30.00000000
epoch 11: w = 1.665, loss = 1.16278565
epoch 21: w = 1.934, loss = 0.04506890
epoch 31: w = 1.987, loss = 0.00174685
epoch 41: w = 1.997, loss = 0.00006770
</code></pre></div></div>

<h1 id="loss-e-optimizer-di-pytorch">LOSS e OPTIMIZER di PyTorch</h1>
<p>Qqui vediamo qualche esempio di:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">LOSS</code> function (ovvero il criterion)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimizer</code>, ovvero le metodologie che vengono usate per fare l’update dei pesi per migliorare la loss (dato che devo minimizzare la loss sto facendo una ottimizzazione, o minimizzazione nel dettaglio!)</li>
</ul>

<p>Questo codice e’ molto simile al precedente. La differenza e’ nell’optimizer, ovvero che strategia viene portata avanti per minimizzare le LOSS. In pratica ci sono varie funzioni che prendono come argomento il gradiente rispetto ad un tensore e minimizzano la funzione.</p>

<p>Passi:</p>
<ol>
  <li>si disegna il modello</li>
  <li>si costruiscono la <strong>loss</strong> e l’<strong>optimizer</strong></li>
  <li>si fa un loop di training</li>
</ol>

<p>Consideriamo un grafo computazionale ancora del tipo linear regression.</p>

<p><strong>optimizer.step()</strong> e’ il metodo che ci fa muovere tra i parametri secondo l’algoritmo di ottimizzazione.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Linear regression
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span>
<span class="c1"># 0) Training samples
</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>   <span class="c1"># weights
</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Prediction before training: f(5) = {forward(5).item():.3f}'</span><span class="p">)</span>

<span class="c1"># 2) Define loss and optimizer    
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>               <span class="c1"># questo viene passato come parmetro all'optimizer
</span><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">100</span>


<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>                <span class="c1"># e' una funzione predefinita di Torch  
</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">w</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># Optimizer ha come parameri di imput [w] (pesi) e lr=learning_rate
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>       <span class="c1"># TRAINING LOOP
</span>    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>       <span class="c1"># FORWARD 
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>       <span class="c1"># LOSS     
</span>    <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                   <span class="c1"># Backward (e' un metodo sul tensore dato dalla loss) 
</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>               <span class="c1"># Optimizer, uso il metodo .step() 
</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>          <span class="c1"># azzera i gradienti usati per l'ottimizzatore
</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'epoch '</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="s">': w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="s">' loss = '</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Prediction after training: f(5) = {forward(5).item():.3f}'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prediction before training: f(5) = 0.000
epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=&lt;MseLossBackward&gt;)
epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=&lt;MseLossBackward&gt;)
epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=&lt;MseLossBackward&gt;)
epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=&lt;MseLossBackward&gt;)
epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=&lt;MseLossBackward&gt;)
epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=&lt;MseLossBackward&gt;)
epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=&lt;MseLossBackward&gt;)
epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=&lt;MseLossBackward&gt;)
epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=&lt;MseLossBackward&gt;)
epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=&lt;MseLossBackward&gt;)
Prediction after training: f(5) = 10.000
</code></pre></div></div>

<h1 id="modelli-di-pytorch">Modelli di PyTorch</h1>
<p>qui vediamo come usare i modelli preinstallati di pytorch.</p>

<ul>
  <li><strong>model = nn.Linear(input_size, output_size)</strong>  modello lineare, ha 2 argomenti: i parametri in ingresso e in uscita.</li>
  <li><strong>X = torch.tensor([[1], [2], [3], [4]])</strong> occhio al formato [1] = prima riga, [2] =seconda riga, [3] = terza riga, [4]= quarta riga. X.shape = 4 righe, 1 colonna.</li>
  <li>ATTENTO: <strong>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</strong> all’ottimizzatore da’ in pasto un parametro, il learning rate.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Linear regression
# f = w * x 
</span>
<span class="c1"># here : f = 2 * x
</span>
<span class="c1"># 0) Training samples, watch the shape!
</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>                            <span class="c1"># 4 righe, 1 colonna. 4 osservazioni 1 sola FEATURE (predictor)
</span><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'#samples: {n_samples}, #features: {n_features}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#samples: 4, #features: 1
torch.Size([4, 1])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 0) create a test sample
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>        <span class="c1"># costruisco un nuovo punto per testare
</span>
<span class="c1"># 1) Design Model, the model has to implement the forward pass!
# Here we can use a built-in model from PyTorch
</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">n_features</span>                                
<span class="n">output_size</span> <span class="o">=</span> <span class="n">n_features</span>

<span class="c1"># we can call this model with samples X
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>             <span class="c1"># modello di PyTorch
</span>
<span class="s">'''
class LinearRegression(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        # define diferent layers
        self.lin = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.lin(x)

model = LinearRegression(input_size, output_size)
'''</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Prediction before training: f(5) = {model(X_test).item():.3f}'</span><span class="p">)</span>


<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>                       <span class="c1"># learning rate
</span><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">100</span>                              <span class="c1"># numero iterazioni  
</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>                        <span class="c1"># funzione loss  
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 3) Training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    <span class="c1"># predict = forward pass with our model
</span>    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># loss
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>

    <span class="c1"># calculate gradients = backward pass
</span>    <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update weights
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># zero the gradients after updating
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="c1"># unpack parameters
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'epoch '</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="s">': w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s">' loss = '</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Prediction after training: f(5) = {model(X_test).item():.3f}'</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="dataset-e-dataloader">Dataset e DataLoader</h1>
<p>Qui viene definito il dataloader. Supponi di avere un classificatore di immagini. 
In ingresso prende una immagine e  in uscita mi dice a che classe appartiene (per esempio gatto-cane).
 Non posso fare il training su una sola immagine, altrimenti farei un overfit. Quello che normalmente 
si fa e’: passare dei batch (infornate di immagini), fare il training e poi fare il trainig su nuovi batch. 
In alcuni casi in modo incrementale, ovvero batch successivi includono quelli precedenti.</p>

<p>Secondo Python Engineer e’ anche vero il contrario. Se passp tutti i dati di training, allora fare delle gradient 
calculations (backpropagation) diventa computazionalemtne oneroso.</p>

<p><strong>Osservazione</strong> di natura <code class="language-plaintext highlighter-rouge">notazionale</code>, di solito Loeber nelle istanze che crea, usa il medesimo 
nome della funzione/classe di Torch, ma con tutte le lettere minuscole. Per esmpio, in PyTorch 
esiste <code class="language-plaintext highlighter-rouge">Dataset</code> e lui chiama la sua istanza: <code class="language-plaintext highlighter-rouge">dataset</code> (minuscolo). Mi pare un’ottima 
convenzione che aiuta a ricordare i nomi delle funzioni, metodi e classi.</p>

<ul>
  <li>si fa un loop (esterno) su tutte le epoch</li>
  <li>per ogni epoch si fa un loop (interno) su tutti i batch</li>
  <li>l’ottimizzazione viene fatta <strong>solo</strong> sul batch</li>
</ul>

<p>Lingo:</p>
<ul>
  <li><strong>epoch</strong> un passo forward e un backward di <strong>TUTTI</strong> i campioni del training.</li>
  <li><strong>batch</strong> un sottoinsieme di elementi del training dataset</li>
  <li><strong>batch_size</strong> numero di campioni di training in un forward/backward pass.</li>
  <li><strong>numero di iterazioni</strong> numero di passi, ogni passo(forward/backward) usa “batch_size” campioni</li>
</ul>

<p>Per esempio:</p>
<ul>
  <li>100 campioni</li>
  <li>batch_size=20</li>
  <li>5 iterazioni  formano  1 epoch</li>
</ul>

<p>I DataLoader sono <strong>CLASSI</strong>, fanno la computazione del batch (la gestiscono).
Vengono ereditati da “torch.utils.data import DataLoader.</p>

<ol>
  <li>implementano un Dataset custom (voluto dall’utente)</li>
  <li>inherit Dataset</li>
  <li>implement <strong>__init__</strong>, <strong>__getitem__</strong>, e <strong>__len__</strong></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>  
<span class="kn">from</span>   <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span> 

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">math</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">WineDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>           <span class="c1"># Eredito dalla classe "Dataset" di torch.utils.data
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>               <span class="c1"># metodo __init__, inizializza, leggi ecc 
</span>        <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s">'./data/wine/wine.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">','</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>      <span class="c1"># definisce attributo n_samples = numero di righe
</span>
        <span class="c1"># Costruisci due attributi: sono i dati di training e le labels (come tensori) 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">x_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>  <span class="c1"># size [n_samples, n_features]
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">y_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">xy</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span> <span class="c1"># size [n_samples, 1] (la colonna zero sono gli obiettivi)
</span>
    <span class="c1"># support indexing such that dataset[i] can be used to get i-th sample
</span>    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>                     <span class="c1"># questo mi fa scegliere i pezzi del dataset
</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="c1"># chiamando len(dataset) si ottiene la size 
</span>    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span>
    
<span class="n">dataset</span> <span class="o">=</span> <span class="n">WineDataset</span><span class="p">()</span>  <span class="c1"># carico il dataset
#first_data = dataset[0]
#features, labels = first_data
</span></code></pre></div></div>

<p>In questo dataset ci sono 3 categorie di  vino e sono la prima colonna del dataset. Tutte le altre colonne sono le features. Quindi nella classe sopra ho diviso mettendo [0] per le label.
A questo punto l’oggetto dataset contiene varie proprieta’.</p>

<p>Ora costruisco un dataloader, prendendo la classe che esiste gia’ in Torch. <br />
Dato che ho gia’ importato l’oggetto DataLoader da <code class="language-plaintext highlighter-rouge">torch.utils.data</code>, basta che gli passo
i parametri corretti.</p>

<h2 id="dataset">Dataset</h2>
<p>In torchvision ci sono gia’ molti dataset disponibili che consentono di fare molti esperiemnti.
Le classi Dataset e Dataloader invece sono in <code class="language-plaintext highlighter-rouge">torch.utils.data</code></p>

<ul>
  <li>un Dataset e’ <code class="language-plaintext highlighter-rouge">subscriptable</code> (ovvero se si chiama mioDataset e faccio mioDataset[1], ottengo l’oggetto al secondo posto del dataset)</li>
  <li>nel dataset (solitamente) ci sono sia i <code class="language-plaintext highlighter-rouge">dati</code> che le <code class="language-plaintext highlighter-rouge">label</code></li>
  <li><strong>non</strong> riesco a trasformare un dataset in una funzione iteratrice</li>
</ul>

<h2 id="dataloader">DataLoader</h2>

<ul>
  <li>un  DataLoader <strong>non</strong> e’ <code class="language-plaintext highlighter-rouge">subscriptable</code>, ma posso trasformarlo in un iteratore tramite <code class="language-plaintext highlighter-rouge">iter</code> e accedere quindi ai pezzi uno per volta (saranno i batch)</li>
  <li>uso la funzione iter() per trasformare il dataloader in una funzione iteratrice (da mettere nei loop) (ho creato una nuova funzione che e’ l’iteratore del dataloader)</li>
  <li>a questo punto posso prendere i vari pezzi</li>
  <li><strong>Attenzione</strong> se faccio <code class="language-plaintext highlighter-rouge">data=dataiter.next()</code> ci possono essere dei problemi e il sistema puo’ non avere abbastanza memoria, per risolvere il problema si deve mettere:
<code class="language-plaintext highlighter-rouge">num_workers= 0</code>  (e non 2 come nell’esempio).
https://stackoverflow.com/questions/60101168/pytorch-runtimeerror-dataloader-worker-pids-15332-exited-unexpectedly</li>
</ul>

<p>Argomenti di un Dataloader:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">dataset= mio_dataset()</code>  <code class="language-plaintext highlighter-rouge">mio_dataset</code> e’ un oggetto ereditato da <code class="language-plaintext highlighter-rouge">dataset</code> (usa dataset come nome standard, in modo da ricordare)</li>
  <li><code class="language-plaintext highlighter-rouge">batch_size = 4</code>             (oppure crea una variabile batch_size)</li>
  <li><code class="language-plaintext highlighter-rouge">shuffle=True</code>               (fa shuffling, non chiaro in quali circostanze usare, la logica vuole che quando si costruiscono i train e validation sets si facciano dei sampling random. Per questo si fa lo shuffling all’interno del dataset</li>
  <li><code class="language-plaintext highlighter-rouge">num_workers</code>                (occhio che se metto 4 come nel tutorial mi da errore: devo mettere 0)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span><span class="p">)</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>  <span class="c1"># trasformato in una funzione iteratrice
</span><span class="n">data</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>        <span class="c1"># prendo il prossimo oggetto (il primo)
#data = next(dataiter)
#features, labels = data
#print(features, labels)
</span></code></pre></div></div>

<p>A questo punto fa un training loop finto per provare a vedere come funge.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Attento</code> in enumerate non mette la funzione iteratrice ma il <code class="language-plaintext highlighter-rouge">dataloader</code>.</li>
  <li>ho provato a fare iterare su data ma da’ errore: too many values to unpack (expected 2)</li>
  <li>occhio la cella sotto funge anche con dataiter solo se prima faccio girare la cella sopra.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#for i, j in enumerate(dataloader):
#    print(i,j)
</span><span class="n">dataloader</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;torch.utils.data.dataloader.DataLoader at 0x216799d3670&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#for i, j in enumerate(dataset):
#    print(i,j)    
</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nb">id</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-33-4dc6e5e585ab&gt; in &lt;module&gt;
      2 #    print(i,j)
      3 id = iter(dataset)
----&gt; 4 id.next()


AttributeError: 'iterator' object has no attribute 'next'
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">total_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">total_samples</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span> <span class="c1"># ceil altrimenti arrotonda per difetto
#n_iterations 
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>   <span class="c1"># giro tra le epoche
</span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>    <span class="c1"># qui ha passato il dataloader
</span>        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1">#print( f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}' ) 
</span>            <span class="k">pass</span>  <span class="c1"># se non voglio stampare altrimenti decommenta sopra
</span>        
<span class="c1"># Idea mia, e se invece di usare un enumerate usassi data (che e' una funzione iteratrice?)
</span>
<span class="c1">#j=0
#for epoch in range(num_epochs):   # giro tra le epoche
#    for inputs, labels in dataiter:   # qui ha passato il data (iteratrice)
#        j=j+1
#        print(j)
#        if (j+1)%5 == 0:
#            print( f'epoch {epoch+1}/{num_epochs}, step {j+1}/{n_iterations}, inputs {inputs.shape}' )    
</span>
<span class="c1"># dataset pre-installati:
</span>
<span class="c1">#torchvision.datasets.MNIST()  # occhio che e'  M N I S T
# fashion-mnist
# cifar
# coco
</span></code></pre></div></div>

<h1 id="dataset-transforms">Dataset Transforms</h1>

<p>In pratica si deve passare un argomento <code class="language-plaintext highlighter-rouge">transform</code> alla classe associata al dataset.</p>

<p>Documentazione sulle possibili trasformazioni:
https://pytorch.org/docs/stable/torchvision/transforms.html</p>

<p>riassunto delle trasformazioni (UTILE, lo fa Python engineer):</p>

<p>Quando carichi un dataset da <code class="language-plaintext highlighter-rouge">torchvision</code> puoi usare l’argomento: download=True!</p>

<p>Traformazioni sulle <code class="language-plaintext highlighter-rouge">Immagini</code>:</p>
<ul>
  <li>CenterCrop, Grayscale, Pad, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomRotation, Resize, Scale</li>
</ul>

<p>Sui <code class="language-plaintext highlighter-rouge">Tensori</code>:</p>
<ul>
  <li>LinearTransformation, Normalize, RandomErasing</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Conversion</code>:
-ToPILImage: da tensore o ndarray  (PILI = Pillow image)
-ToTensor: da numpy.ndarray o PILImage</p>

<p>Generic:</p>
<ul>
  <li>lambda</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Custom:</code></p>
<ul>
  <li>Si puo’ scrivere una propria classe</li>
</ul>

<p>Trasformazioni <code class="language-plaintext highlighter-rouge">composte multiple</code>:</p>
<ul>
  <li>
    <p>composed = transforms.Compose( [Rescale(256] , RandomCrop(224)] )</p>
  </li>
  <li>torchvision.transforms.ReScale(256)     # occhio che qui ha messo S maiuscola</li>
  <li>torchvision.transforms.ToTensor()</li>
</ul>

<p><strong>Trasformazioni</strong> <br />
Ora prendo la classe usata sopra per i dataset e aggiungo un argomento: transform,
che specifica quali trasformazioni posso applicare!</p>

<ul>
  <li>va passato qualcosa al momento della creazione della classe dataset</li>
  <li>viene fatto un esempio con una <code class="language-plaintext highlighter-rouge">trasformazione custom</code>, usando una <strong>classe</strong></li>
  <li>ho un problema, mi dice che, al contrario del codice del corso, WineDataset non ha l’attributo transform. in particolare succede se faccio: 
dataset = WineDataset(transform =ToTensor())
dataset[0] &lt;- qui e’ il problema</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#dataset = torchvision.datasets.MNIST(root='./data', download= True, transform=torchvision.transforms.ToTensor())
</span>
<span class="k">class</span> <span class="nc">WineDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>   <span class="c1"># posso anche non passare transform, di default = None
</span>        <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s">'./data/wine/wine.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">','</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">]]</span>  <span class="c1"># NOTA che scrivo [0]
</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span> <span class="c1"># quando chiamo la trasformazione dall'istanza.
</span>    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>            <span class="c1"># questo mi prende lo specifico dato alla posizione index
</span>        <span class="c1">#return self.x[index], self.y[index]  # non ritorna l'oggetto, ma voglio trasformare!
</span>        <span class="n">sample</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>  <span class="c1"># costruisco l'oggetto
</span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>                      <span class="c1"># se e' presente
</span>             <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tranform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
                
        <span class="k">return</span> <span class="n">sample</span>                           <span class="c1"># lo metto qui in modo che ritorni qualcosa comunque  
</span>
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">n_samples</span>

    
<span class="c1">############## trasformazione custom ###############
############## abbiamo bisogno di un metodo chiamato __call__
</span><span class="k">class</span> <span class="nc">ToTensor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>   <span class="c1">##############  FONDAMENTALE ########## 
</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">sample</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    
    

<span class="c1">#dataset = WineDataset(transform =None)
#dataset = WineDataset(transform=ToTensor)
#dataset = WineDataset(transform = ToTensor())
#dataset[0]
</span>
<span class="c1">#first_data = dataset[0]        ################### NON FUNGE #################
#features, labels  = first_data
#print(type(features), type(labels))
</span>

<span class="c1">##  posso fare trasoformazioni multiple:
</span>
<span class="k">class</span> <span class="nc">MulTransform</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">sample</span>
        <span class="n">inputs</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span>

<span class="n">composed</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">MulTransform</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">WineDataset</span> <span class="p">(</span><span class="n">transform</span> <span class="o">=</span> <span class="n">composed</span><span class="p">)</span>
<span class="c1">#first_data = dataset[0]     ###################### NON FUNGE #####################
</span></code></pre></div></div>

<h1 id="softmax-e-cross-entropy-loss">Softmax e Cross-Entropy Loss</h1>
<p>Queste sono tra le funzioni piu’ comuni per “schiacciare i risultati” (un po’ come la logit(p) =ln(p/1-p) =ln(odds) )</p>

<p><code class="language-plaintext highlighter-rouge">Softmax</code>:
<script type="math/tex">\displaystyle S(y_i) = \frac{e^{y_i}}{\sum e^{y_i}}</script>
I risultati vanno quindi tra 0 e 1.
A denominatore sembra una funzione di ripartizione (ma i valori non sono negativi).</p>

<p>Chiaramente $ \sum S(y_i) =1$ quindi le $S(y_i)$ possono essere interpretate come delle <code class="language-plaintext highlighter-rouge">probabilita'</code>. 
Per questo vengono applicate come layer in uscita dopo l’ultimo strato in modo che ad ognuna delle classi sia associata una probabilita’.
<img src="/images/posts/pytorch/softmax.png" alt="cane" /></p>

<ul>
  <li>
    <p>Se fossero solo <strong>2 le classi</strong> come nella regressione logistica si userebbe la <strong>sigmoid</strong> function (che e’ la versione non generalizzata ma con solo 2 classi in pratica):
<script type="math/tex">Sigmoid(x) = \frac{1}{1+e^{-x}}</script></p>
  </li>
  <li>
    <p>Come Loss si potrebbe usare la <code class="language-plaintext highlighter-rouge">nn.BCELoss()</code>  (binary cross entropy loss). In questo caso pero’ BISOGNA implementare la sigmoide dopo l’ultimo strato.</p>
  </li>
  <li>
    <p>La cross entropy di Pytorch implementa gia’ la <strong>softmax</strong> dall’ultimo strato, per questo bisogna evitare di applicare la softmax un’altra volta!</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                       <span class="c1"># costruisco una softmax di un ARRAY x
</span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span>  <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># ho tutti gli elementi
</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mf">1.</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.1</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1">########## qui uso una softmax gia' presente in Torch
</span><span class="n">x</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">])</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># devo specificare la dimensione dove giro
</span><span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

<span class="n">x</span><span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">500</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span><span class="mf">0.01</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span>  <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span>
<span class="c1">#y = sigmoid(x)
</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1">#x
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.65900114 0.24243297 0.09856589]
tensor([0.6590, 0.2424, 0.0986])





[&lt;matplotlib.lines.Line2D at 0x2165c85f910&gt;]
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_52_2.png" alt="png" /></p>

<h2 id="cross-entropy-loss">Cross-Entropy Loss</h2>
<p>La Cross-Entropy Loss e’ una funzione di Loss, che  viene spesso combinata alla soft-max.</p>

<p>Come la maggior parte delle funzioni di Loss trasforma i valori ottenuti (output) e le label in un
singolo valore. Dato ci sono 2 vettori N-dimensionali:</p>
<ul>
  <li>il vettore calcolato (a partire dall’input) $\hat{Y}= [0.7, 0.2, 0.1]$ Gli ingressi devono poter essere interpretabili come probabilita’, ergo devono essere nel range (0,1]. Questo e’ il motivo per cui gli si danno in pasto dei valori che sono gia’ stati convertiti con delle Softmax.</li>
  <li>e quello osservato $Y= [1,0,0]$ (<strong>One-Hot Encoded</strong> labels) <code class="language-plaintext highlighter-rouge">dubbio</code>, ma cosi’ facendo non e’ manco piu’ una somma! tutte le $Y_i$ vengono azzerate tranne 1. E’ vero solo se faccio un oggetto per volta. Se faccio un batch di oggetti e quindi sommo i risultati (per ottenere la empirical loss), avro’ piu’ di un valore diverso da 0.</li>
</ul>

<p>la <code class="language-plaintext highlighter-rouge">Cross Entropy Loss</code> viene definita come:
<script type="math/tex">D(\hat{Y}, Y) = - \frac{1}{N} \sum Y_i \cdot log(\hat{Y}_i)</script>
Nota che la Cross-Entropy loss restituisce valori [0,1].</p>

<p><strong>Migliore</strong> la predizione, piu’ <strong>bassa</strong> e’ la Loss. <br /></p>

<p><strong>Domanda</strong>: come ottengo questa formula a partiere dall’entropia di <code class="language-plaintext highlighter-rouge">Shannon</code>?</p>

<p>Quindi e’ un’entropia di Shannon ma in cui da un lato inserisco i valori osservati e quelli ottenuti. Chiaramente serve che le Y e $\hat{Y}$ siano in un range [0,1] se voglio ottenere dei risultati simili all’entropia di Shannon.</p>

<p><strong>Attenzione</strong>  nel codice del video, Loeber non segue la definizione in quanto <strong>non</strong> divide per il numero di oggetti, non so perche’. Quindi i risultati che ottiene non sono nel range [0,1]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>       <span class="c1"># sono 2 array N-dim
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span> <span class="n">actual</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span> <span class="p">))</span>  <span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_pred_good</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">])</span>
<span class="n">y_pred_bad</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>

<span class="n">l1</span>  <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred_good</span><span class="p">)</span>
<span class="n">l2</span>  <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred_bad</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Loss1 : {l1:.4f}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Loss2 : {l2:.4f}'</span><span class="p">)</span>

      
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loss1 : 0.1189
Loss2 : 0.7675
</code></pre></div></div>

<h2 id="crossentropyloss">CrossEntropyLoss</h2>
<p>la funzione nn.CrossEntropyLoss <strong>usa gia’</strong>:<br />
<code class="language-plaintext highlighter-rouge">nn.LogSoftMax -&gt; nn.NLLLos</code>  (negative log likelihood loss)</p>

<p>Per questo <code class="language-plaintext highlighter-rouge">NON</code> le (a nn.LogSoftMax) si devono dare in pasto i vettori codificati tramite:</p>

<ul>
  <li>One Hot encoding (<strong>NO</strong>)</li>
  <li>Softmax (<strong>NO</strong>)</li>
</ul>

<p>La funzione di Loss in PyTorch consente di avere samples multipli. <br />
Ovvero si ha una loss per il primo sample, una per il secondo sample, ecc in pratica viene un vettore di loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">######## QUI usiamo la cross entropy embedded in Torch
</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># questa e' la classe corretta e' UN Solo valore
</span>
<span class="c1">#qui sotto la dimensione e' n_samples x n_classes =  1 x 3  (in questo caso)
## e' un ARRAY di ARRAY
</span><span class="n">Y_pred_good</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span>  <span class="p">[</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="p">]</span> <span class="p">)</span>  <span class="c1"># e' buona perche' la classe 0 ha il valore maggiore
</span>
<span class="n">Y_pred_bad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span>  <span class="p">[</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span> <span class="p">]</span> <span class="p">)</span>   <span class="c1"># e' cattiva perche' il valore maggiore e' per la classe 1
</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># istanzio la funzione
</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span> <span class="p">(</span><span class="n">Y_pred_good</span><span class="p">,</span> <span class="n">Y</span> <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span> <span class="p">(</span><span class="n">Y_pred_bad</span> <span class="p">,</span> <span class="n">Y</span> <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="c1">### per ottenere le predizioni:
</span>
<span class="n">_</span><span class="p">,</span> <span class="n">pred1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span> <span class="p">(</span><span class="n">Y_pred_good</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># il secondo ingresso e' la dimensione dove gira?
</span><span class="n">_</span><span class="p">,</span> <span class="n">pred2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span> <span class="p">(</span><span class="n">Y_pred_bad</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">pred1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pred2</span><span class="p">)</span>

<span class="c1">############## sample multipli ###########################
</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>   <span class="c1"># vettore 3 (n_samples)
</span>
                            <span class="c1"># vettore n_labels x n_samples
</span><span class="n">Y_pred_good</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span>  <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]</span> <span class="p">,</span>
                               <span class="p">[</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="p">,</span>
                               <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="p">]</span> <span class="p">)</span>

<span class="n">Y_pred_bad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span>  <span class="p">[</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="p">,</span>
                               <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]</span> <span class="p">,</span>
                               <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="p">]</span> <span class="p">)</span> 

<span class="k">print</span><span class="p">(</span><span class="n">loss</span> <span class="p">(</span><span class="n">Y_pred_good</span><span class="p">,</span> <span class="n">Y</span> <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span> <span class="p">(</span><span class="n">Y_pred_bad</span> <span class="p">,</span> <span class="n">Y</span> <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">_</span><span class="p">,</span> <span class="n">pred1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span> <span class="p">(</span><span class="n">Y_pred_good</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># il secondo ingresso e' la dimensione dove gira?
</span><span class="n">_</span><span class="p">,</span> <span class="n">pred2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span> <span class="p">(</span><span class="n">Y_pred_bad</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pred1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pred2</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.4170299470424652
1.840616226196289
tensor([0])
tensor([1])
0.3018244206905365
1.6241613626480103
tensor([2, 0, 1])
tensor([0, 2, 1])
</code></pre></div></div>

<h2 id="esempio-applicazione-di-softmax">Esempio Applicazione di Softmax</h2>
<ul>
  <li>un solo hidden layer con <code class="language-plaintext highlighter-rouge">hidden_size</code> nodi</li>
  <li><code class="language-plaintext highlighter-rouge">input_size</code> e’ per esempio il num di punti dell’immagine</li>
  <li><code class="language-plaintext highlighter-rouge">num_classes</code> e’ il numero di possibili classi in uscita (output_size)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">NeuralNet2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>   <span class="c1"># questo serve per ereditare il costruttore?
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>        <span class="c1">#   
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>                                    <span class="c1"># metodo nuovo 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>       <span class="c1"># altro metodo che prende 2 arg   
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>            <span class="c1"># devo passare l'input: x
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="c1"># NON USARE softmax, e' gia' messa nella Loss che usiamo popi
</span>        <span class="k">return</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
       
<span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet2</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># istanzio il modello
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>                                   <span class="c1"># Applica gia' la Softmax di default.
</span>
        
<span class="c1">################# Classificazione binaria ############################
# num_classes = 1   # il risultato vale un numero da [0,1]
# model = NeuralNet1(input_size = 28*28, hidden_size = 5)
# critetrion = nn.BCELoss()
</span></code></pre></div></div>

<h1 id="activation-function">Activation Function</h1>
<p>https://www.youtube.com/watch?v=3t9lZM7SS7k&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=12&amp;ab_channel=PythonEngineer</p>

<ul>
  <li>ReLU  rectified linear unit _/</li>
</ul>

<p>Le funzioni di attivazione vengono applicate dopo avere fatto il prodotto scalare e si applicano a tutti i neuroni/nodi (hidden).</p>

<p>Nel video  dice che se non mettiamo delle funzioni di attivazione si ha un gigantesco livello lineare (il disegno e’ in un certo senso sbagliato, quello al minuto 1:15). All’inizio pensavo sbagliasse, ma ripensandoci ha completamente ragione. Per esempio supponiamo che ci siano solo due variabili di ingresso (x,y) e 2 nodi nel primo hidden layer:</p>
<ul>
  <li>$a_1x+b_1y$</li>
  <li>$c_1x+d_1y$</li>
</ul>

<p>(evito i bias che tanto sono costanti). A questo punto mettiamo un secondo hidden layer con 2 nodi e ottengo:</p>
<ul>
  <li>$a_2(a_1x+b_1y) + b_2(c_1x+d_1y)$  $\rightarrow$   $(a_2 a_1 +  b_2 c_1) x+(a_2 b_1 + b_2 d_1)y $</li>
  <li>$c_2(a_1x+b_1y) + d_2(c_1x+d_1y)$  $\rightarrow$   $(c_2 a_1 +  d_2 c_1) x+(c_2 b_1 + d_2 d_1)y $</li>
</ul>

<p>Non ho mai termini quadratici o cubici in x o y. 
Se non ci fossero le funzioni di attivazione sarebbe proprio un modello lineare, in cui non avrebbe nemmeno senso mettere tanti nodi al primo hidden layer (e tantomeno mettere piu’ di un layer!)</p>

<p>Alla fine del capitolo successivo (il 13) ho provato a fare un modello senza funzioni di attivazione. I risultati sono interessanti.</p>

<p>In ogni caso le funzioni lineari applicate dopo ogni layer migliorano le prestazioni.</p>

<ul>
  <li>Step function $\displaystyle f(x)= \begin{cases} 0 &amp; \text{if } x &lt; 0  \ 
                                    1 &amp; \text{if } x\ge 0\end{cases}$</li>
  <li>Sigmoid <code class="language-plaintext highlighter-rouge">nn.Sigomoid</code> $\displaystyle f(x) = \frac{1}{1+e^{-x}}$</li>
  <li>Tanh  <code class="language-plaintext highlighter-rouge">nn.TanH</code>   $\displaystyle f(x) = \frac{2}{1+e^{-2x}} -1$</li>
  <li>ReLU  <code class="language-plaintext highlighter-rouge">nn.ReLU</code>   $\displaystyle f(x)= \begin{cases} 0 &amp; \text{if } x &lt; 0  \ 
                                    x &amp; \text{if } x\ge 0\end{cases}$</li>
  <li>Leacky ReLU <code class="language-plaintext highlighter-rouge">nn.LeakyReLU</code> $\displaystyle f(x)= \begin{cases} ax &amp; \text{if } x &lt; 0  \ 
                                    x &amp; \text{if } x\ge 0\end{cases}$  con $a &lt; 1$</li>
  <li>Softmax <code class="language-plaintext highlighter-rouge">nn.Softmax</code>  $\displaystyle S(y_i) = \frac{e^y_i}{ \sum_i e^{y_i}}$</li>
</ul>

<p>Sono disponibili anche come funzioni di Torch, ma in questo caso i nomi sono tutti in <code class="language-plaintext highlighter-rouge">minuscolo</code>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">nn.ReLU()</code>  $\leftrightarrow$ <code class="language-plaintext highlighter-rouge">torch.relu()</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.Sigmoid()</code>  $\leftrightarrow$ <code class="language-plaintext highlighter-rouge">torch.sigmoid()</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.Softmax()</code>  $\leftrightarrow$ <code class="language-plaintext highlighter-rouge">torch.softmax()</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.TanH()</code>  $\leftrightarrow$ <code class="language-plaintext highlighter-rouge">torch.tanh()</code></li>
</ul>

<p>In alcuni casi le funzioni non sono disponibili da Torch,
ma si deve passare da torch.nn.functional:</p>

<ul>
  <li>F.relu()</li>
  <li>F.leaky_relu()</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># opzione 1: creare un modulo nn 
</span><span class="k">class</span> <span class="nc">NeuralNet1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>   <span class="c1"># questo serve per ereditare il costruttore?
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>        <span class="c1">#   
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>                                    <span class="c1"># metodo nuovo 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>       <span class="c1"># altro metodo che prende 2 arg 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>            <span class="c1"># devo passare l'input: x
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span> 
    
    
        
<span class="c1"># opzione 2: usare le funzioni di attivazione di Torch 
</span><span class="k">class</span> <span class="nc">NeuralNet2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>   <span class="c1"># questo serve per ereditare il costruttore?
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>        <span class="c1">#   
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>       <span class="c1"># altro metodo che prende 2 arg 
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>                     <span class="c1"># devo passare l'input: x
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>     <span class="c1"># ho usato le funzioni di torch
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span> 
                            
</code></pre></div></div>

<h1 id="feed-forward-neural-network">Feed Forward Neural Network</h1>
<p>Questo e’ il primo esempio di rete neurale funzionante. Nota che qui NON vengono fatti i passaggi preliminari per conoscere la struttura del datset, per vedere se le classi sono bilanciate, o per fare delle trasformazioni. L’ipotesi di partenza e’ che il dataset sia ok.</p>

<p><code class="language-plaintext highlighter-rouge">Scopo</code>:<br /></p>
<ul>
  <li>costruire una fully connected neural network con 1 hidden layer (Feed Forward: una volta fatto il passo il lavoro e’ finito).</li>
  <li>per il riconoscimento di immagini del dataset MNIST  (sono i numeri da 0 a 9 scritti a mano da varie persone, in immagini 28 x28 pixel con un solo canale di colore)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Metodo</code>:<br /></p>

<ol>
  <li>Dataset e Dataloader:
    <ul>
      <li>Si importa un dataset, per esempio: <code class="language-plaintext highlighter-rouge">torchvision.datasets.MNIST(...)</code></li>
      <li>Si creano 2 dataloader, uno per il train e uno per il test, per esempio: <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader(...)</code></li>
    </ul>
  </li>
  <li>Costruzione e istanziazione del modello:
    <ul>
      <li>si importa la classe <code class="language-plaintext highlighter-rouge">nn.Model</code>, a cui si devono poter passare come parametri le dimensioni dei tensori iniziali, hidden e output…</li>
      <li>in nn.Model, all’interno del metodo <code class="language-plaintext highlighter-rouge">__init__</code> si costruiscono i metodi e gli attributi che serviranno al grafo computazionale (per esempio <code class="language-plaintext highlighter-rouge">self.l1= nn.Linear</code>, <code class="language-plaintext highlighter-rouge">self.n_output= n_output</code>…)</li>
      <li>in nn.Model si definiscono anche le ReLU, SoftMax, ecc  <code class="language-plaintext highlighter-rouge">self.relu= nn.ReLU()</code></li>
      <li>eventualmente gli oggetti vengono mandati sul device: <code class="language-plaintext highlighter-rouge">.to(device)</code></li>
      <li>in nn.Model si costruisce il metodo <code class="language-plaintext highlighter-rouge">forward()</code> (il nome deve essere proprio forward) in cui si mettono i vari passaggi costruire il <code class="language-plaintext highlighter-rouge">grafo computazionale</code></li>
      <li>Finita la costruzione del modello se ne fa un’istanza passando come argomenti i valori corretti delle dimensioni dei tensori</li>
      <li>si usa il metodo per mandare il modello sulla GPU: <code class="language-plaintext highlighter-rouge">modello = NeuralNet(input_size,...).to(device)</code>  in modo che sia sulla <code class="language-plaintext highlighter-rouge">GPU</code></li>
    </ul>
  </li>
  <li>Scelta della funzione di loss (<code class="language-plaintext highlighter-rouge">criterion</code>) e del metodo di ottimizzazione (<code class="language-plaintext highlighter-rouge">optimizer</code>)
    <ul>
      <li>si sceglie una funzione di loss (che per esempio possiamo chiamare <code class="language-plaintext highlighter-rouge">criterion</code>) (occhio che la CrossEntropy include gia’ softmax)</li>
      <li>si applica <code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> per evitare che l’ottimizzatore entri nel grafo computazionale</li>
      <li>ci si ricorda di</li>
      <li><code class="language-plaintext highlighter-rouge">loss.backwards()</code> costruisce i gradienti rispetto ai pesi (che hanno autograd =True) tramite la chain rule. Il gradiente della loss serve poi per l’ottimizzazione</li>
      <li>si sceglie una funzione di ottimizzazione <code class="language-plaintext highlighter-rouge">optimizer</code>:  SGD, Adam, … : per esempio: <code class="language-plaintext highlighter-rouge">optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)</code> Nota che bisogna passare i parametri del modello che vengono ottenuti tramite <code class="language-plaintext highlighter-rouge">model.parameters()</code>, perche’ l’ottimizzatore sappia cosa deve modificare.</li>
      <li><code class="language-plaintext highlighter-rouge">optimizer.step()</code> fa un passo nella direzione dell’ottimizzazione in funzione del learning rate.</li>
    </ul>
  </li>
  <li>Loop sulle epoche e sui batch.
    <ul>
      <li>si fa un loop sulle epoche (sceglo io quante).</li>
      <li>si fa un loop per tutti i batch di ogni epoca.</li>
      <li>con <code class="language-plaintext highlighter-rouge">enumerate</code> si spacchettano le immagini e le label dal dataloader.</li>
      <li>si usa <code class="language-plaintext highlighter-rouge">view</code> o <code class="language-plaintext highlighter-rouge">reshape</code> per trasformare gli oggetti in 1D (il risultato e’ lo stesso)</li>
      <li><strong>NON chiaro</strong>, che differnza c’e’ tra passare un’immagine e un batch di immagini al modello? come viene gestito?</li>
      <li><strong>NON</strong> si chiama <code class="language-plaintext highlighter-rouge">forward</code>, ma si passano gli oggetti da classificare al <code class="language-plaintext highlighter-rouge">modello</code> (perche’ il modello nn.Model ha gia’ implementato un metodo <code class="language-plaintext highlighter-rouge">__call__</code> e questo in pratica fa si che quando si chiami il modello come se fosse una funzione, allora si fa entrare in azione il metodo <code class="language-plaintext highlighter-rouge">forward</code>)</li>
      <li>si crea una <code class="language-plaintext highlighter-rouge">loss= criterion(output, labels)</code>  (posso comodamente cambiare la loss cambiando il criterio)</li>
      <li><code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> ci si assicura che la backpropagation e l’ottimizzazione non modifichino i gradienti. Si evita quindi che entrino nel grafo computazionale.</li>
      <li>si costruisce la chain rule con:  <code class="language-plaintext highlighter-rouge">loss.backward()</code></li>
      <li>si ottimizzano i pesi usando un metodo del criterio di ottimizzazione: <code class="language-plaintext highlighter-rouge">criterion.step()</code></li>
    </ul>
  </li>
  <li>Si fa la validazione del modello tramite il test set
    <ul>
      <li>in questo caso si vuole evitare che i gradienti vengano fatti: <code class="language-plaintext highlighter-rouge">with torch.no_grad()</code></li>
      <li>si fanno i loop su tutti i batch (non ha senso sulle epoche) del test dataloader</li>
      <li>si usano delle metriche, per esempio l’<code class="language-plaintext highlighter-rouge">accuracy</code>.</li>
    </ul>
  </li>
</ol>

<p>Note:</p>
<ul>
  <li>Per supporto <code class="language-plaintext highlighter-rouge">GPU</code> si costruisce un oggetto device come alla riga 8</li>
  <li>piu’ tardi si fara’ un push to device, ottenuto sia per il modello che per i tensori tramite il metodo <code class="language-plaintext highlighter-rouge">.to(device)</code> (con l’oggetto device opportunamente definito prima, riga 8.</li>
  <li>Occhio che <strong>non esiste</strong> “transform.ToTensor” ma “transforms”.ToTensor con una <strong>s</strong> dopo transform!!!! python mi dava errore: non esiste transform, ma l’errore era una semplice dimenticanza.</li>
  <li>nota che la prima volta che carico il dataset ci mette un po’, la seconda volta MOLTO meno (lo mette da qualche parte in memoria forse)</li>
</ul>

<p>All’inizio ci serve l’oggetto <code class="language-plaintext highlighter-rouge">Module</code> preso da <code class="language-plaintext highlighter-rouge">nn</code>. Questo oggetto e’ una specie di contenitore che connette le varie parti di una rete neurale.</p>

<p>Un layer fully connected da nn.Module ha bisogno di 3 parametri:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">input size</code> (numero di punti dell’immagine)</li>
  <li><code class="language-plaintext highlighter-rouge">hidden size</code> (numero di nodi nascosti)</li>
  <li><code class="language-plaintext highlighter-rouge">output size</code> (in questo caso sono le classi in uscita che voglio trovare).</li>
</ul>

<p>Ricordiamoci di chiamare il metodo <code class="language-plaintext highlighter-rouge">super</code>. Questo mi consente di poter usare i metodi della superclass (la classe che ho ereditato) senza dover riscriverli da zero. Una buona spiegazione: https://realpython.com/python-super/</p>

<p>Il metodo forward ha 2 argomenti: <code class="language-plaintext highlighter-rouge">self</code> e <code class="language-plaintext highlighter-rouge">x</code></p>
<ul>
  <li>self serve perche’ siamo dentro la classe e vogliamo accedere a tutti gli della istanza</li>
  <li>x e’ invece l’immagine (il batch di immagini) che vogliamo catalogare</li>
</ul>

<p>posso prendere invece che self.relu   nn.ReLU ? (provare)</p>

<ul>
  <li>Non applico la softmax alla fine perche’ la cross entropy 
ha gia’ la softmax incorporata</li>
</ul>

<p>Forward pass: ho un problema mi dice che non sono allineati gli oggetti
su cpu e gpu, in particolare dice che :</p>
<ul>
  <li>out e’ su CPU</li>
  <li>il tensore passato come argomento uno al modello e’ su CPU</li>
  <li>ma lui se li aspetta sulla GPU</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">RISOLTO</code>: dovevo usare il metodo to.(device) quando istanziavo il modello! (riga 17 del modello)
Attento che lui nel video non lo faceva (probabilmente perche’ ha piu’ volte detto che il suo laptop non ha supporto GPU, quindi ovunque lui mandi qualcosa su device, resta su cpu)!</p>

<p>Nel seguito, prima spezzo in varie celle i passaggi, e poi per ultimo costruisco una cella con tutti i passaggi congiunti in modo da avere un luogo dove fare qualche esperimento.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>   <span class="c1"># device, dove mandare i vari oggetti 
</span>
<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>    <span class="c1">#  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">1000</span>   <span class="c1">#  scelto da me, e' la dimensione della hidden layer
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri da 0 a 9, quindi la dimensione dell'output =10
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>        <span class="c1">#  faccio 2 giri completi sul dataset di train
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>      <span class="c1">#  questo no so come sia stato scelto, comunque posso modificare
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo (potrei poi usare delle funzioni per cambiare questo parametro durante l'evoluzione)
</span>
<span class="c1">#carico i dataset MNIST
#  root e' dove viene messo il dataset quando viene caricato 
#  gli diciamo poi che e' un dataset usato per il _training_
#  Occhio che poi quando facciamo train=False per costruire il dataset di test, lui splitta automaticamente
#  mi domando in quale percentuale. 
#  Trasformiamo le immagini facendo diventare le immagini in tensori
#  lo scarico se non e' gia' nella dir data
</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># ho gia' scaricato tutto con il train_datast
</span>

<span class="c1"># costruisco i dataloader:
#DataLoader necessita di almeno 2 parametri:
#  il nome del dataset
#  il la dimensione del batch
#  poi si fa shuffle = True/False
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">## guardiamo un batch dei dati:
</span>
<span class="n">examples</span> <span class="o">=</span>  <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">examples</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1">### nota che la dimensione dei sample e' la seguente
# 100  = numero di immagini nel batch (se non metti batch_size, di default vale 1)
#   1  = numero di canali solitamente i colori
#  28  =  numero di ingressi sull'asse delle x
#  28  = numero di ingressi sull'asse delle y
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([100, 1, 28, 28]) torch.Size([100])
</code></pre></div></div>

<p>qui disegno i sample, ricorda che subplot, indica la struttra, righe e colonne e poi l’indice dice quale di questi e’.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># i primi parametri indicano 2 righe e 3 colonne, e poi il numero dell'immagine corrente.
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span> <span class="s">'gray'</span><span class="p">)</span>  <span class="c1"># occhio che c'e' solo samples[i][0], le label sono state messe in labels.
</span>
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_64_0.png" alt="png" /></p>

<h2 id="il-modello">il Modello</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># qui costruisco il modello: e' una classe
</span>
<span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>                                    <span class="c1"># lo chiamo NeuralNet e lo importo da nn.Module
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>  <span class="c1"># il costruttore
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>                      <span class="c1"># super per ereditare da nn.Module
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>           <span class="c1"># L MAIUSCOLA per Linear
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>                                  <span class="c1"># funzione di attivazione ReLU 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>          <span class="c1"># secondo(ultimo) strato fully connected
</span>       
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>    <span class="c1"># l'argomento self indica che viene lanciato su se stessi, la x e' il batch di immagini
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># sfrutto il metodo self.l1 a cui ho gia' dato dei parametri prima
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># ReLU non aveva bisogno di parametri: posso prendere nn.ReLU ?
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>            <span class="c1"># DEVE restituire qualcosa: l'output
</span>    

<span class="c1"># qui creo una istanza del modello:
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="criterion-e-optimizer">Criterion e Optimizer</h2>
<ul>
  <li>Qui sotto costruisco la funzione di <strong>LOSS</strong></li>
  <li>Poi costruisco l’<strong>optimizer</strong>, ovvero il metodo che mi “aggiusta” i pesi della rete neurale secondo determinati schemi. Attento, devo passare degli argomeni all’optimizer.
In particolare c’e’ un metodo del modello che si chiama <code class="language-plaintext highlighter-rouge">model.parameters()</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>                                    <span class="c1"># non ha bisogno di parametri
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># qui devo passare i parametri del modello!
</span></code></pre></div></div>

<h2 id="training-loop">Training loop</h2>
<p>Qui sotto faccio il training che e’ composto da un loop esterno <code class="language-plaintext highlighter-rouge">epoch</code> e uno interno sui <code class="language-plaintext highlighter-rouge">batch</code> della singola epoch.</p>

<p><code class="language-plaintext highlighter-rouge">Osservazioni</code>:</p>

<ul>
  <li>Il loop sui batches e’ fatto in modo interessante, con <code class="language-plaintext highlighter-rouge">enumerate(dataloader)</code>. In questo modo ho un indice che mi dice in quale batch sono, inoltre ho creato una tupla contenente sia l’immagine che la corrispondente label (tra tonde)</li>
  <li>devo mandare sia le immagini che le label sul device <code class="language-plaintext highlighter-rouge">.to(device)</code></li>
  <li>uso il modello, non uso il metodo del modello (se faccio model.forward() cosa succede? niente funziona allo stesso modo!)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                  <span class="c1"># Loop sulle epochs
</span>   <span class="c1"># for steps in range(n_total_steps):           # loops sui batches 
</span>   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>   <span class="c1"># loop sui batch     
</span>        <span class="c1"># 100, 1, 28, 28  (batch, canali, x, y) forma del tensore images
</span>        <span class="c1"># 100, 28x28=784  forma voluta dall'hidden layer
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     <span class="c1">#usando reshape con il -1
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward pass
</span>        <span class="c1"># print (images.is_cuda)        
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">images</span><span class="p">)</span>        <span class="c1">#  non chiama il metodo forward: perche'?
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                 
        <span class="c1"># backward pass
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>   <span class="c1"># non voglio che vengano inseriti nel backward pass
</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1">#  &lt;========  fa tutto lui, calcola chain rule
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         <span class="c1"># aggiorna i pesi
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>        
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 1 / 2, step 100/600, loss= 0.3839
epoch 1 / 2, step 200/600, loss= 0.2682
epoch 1 / 2, step 300/600, loss= 0.1468
epoch 1 / 2, step 400/600, loss= 0.1664
epoch 1 / 2, step 500/600, loss= 0.0730
epoch 1 / 2, step 600/600, loss= 0.1700
epoch 2 / 2, step 100/600, loss= 0.0749
epoch 2 / 2, step 200/600, loss= 0.2193
epoch 2 / 2, step 300/600, loss= 0.1413
epoch 2 / 2, step 400/600, loss= 0.0539
epoch 2 / 2, step 500/600, loss= 0.0470
epoch 2 / 2, step 600/600, loss= 0.0669
</code></pre></div></div>

<h2 id="test-loop">Test Loop</h2>
<p>qui calcolo l’accuracy con i dati di test.</p>

<ul>
  <li>La funzione <code class="language-plaintext highlighter-rouge">torch.max</code> restituisce: valori e l’indice. Noi vogliamo l’indice che e’ la classe.</li>
  <li>per calcolare il numero totale di immagini processate, conto il numero di righe in labels.</li>
  <li>per ogni predizione corretta aggiungo 1 “sum()” e estraggo dal tensore con item() (non chiarissimo il sum).</li>
  <li>nota la scrittura con l’  ==, quindi facciamo una specie di if “online”</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># numero di predizioni azzeccate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># ? 
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>   <span class="c1"># qui il modello e' gia' trainato!
</span>        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># prendo la classe che ha il valore massimo
</span>        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># numero di samples nel batch corrente (nell'ultimo sono diversi spesso)
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span> <span class="c1"># accuratezza in percentuale
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accuracy =97.07
</code></pre></div></div>

<h2 id="prove-sul-modello">Prove sul modello</h2>
<p>In questa cella faccio le mie varianti in modo da poter capire meglio i vari passaggi.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="c1">#device = torch.device('cpu')
</span>
<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>    <span class="c1">#  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">2000</span>   <span class="c1">#  scelto da me
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># ho gia' scaricato tutto con il train_datast
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">### nota che la dimensione dei sample e' la seguente
# 100  = numero di immagini nel batch (se non metti batch_size, di default vale 1)
#   1  = numero di canali solitamente i colori
#  28  =  numero di ingressi sull'asse delle x
#  28  = numero di ingressi sull'asse delle y
</span>
<span class="k">def</span> <span class="nf">myActiv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>    <span class="c1"># e' difficile costruire delle funzioni di attivazioni CUSTOM (vedi sopra)
</span>    <span class="c1">#return 1. +x/10. + 0.5*(x/10.)**2+1./6.*(x/10.)**3
</span>    <span class="k">return</span>  <span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span>
        

<span class="c1">############  MODELLO ####################
# qui costruisco il modello: e' una classe
</span>
<span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># L MAIUSCOLA
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">my</span> <span class="o">=</span> <span class="n">myActiv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
       
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># sfrutto il metodo self.l1 a cui ho gia' dato dei parametri prima
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># ReLU non aveva bisogno di parametri: posso prendere nn.ReLU ?
</span>        <span class="c1">#out = self.sigmoid(out)
</span>        <span class="c1">#out = self.softplus(out)
</span>        <span class="c1">#out = self.my(out)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>            <span class="c1"># DEVE restituire qualcosa
</span>

<span class="c1">############### ISTANZIO MODELLO ########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1">############### CRITERION E OPTIMIZER ###
</span><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>   <span class="c1"># non ha bisogno di parametri
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>       <span class="c1"># qui devo passare i parametri del modello!
</span>
<span class="c1">############### TRAINING LOOP ############
</span><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>


<span class="n">tot</span> <span class="o">=</span><span class="mi">0</span> <span class="c1"># PA
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                  <span class="c1"># Loop sulle epoch  
</span>   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>   <span class="c1"># loop sui batch, uso enumerate cosi' so in che batch sono     
</span>        <span class="c1"># 100, 1, 28, 28  (batch, canali, x, y) forma del tensore images
</span>        <span class="c1"># 100, 28x28=784  forma voluta dall'hidden layer
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     <span class="c1">#usando reshape con il -1
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
              
        <span class="c1">#outputs = model (images)        #  non chiama il metodo forward: perche' nn.Model ha il __call__!
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>        <span class="c1">#  non chiama il metodo forward: perche' nn.Model ha il __call__!
</span>        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">tot</span> <span class="o">=</span> <span class="n">tot</span><span class="o">+</span><span class="mi">1</span> <span class="c1"># PA per fare delle modifiche sul Learning Rate
</span>        <span class="k">if</span> <span class="p">(</span><span class="n">tot</span><span class="o">%</span><span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span><span class="mf">0.8</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Learning rate {learning_rate}'</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        
        <span class="c1"># backward pass
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>   <span class="c1"># non voglio che vengano inseriti nel backward pass
</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1">#  &lt;========  fa tutto lui, calcola chain rule
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         <span class="c1"># aggiorna i pesi
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>        

            
<span class="c1">############ TEST LOOP #################
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># numero di predizioni azzeccate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># ? 
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>   <span class="c1"># qui il modello e' gia' trainato!
</span>        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># prendo la classe che ha il valore massimo
</span>        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># numero di samples nel batch corrente (nell'ultimo sono diversi spesso)
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span> <span class="c1"># accuratezza in percentuale
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>            
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1 / 1, step 100/600, loss= 0.2884
Learning rate 0.0008
Epoch 1 / 1, step 200/600, loss= 0.1714
Epoch 1 / 1, step 300/600, loss= 0.1254
Learning rate 0.00064
Epoch 1 / 1, step 400/600, loss= 0.1586
Epoch 1 / 1, step 500/600, loss= 0.1043
Learning rate 0.0005120000000000001
Epoch 1 / 1, step 600/600, loss= 0.0829
accuracy =97.06
</code></pre></div></div>

<h2 id="prova-senza-funzioni-di-attivazione-e-un-modello-lineare">Prova senza funzioni di attivazione: e’ un modello lineare!</h2>
<p>Questi sono i risultati ottenuti quando elimino la activation function e lascio solo il
modello lineare sottostante, dove c’e’ il primo passo la fully connected tra le immagini e lo strato hidden
e dallo strato hidden all’output. I risultati sono in funzione di vari parametri variati.
Con questo dataset i risultati non sono male, si hanno delle  buone accuracy a condizione che 
il numero di neuroni sia paragonabile (anche 4 e’ sufficiente!) al numero di possibili output.</p>

<p>Nota pero’ che comunque ho la softmax finale per la cross entropy come loss.</p>

<ul>
  <li>senza attivazione: accuracy = 91.28    hidden_size  =2000 epoch =1</li>
  <li>senza attivazione: accuracy = 90.69    hidden_size  =10   epoch =1</li>
  <li>senza attivazione: accuracy = 92.54    hidden_size  =10   epoch =4</li>
  <li>senza attivazione: accuracy = 38.22    hidden_size  =1    epoch =4</li>
  <li>senza attivazione: accuracy = 41.95    hidden_size  =1    epoch =14</li>
  <li>senza attivazione: accuracy = 67.53    hidden_size  =2    epoch =4</li>
  <li>senza attivazione: accuracy = 86.04    hidden_size  =4    epoch =4</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="c1">#device = torch.device('cpu')
</span>
<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>    <span class="c1">#  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">4</span>      <span class="c1">#  scelto da me
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">4</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># ho gia' scaricato tutto con il train_datast
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">### nota che la dimensione dei sample e' la seguente
# 100  = numero di immagini nel batch (se non metti batch_size, di default vale 1)
#   1  = numero di canali solitamente i colori
#  28  =  numero di ingressi sull'asse delle x
#  28  = numero di ingressi sull'asse delle y
</span>
<span class="k">def</span> <span class="nf">myActiv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>    <span class="c1"># e' difficile costruire delle funzioni di attivazioni CUSTOM (vedi sopra)
</span>    <span class="c1">#return 1. +x/10. + 0.5*(x/10.)**2+1./6.*(x/10.)**3
</span>    <span class="k">return</span>  <span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span>
        

<span class="c1">############  MODELLO ####################
# qui costruisco il modello: e' una classe
</span>
<span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># L MAIUSCOLA
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
       
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># sfrutto il metodo self.l1 a cui ho gia' dato dei parametri prima
</span>        <span class="c1">#out = self.relu(out)  # ReLU non aveva bisogno di parametri: posso prendere nn.ReLU ?
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>            <span class="c1"># DEVE restituire qualcosa
</span>

<span class="c1">############### ISTANZIO MODELLO ########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1">############### CRITERION E OPTIMIZER ###
</span><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>   <span class="c1"># non ha bisogno di parametri
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>       <span class="c1"># qui devo passare i parametri del modello!
</span>
<span class="c1">############### TRAINING LOOP ############
</span><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                  <span class="c1"># Loop sulle epoch  
</span>   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>   <span class="c1"># loop sui batch, uso enumerate cosi' so in che batch sono     
</span>        <span class="c1"># 100, 1, 28, 28  (batch, canali, x, y) forma del tensore images
</span>        <span class="c1"># 100, 28x28=784  forma voluta dall'hidden layer
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     <span class="c1">#usando reshape con il -1
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
              
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">images</span><span class="p">)</span>        <span class="c1">#  non chiama il metodo forward: perche'?
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                 
        <span class="c1"># backward pass
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>   <span class="c1"># non voglio che vengano inseriti nel backward pass
</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1">#  &lt;========  fa tutto lui, calcola chain rule
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         <span class="c1"># aggiorna i pesi
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>        

            
<span class="c1">############ TEST LOOP #################
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># numero di predizioni azzeccate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># ? 
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>   <span class="c1"># qui il modello e' gia' trainato!
</span>        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># prendo la classe che ha il valore massimo
</span>        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># numero di samples nel batch corrente (nell'ultimo sono diversi spesso)
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span> <span class="c1"># accuratezza in percentuale
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>            
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 1 / 4, step 100/600, loss= 1.4573
epoch 1 / 4, step 200/600, loss= 1.0294
epoch 1 / 4, step 300/600, loss= 0.8547
epoch 1 / 4, step 400/600, loss= 0.6365
epoch 1 / 4, step 500/600, loss= 0.4938
epoch 1 / 4, step 600/600, loss= 0.6704
epoch 2 / 4, step 100/600, loss= 0.5902
epoch 2 / 4, step 200/600, loss= 0.4542
epoch 2 / 4, step 300/600, loss= 0.5579
epoch 2 / 4, step 400/600, loss= 0.7067
epoch 2 / 4, step 500/600, loss= 0.4833
epoch 2 / 4, step 600/600, loss= 0.6566
epoch 3 / 4, step 100/600, loss= 0.6494
epoch 3 / 4, step 200/600, loss= 0.4046
epoch 3 / 4, step 300/600, loss= 0.7297
epoch 3 / 4, step 400/600, loss= 0.3742
epoch 3 / 4, step 500/600, loss= 0.4305
epoch 3 / 4, step 600/600, loss= 0.4805
epoch 4 / 4, step 100/600, loss= 0.6506
epoch 4 / 4, step 200/600, loss= 0.3813
epoch 4 / 4, step 300/600, loss= 0.3637
epoch 4 / 4, step 400/600, loss= 0.6228
epoch 4 / 4, step 500/600, loss= 0.5557
epoch 4 / 4, step 600/600, loss= 0.5079
accuracy =86.55
</code></pre></div></div>

<h1 id="convolutional-neural-network">Convolutional Neural Network</h1>

<p>Lezione del MIT sulle CNN: https://www.youtube.com/watch?v=AjtX1N_VT9E&amp;ab_channel=AlexanderAmini</p>

<p>Lezione estesa:
https://www.youtube.com/watch?v=AjtX1N_VT9E&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=3&amp;ab_channel=AlexanderAmini</p>

<p>Materiale della lezione:
http://introtodeeplearning.com​</p>

<p>qui vediamo degli esempi specifici riguardo le reti convoluzionali.</p>
<ul>
  <li>L’esempio e’ basato sul dataset CIFAR-10.</li>
  <li>10 classi, 6000 immagini per classe.</li>
  <li>ogni immagine sono 32 x 32</li>
  <li>ogni immagine ha 3 canali</li>
  <li>50 000 immagini di training e 10 000 immagini di test</li>
  <li>Il dataset sembra gia’ essere diviso in batches</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Problema</code> ho implementato fino a criterion ma ho un errore quando istanzio il modello (prima di averlo riempito…
forse e’ per quello). <code class="language-plaintext highlighter-rouge">'ConvNet' object has no attribute '_modules'</code></p>

<p>-Domanda: Non mi e’ chiaro come vengano gestiti i casi di 3 canali di colore. Come faccio ad associarli a una singola immagine?
-Risposta. Posso pensare i 3 canli come un’immagine una sopra l’altra. Le convoluzioni applicano in modo indipendente ai 3 canali, ma alla fine quando faccio un flatten li metto tutti in un unico tensore 1D (forse)</p>

<p>Regola del numero di punti restanti in funzione di una convoluzione/ pooling,</p>
<ul>
  <li>w  = larghezza dell’<code class="language-plaintext highlighter-rouge">immagine</code>.</li>
  <li>F  = larghezza del <code class="language-plaintext highlighter-rouge">filtro</code></li>
  <li>P  = larghezza del <code class="language-plaintext highlighter-rouge">padding</code></li>
  <li>S  = stride</li>
</ul>

<p>Risultato (minuto 14:00 del tutorial 14): 
<script type="math/tex">\frac{W-F+2P}{S}+1</script>
(in questo caso P =0 dato che non ho padding, e S=1 in quanto il kernel si muove di un quadretto per volta)
<!---
![convoluzione](/images/posts/pytorch/convoluzione.png)
-->
<img src="/images/posts/pytorch/convoluzione.png" alt="drawing" width="600" />
(nota che i quadrati colorati successivi sono un po’ piu’ piccoli per evitare sovrapposizioni ma riguardano 
tutti i punti delle celle che toccano)
Ora viene implementata una rete neurale con la seguente (immagine da internet, non penso sia di Loeber) struttura:</p>

<p><img src="/images/posts/pytorch/auto-convolution.jpeg" alt="drawing" width="600" /></p>

<ul>
  <li>conv + relu</li>
  <li>pooling</li>
  <li>conv + relu</li>
  <li>pooling</li>
  <li>fully connected</li>
  <li>fully connected</li>
  <li>fully connected</li>
</ul>

<h2 id="come-misurare-le-dimensioni-dei-tensori-in-uscita">Come misurare le dimensioni dei tensori in uscita</h2>

<p>Quando si hanno delle convoluzioni o dei pooling, la dimensione dei tensori in uscita non
e’ facilissima da calcolare al volo, fortunatamente esiste una formula semplice.</p>

<ol>
  <li>In torch le convoluzioni <code class="language-plaintext highlighter-rouge">nn.Conv2d(3,6,5)</code> hanno 3 parametri:</li>
</ol>

<ul>
  <li>il primo parametro e’ il numero di <strong>canali in ingresso</strong> (per esempio 3, associati a r g b).</li>
  <li>il secondo parametro e’ il numero di <strong>canali in USCITA</strong>, nell’esempio sopra 6. Cosa significa? significa che ho preso 6 
kernel differenti (riempiti con valori random) e li ho applicati tutti e 6 (ipotesi mia)!</li>
  <li>
    <p>il terzo parametro e’ la <strong>dimensione del kernel</strong></p>
  </li>
  <li>ci sono i canali (in entrata di solito sono i colori, in uscita non hanno questo significato, in quanto possono cambiare.</li>
</ul>

<ol>
  <li>I max pooling hanno 2 parametri, per esempio: <code class="language-plaintext highlighter-rouge">nn.MaxPool2d(2,2)</code>:
    <ul>
      <li>la dimensione del kernel</li>
      <li>lo stride</li>
    </ul>
  </li>
</ol>

<p>Riassumendo si usa la formula <strong>(w - f +2p )/s +1</strong> sia per convoluzione che per pooling:</p>
<ul>
  <li>prima convoluzione (kernel 5 x 5) passo da 3 canali 32 x 32 a   : (32-5-0)/1+1  = 28 x 28   ( 6 canali ,scelto da me)</li>
  <li>primo pooling (kernel 2 x 2) con stride 2 passo da 6 canali 28 x 28 -&gt; (28-2-0)/2 + 1 = 14 x 14 ( 6 canali, non modificabile)</li>
  <li>seconda convoluzione (kernel 5 x5) passo da 6 canali 14 x 14 a -&gt; (14 - 5 -0)/1+1 = 10 x 10 (16 canali, scelto da me)</li>
  <li>secondo pooling (kernel 2 x 2) passp da 16 canali 10 x 10 a -&gt; (10-2 -0)/2 +1 = 5 x 5 (16 canali non modificabile)</li>
</ul>

<p>Quindi quando faccio un flatten alla fine ottengo: 5 x 5 x 16  <strong>esatto</strong> come nel video!</p>

<p>In grassetto metto i valori che scelgo io (per esempio i canali di ouput con la convolione)</p>

<!---
|   operazione  |    kernel     |  Stride  |  canali input | figura input |  canali output |  figura output |
| ------------- | ------------- | -------- | ------------- | ------------ | -------------- | -------------- |   
| I convoluzione  |     5 x 5     |     1    |       3       |   32 x 32    |       **6**      |  (32-5-0)/1+1 =28 |
| I max pooling   |     2 x 2     |     2    |       6       |   28 x 28    |         6      |  (28-2-0)/2+1 =14 |
| II convoluzione  |     5 x 5     |     1    |      6        |  14 x 14    |      **16**      |  (14-5-0)/1+1 =10 |
| II max pooling   |     2 x 2     |     2    |     16       |   10 x 10    |        16      |  (10-2-0)/2+1 = 5 |
--->

<table>
  <thead>
    <tr>
      <th>operazione</th>
      <th>canali input</th>
      <th>canali output</th>
      <th>kernel</th>
      <th>Stride</th>
      <th>figura input</th>
      <th>figura output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>I convoluzione</td>
      <td><strong>3</strong></td>
      <td><strong>6</strong></td>
      <td><strong>5</strong> x 5</td>
      <td>1</td>
      <td>32 x 32</td>
      <td>(32-5-0)/1+1 =28</td>
    </tr>
    <tr>
      <td>I max pooling</td>
      <td>6</td>
      <td>6</td>
      <td><strong>2</strong> x 2</td>
      <td><strong>2</strong></td>
      <td>28 x 28</td>
      <td>(28-2-0)/2+1 =14</td>
    </tr>
    <tr>
      <td>II convoluzione</td>
      <td><strong>6</strong></td>
      <td><strong>16</strong></td>
      <td><strong>5</strong> x 5</td>
      <td>1</td>
      <td>14 x 14</td>
      <td>(14-5-0)/1+1 =10</td>
    </tr>
    <tr>
      <td>II max pooling</td>
      <td>16</td>
      <td>16</td>
      <td><strong>2</strong> x 2</td>
      <td><strong>2</strong></td>
      <td>10 x 10</td>
      <td>(10-2-0)/2+1 = 5</td>
    </tr>
  </tbody>
</table>

<p><strong>Osservazione</strong></p>
<ul>
  <li>alla funzione di convoluzione vengono passati come argomenti solo: i <code class="language-plaintext highlighter-rouge">canali di ingresso</code>, <code class="language-plaintext highlighter-rouge">canali di uscita</code> e la <code class="language-plaintext highlighter-rouge">dimensione del kernel</code>, per esempio: <code class="language-plaintext highlighter-rouge">nn.Conv2d(3,6,5)</code></li>
  <li><code class="language-plaintext highlighter-rouge">Non</code> si passa la dimensione delle figure, viene presa in automatico!</li>
  <li>alla funzione di max pool invece si passano solo: la <code class="language-plaintext highlighter-rouge">dimensione del kernel</code> e <code class="language-plaintext highlighter-rouge">dimensione della stride</code>:<code class="language-plaintext highlighter-rouge">nn.MaxPool2d(2,2)</code></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Problemi:</code></p>

<ul>
  <li>ho provato ha mettere dei valori custom degli strati finali fully connected, non mi prende hidden_size2,
ma se metto un valore preciso lo prende… perche? Il motivo era che avevo messo queste quantita’ dentro la classe
indentando. Se le metto fuori, vengono prese come variabili globali!</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span>    <span class="c1">#  =1024 sono le dimensioni delle immagini
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">6</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span><span class="n">hidden_size1</span>  <span class="o">=</span> <span class="mi">220</span>   <span class="c1">#  scelto da me
</span><span class="n">hidden_size2</span>  <span class="o">=</span> <span class="mi">184</span>   <span class="c1">#  scelto da me 
</span>
<span class="n">transform</span> <span class="o">=</span>  <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span> <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>  <span class="p">)</span>  <span class="p">]</span> <span class="p">)</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transform</span><span class="p">,</span>            <span class="c1"># uso le trasformazioni indicate sopra
</span>                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># ho gia' scaricato tutto con il train_datast
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Pytorch lavora con numeri, qui assegno i nomi corrispondenti
#             0       1       2       3      4      5       6       7       8        9 
</span><span class="n">classes</span> <span class="o">=</span> <span class="p">(</span><span class="s">'plane'</span><span class="p">,</span> <span class="s">'car'</span><span class="p">,</span> <span class="s">'bird'</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">,</span> <span class="s">'deer'</span><span class="p">,</span> <span class="s">'dog'</span><span class="p">,</span> <span class="s">'frog'</span><span class="p">,</span> <span class="s">'horse'</span><span class="p">,</span> <span class="s">'ship'</span><span class="p">,</span> <span class="s">'truck'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="c1"># "denormalizza" forse per avere dei colori migliori
</span>    <span class="n">npimg</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">npimg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)))</span>


<span class="c1">######### MODELLO ###############
</span>
<span class="k">class</span> <span class="nc">ConvNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>     <span class="c1"># input channel size, output channel size,  Kernel size 5 (5x5)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>      <span class="c1"># kernel size, stride  (e' un quadrato 4x4 diviso in 4 parti 2x2)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>    <span class="c1"># input channel size = out di prima = 6, scelgo l'out= 16 e kernel size 5 (5x5)  
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="n">hidden_size1</span><span class="p">)</span> <span class="c1"># fully connected (spiegato dopo) ingresso, e 120 in uscita
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size1</span><span class="p">,</span> <span class="n">hidden_size2</span><span class="p">)</span>     <span class="c1"># in ingresso prende l'uscita del precedente e in output un tensor 1D con 84 ingressi
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>      <span class="c1"># in uscita ho solo le 10 classi di oggetti (riga 38)            
</span>        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1">#  MaxPool(ReLU(convoluzione(immagine))
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1">#  MaxPool(ReLU(convoluzione( ))
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>                <span class="c1">#  metto l'immagine processata in un vettore 1D
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>               <span class="c1">#  ReLU(fc())
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>               <span class="c1">#  ReLU(fc())
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1">#  qui non faccio la ReLU, perche' poi nella cross entropy c'e' softmax
</span>        <span class="k">return</span> <span class="n">x</span>



<span class="n">model</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                  <span class="c1"># metto sul device    
</span>    
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>             <span class="c1"># serve per fare la LOSS, include la softmax per l'ultimo strato
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>
    
<span class="n">n_total_steps</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># forma [4, 3, 32, 32] -&gt; [4, 3, 1024]       4 immagini, 3 canali di colore, 32 x 32 
</span>        <span class="c1"># input layer: 3 canali di input, 6 canali di uscita, 5 kernel size
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># forward
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="c1"># backward e ottimizzazione dei pesi
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>              <span class="c1"># chain rule
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>             <span class="c1"># step di ottimizzazione dato il learning rate
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>     
<span class="k">print</span><span class="p">(</span><span class="s">"Training Terminato"</span><span class="p">)</span>

<span class="c1"># qui faccio la fase di testing:
</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_class_correct</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>  <span class="c1"># e' una lista uso una list-comprehension
</span>    <span class="n">n_class_samples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>  <span class="c1"># non chiaro cosa sia questo
</span>    
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>      <span class="c1"># non serve una quadra?
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>       <span class="c1"># questa da capire bene 
</span>        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>        <span class="c1"># gira nel batch, label della singola immagine
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>      <span class="c1"># guarda la riga 90
</span>            
            <span class="k">if</span> <span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">pred</span><span class="p">):</span>
                <span class="n">n_class_correct</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># fa un istogramma
</span>            <span class="n">n_class_samples</span> <span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span>      <span class="c1"># ne ho trovsata una un piu' della classe label
</span>    
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span>      <span class="c1"># percentuale di corrette sul totale dei sample
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Accuracy  = {acc}:.4f'</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span> <span class="o">*</span><span class="n">n_class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span> <span class="n">n_class_samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># accuracy della singola classe
</span>        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Accuracy della classe {classes[i]}: {acc} </span><span class="si">%</span><span class="s">'</span>  <span class="p">)</span>
    
        
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Files already downloaded and verified
epoch 1 / 6, step 1000/12500, loss= 2.3000
epoch 1 / 6, step 2000/12500, loss= 2.3080
epoch 1 / 6, step 3000/12500, loss= 2.3117
epoch 1 / 6, step 4000/12500, loss= 2.2834
epoch 1 / 6, step 5000/12500, loss= 2.2788
epoch 1 / 6, step 6000/12500, loss= 2.2987
epoch 1 / 6, step 7000/12500, loss= 2.2965
epoch 1 / 6, step 8000/12500, loss= 2.3122
epoch 1 / 6, step 9000/12500, loss= 2.1770
epoch 1 / 6, step 10000/12500, loss= 2.2453
epoch 1 / 6, step 11000/12500, loss= 1.9151
epoch 1 / 6, step 12000/12500, loss= 2.1131
epoch 2 / 6, step 1000/12500, loss= 2.2575
epoch 2 / 6, step 2000/12500, loss= 1.6527
epoch 2 / 6, step 3000/12500, loss= 1.3318
epoch 2 / 6, step 4000/12500, loss= 1.4340
epoch 2 / 6, step 5000/12500, loss= 2.3746
epoch 2 / 6, step 6000/12500, loss= 1.9364
epoch 2 / 6, step 7000/12500, loss= 2.1739
epoch 2 / 6, step 8000/12500, loss= 2.1157
epoch 2 / 6, step 9000/12500, loss= 1.5521
epoch 2 / 6, step 10000/12500, loss= 1.7367
epoch 2 / 6, step 11000/12500, loss= 1.6836
epoch 2 / 6, step 12000/12500, loss= 1.8475
epoch 3 / 6, step 1000/12500, loss= 1.7431
epoch 3 / 6, step 2000/12500, loss= 2.2294
epoch 3 / 6, step 3000/12500, loss= 2.5908
epoch 3 / 6, step 4000/12500, loss= 1.4687
epoch 3 / 6, step 5000/12500, loss= 1.6939
epoch 3 / 6, step 6000/12500, loss= 1.4162
epoch 3 / 6, step 7000/12500, loss= 1.5874
epoch 3 / 6, step 8000/12500, loss= 1.5107
epoch 3 / 6, step 9000/12500, loss= 1.7396
epoch 3 / 6, step 10000/12500, loss= 1.6814
epoch 3 / 6, step 11000/12500, loss= 2.7111
epoch 3 / 6, step 12000/12500, loss= 1.9274
epoch 4 / 6, step 1000/12500, loss= 1.4081
epoch 4 / 6, step 2000/12500, loss= 1.0557
epoch 4 / 6, step 3000/12500, loss= 1.9581
epoch 4 / 6, step 4000/12500, loss= 1.2325
epoch 4 / 6, step 5000/12500, loss= 2.0073
epoch 4 / 6, step 6000/12500, loss= 1.1686
epoch 4 / 6, step 7000/12500, loss= 2.1211
epoch 4 / 6, step 8000/12500, loss= 1.8252
epoch 4 / 6, step 9000/12500, loss= 1.3711
epoch 4 / 6, step 10000/12500, loss= 0.7326
epoch 4 / 6, step 11000/12500, loss= 1.5150
epoch 4 / 6, step 12000/12500, loss= 1.2301
epoch 5 / 6, step 1000/12500, loss= 1.7059
epoch 5 / 6, step 2000/12500, loss= 1.4075
epoch 5 / 6, step 3000/12500, loss= 1.5126
epoch 5 / 6, step 4000/12500, loss= 1.4504
epoch 5 / 6, step 5000/12500, loss= 1.7903
epoch 5 / 6, step 6000/12500, loss= 1.4010
epoch 5 / 6, step 7000/12500, loss= 1.5074
epoch 5 / 6, step 8000/12500, loss= 1.0711
epoch 5 / 6, step 9000/12500, loss= 0.9101
epoch 5 / 6, step 10000/12500, loss= 1.4603
epoch 5 / 6, step 11000/12500, loss= 1.5229
epoch 5 / 6, step 12000/12500, loss= 1.2879
epoch 6 / 6, step 1000/12500, loss= 0.9052
epoch 6 / 6, step 2000/12500, loss= 1.2526
epoch 6 / 6, step 3000/12500, loss= 1.0965
epoch 6 / 6, step 4000/12500, loss= 1.1149
epoch 6 / 6, step 5000/12500, loss= 1.2192
epoch 6 / 6, step 6000/12500, loss= 1.1820
epoch 6 / 6, step 7000/12500, loss= 1.4098
epoch 6 / 6, step 8000/12500, loss= 1.8098
epoch 6 / 6, step 9000/12500, loss= 1.0243
epoch 6 / 6, step 10000/12500, loss= 1.3008
epoch 6 / 6, step 11000/12500, loss= 0.9658
epoch 6 / 6, step 12000/12500, loss= 1.0129
Training Terminato
Accuracy  = 52.13:.4f
Accuracy della classe plane: 49.5 %
Accuracy della classe car: 71.5 %
Accuracy della classe bird: 49.9 %
Accuracy della classe cat: 33.7 %
Accuracy della classe deer: 29.7 %
Accuracy della classe dog: 32.7 %
Accuracy della classe frog: 78.4 %
Accuracy della classe horse: 50.6 %
Accuracy della classe ship: 69.7 %
Accuracy della classe truck: 55.6 %
</code></pre></div></div>

<h1 id="transfer-learning">Transfer Learning</h1>
<p>In questo paragrafro viene spiegato come usare parzialmente dei modelli gia’ “trainati” per fare dei nuovi compiti simili.
Curiosamente questo sembra funzionare molto bene.
Dico curiosamente perche’ non e’ chiaro il motivo per cui una rete neurale che ha un training su degli oggetti possa andare bene anche su degli oggetti differenti. A meno che, in qualche modo, si siano imparate delle caratteristiche generali dal primo training.</p>

<p>Quello che e’ davvero potente e’ che modificando l’ultimo layer posso cambiare 
il numero di valori in uscita. Con Resnet18 ci sono 1000 classi in uscita. Cambio l’ultimo layer e 
me ne servono solo 2 in uscita: no problem!</p>

<ul>
  <li>per esempio faccio il training per un modello che classifica <strong>uccelli</strong> e <strong>gatti</strong></li>
  <li>con una piccola modifica gli faro’ classificare <strong>api</strong>  e <strong>cani</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Osservazione</code></p>
<ul>
  <li>gli <strong>uccelli</strong> hanno caratteristiche in comune alle <strong>api</strong> (non mi pare scelto a caso!)</li>
  <li>i <strong>cani</strong> hanno caratteristiche in comune ai <strong>gatti</strong> (anche questo non mi pare a caso)</li>
</ul>

<p>Quindi il transfer learning, intuitivamente, funziona bene quando le modifiche da fare sono piccole (e’ un pensiero mio).</p>

<p>Si cambia solo l’ultimo strato <strong>fully connected</strong> (rispetto al caso precedente gli ultimi 3 strati),
questo e’ infatti lo <code class="language-plaintext highlighter-rouge">strato di classificazione</code>. Gli strati precedenti tramite le convoluzioni e i pooling dovrebbero estrarre le caratteristiche dei vari oggetti.</p>

<ul>
  <li>Intuitivamente mi viene da dire che gli strati convoluzionali prendono i “pezzi” (p.es le ali, le zampe, ecc, ma nella realta’ prendono dettagli molto piu’ piccoli)</li>
  <li>gli strati finali mettono insieme questi pezzi elementari.</li>
  <li>quindi mi viene da dire che dovrebbe bastare 1 strato fully connected.</li>
  <li>in realta’ nel video di 3blue1brown si vede che non e’ molto chiaro cosa venga preso nei vari passaggi.</li>
</ul>

<p>In questo caso vediamo anche come caricare un modello gia’ “trainato” (devo trovare un nome migliore…ma dire con i pesi ottimizzati e’ lungo e anche modello ottimizzato non mi piace):<br />
<strong>Resnet18NN</strong></p>
<ul>
  <li>basato sul <strong>Imagenet database</strong> (con piu’ di 1 000 000 immagni)</li>
  <li>ha 18 strati</li>
  <li>classifica oggetti di 1000 categorie.</li>
</ul>

<p>Attenzione che qui fa una cosa leggemente diversa da quanto fatto finora, dove si costruisce il modello e lo si lancia da un loop.</p>

<p>In questo caso costruisce una funzione</p>

<h2 id="image-folder">Image Folder</h2>
<p>Costruire un folder dove mettere le immagini che devono servire per train e test. <br />
Il folder che contiene le immagini che vogliamo usare come nuovo training ha il seguente formato:</p>
<ul>
  <li>train
    <ul>
      <li>ants</li>
      <li>bees</li>
    </ul>
  </li>
  <li>val
    <ul>
      <li>ants</li>
      <li>bees</li>
    </ul>
  </li>
</ul>

<p>Quindi se la struttura e’ questa chiamando <code class="language-plaintext highlighter-rouge">datasets.Imagefolder</code> posso leggere e trasformare in un dataset.
Posso inoltre usare l’attributo <code class="language-plaintext highlighter-rouge">classes</code>.</p>

<h2 id="scheduler">Scheduler</h2>
<ul>
  <li>E’ una funzione che modifica il learning rate durante il training per ottimizzare la discesa dei gradienti.</li>
</ul>

<h2 id="fine-tuninig">Fine Tuninig</h2>
<p>tengo tutto il modello pre-trainato ma modifico solo l’ultimo layer</p>

<h2 id="domande-1">Domande</h2>
<p>per come e’ scritto il codice sembra che in varie epoche i modelli siano diversi, non ci sia un aggiornamento continuo. In pratica fa una deepcopy del miglior modello (ma io dovrei salvare, altrimenti perdo tutto e devo rifare)</p>

<p>Non riesco a trovare il modello!? dove lo ha caricato? Non lo ha ancora caricato!</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">model.train()</code>  metodo che fa il training?</li>
</ul>

<h2 id="tempo">Tempo</h2>
<ul>
  <li>se freezo il train tranne l’ultimo layer ci mette 1:42 s</li>
  <li>se re-train tutto il modello ci ha messo circa 2:14 s</li>
</ul>

<p>(ricorda che partiva gia’ trainato)</p>

<h2 id="train-e-eval">Train() e Eval()</h2>
<p>sono due metodi associati al modello che precedentemente non avevo usato</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># prendo e modifico il codice di prima secondo quanto scritto nella lezione 14
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>  <span class="c1"># nuovo non ancora  caricato prima
</span><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="c1">#import torchvision.transforms as transforms
</span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">transforms</span>  <span class="c1"># 
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>                      <span class="c1"># per controllare l'orario
</span><span class="kn">import</span> <span class="nn">os</span>                        <span class="c1"># per interagire con l'os
</span><span class="kn">import</span> <span class="nn">copy</span>                      <span class="c1"># per fare una deep copy del modello che e' un oggetto complesso (non vogliamo shallow copy)
</span><span class="kn">import</span> <span class="nn">sys</span>                       <span class="c1"># per bloccare (c'e' una funzione tipo  stop del fortran)
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>   <span class="c1"># queste non so come sono state scelte
</span><span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>    <span class="c1"># idem
</span>

<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span>    <span class="c1">#  =1024 sono le dimensioni delle immagini
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">6</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span><span class="n">hidden_size1</span>  <span class="o">=</span> <span class="mi">220</span>   <span class="c1">#  scelto da me
</span><span class="n">hidden_size2</span>  <span class="o">=</span> <span class="mi">184</span>   <span class="c1">#  scelto da me 
</span>

<span class="c1"># qui faccio un dizionario che contiene le trasformazioni da applicare
</span><span class="n">data_transforms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'train'</span><span class="p">:</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> 
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">std</span><span class="p">)])</span>
    <span class="p">,</span>
    <span class="s">'val'</span><span class="p">:</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">std</span><span class="p">)</span>
    <span class="p">])</span>
<span class="p">}</span>

<span class="c1"># import data
</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s">'data/hymenoptera_data'</span>         <span class="c1"># dove ci sono le api? Imenotteri
</span><span class="n">sets</span>  <span class="o">=</span> <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]</span>                   <span class="c1"># lista che contiene mi sa che poi non lo usa
</span>

<span class="c1"># Questo sotto e' da pensare un momento
# e' una list comprehension (in un dictionary)
# dove in realta' ci sono 2 directory, poteva mettere il path... e forse scriveva meno.
# in ogni caso la parte che non mi e' chiarissima sono i : dopo la x
# no e' banale: e' un dictionary e la x sono le chiavi e le datasets.Image... sono i valori
# tutti ottenuti con una list comprehension finale
</span><span class="n">image_datasets</span><span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">data_transforms</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="p">)</span>
                 <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]</span> <span class="p">}</span>

<span class="n">dataloaders</span>  <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">[</span><span class="n">x</span><span class="p">],</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">shuffle</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                    <span class="n">num_workers</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span>  <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]}</span>


<span class="n">dataset_sizes</span>  <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]}</span>
<span class="n">class_names</span>  <span class="o">=</span> <span class="n">image_datasets</span><span class="p">[</span><span class="s">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">classes</span>      <span class="c1"># .classes e' un attributo 
</span>
<span class="k">print</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>
<span class="c1">#sys.exit()                      # per fermare qui
</span>

<span class="c1">######### funzione che contiene il training loop  ##############
</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span> 
    <span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>     <span class="c1"># penso chen questo sia il momento d'inizio
</span>
    <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>  <span class="c1"># fa una deep copy del modello
</span>    <span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>                                      <span class="c1"># 
</span>    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch {epoch}/{num_epochs-1}'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'-'</span><span class="o">*</span> <span class="mi">10</span><span class="p">)</span>                             <span class="c1"># disegna una riga orizzontale
</span>        
        <span class="c1"># In ogni epoca si ha una fase di training e validation:
</span>        <span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">phase</span><span class="o">==</span> <span class="s">'train'</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>              <span class="c1"># set model to train mode  #######################
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>               <span class="c1"># set model to evaluation mode
</span>            
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>             <span class="c1"># in qualche modo voglio vedere live se il training va bene
</span>            <span class="n">running_corrects</span> <span class="o">=</span><span class="mi">0</span>
            

            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]:</span>       <span class="c1"># distinguo tra le fasi train/val
</span>                <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
                <span class="c1">################  forward pass
</span>                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">phase</span> <span class="o">==</span> <span class="s">'train'</span><span class="p">):</span>
                    <span class="n">outputs</span> <span class="o">=</span>  <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>
            
                <span class="c1">################ backward pass
</span>                    <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s">'train'</span><span class="p">:</span>
                        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>      <span class="c1"># non mi seve quando faccio l'ottimizzazione
</span>                        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>            <span class="c1"># chain rule
</span>                        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>           <span class="c1"># mi muovo in funzione del gradiente
</span>                 
                <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">running_corrects</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">preds</span><span class="o">==</span><span class="n">labels</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>  
            
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span><span class="s">'train'</span><span class="p">:</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>    <span class="c1"># =========================  questo aggiorna il learning_rate
</span>            
            <span class="n">epoch_loss</span> <span class="o">=</span><span class="n">running_loss</span> <span class="o">/</span> <span class="n">dataset_sizes</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span>
            <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">running_corrects</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="o">/</span> <span class="n">dataset_sizes</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span>
            
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'{phase} Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f} '</span><span class="p">)</span>

            <span class="c1"># deep copy model
</span>            
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span><span class="s">'val'</span> <span class="ow">and</span> <span class="n">epoch_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
                <span class="n">best_acc</span> <span class="o">=</span> <span class="n">epoch_acc</span>
                <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>   <span class="c1"># questo copia il dizionario associato al miglior modello
</span>        
        <span class="k">print</span><span class="p">()</span>
    <span class="n">time_elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span><span class="n">since</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Training completato in {time_elapsed//60:.0f}  minuti {time_elapsed</span><span class="si">%60</span><span class="s">:.0f}s '</span><span class="p">)</span>
           
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_wts</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">model</span>    
        
<span class="c1">######### importiamo il MODELLO ############### 
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span> <span class="p">)</span>       <span class="c1"># qui il modello e' importato da torchvision e lo mette nella cache
</span>
<span class="c1"># se voglio bloccare tutti i parametri tranne quelli dell'ultimo layer basta che 
# li blocco:!
</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>     <span class="c1"># non fa il gradiente!
</span>
<span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>                 <span class="c1"># ho preso il layer fully connected (ultimo) e queste sono le input features?
</span>
<span class="c1"># ora creo un nuovo layer e lo assegno come ultimo layer. 
# come faccio a sapere che l'ultimo layer si chiama `fc`?
</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># ho solo 2 classi in uscita.  DI DEFAULT ha requires_grad = True
</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                    

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span> <span class="p">)</span>   <span class="c1"># occhio che lui aveva importato in modo diverso e chiama solo optim.SGD
</span>
<span class="c1">############ SCHEDULER che fa una update del lr #######
</span>
<span class="n">step_lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># ogni 7 epoch lr =lr*gamma (diventa 1/10)
</span>
<span class="c1">#for epoch in range(100):
#    train()   # optimizer.step()
#    evaluate()
#    scheduler.Step()
</span>    
<span class="n">model</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_lr_scheduler</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># qui alla fine ho trovato il modello ottimale, e potrei salvarlo volendo.
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['ants', 'bees']
Epoch 0/19
----------
train Loss: 0.6456  Acc: 0.6230 
val Loss: 0.5187  Acc: 0.8039 

Epoch 1/19
----------
train Loss: 0.5606  Acc: 0.7049 
val Loss: 0.4390  Acc: 0.8627 

Epoch 2/19
----------
train Loss: 0.5181  Acc: 0.7623 
val Loss: 0.4065  Acc: 0.8301 

Epoch 3/19
----------
train Loss: 0.4886  Acc: 0.7910 
val Loss: 0.3304  Acc: 0.8954 

Epoch 4/19
----------
train Loss: 0.4584  Acc: 0.8197 
val Loss: 0.3134  Acc: 0.8954 

Epoch 5/19
----------
train Loss: 0.4822  Acc: 0.7623 
val Loss: 0.2991  Acc: 0.9085 

Epoch 6/19
----------
train Loss: 0.4221  Acc: 0.8115 
val Loss: 0.2734  Acc: 0.9216 

Epoch 7/19
----------
train Loss: 0.4034  Acc: 0.8156 
val Loss: 0.2741  Acc: 0.9216 

Epoch 8/19
----------
train Loss: 0.3629  Acc: 0.8852 
val Loss: 0.2734  Acc: 0.9216 

Epoch 9/19
----------
train Loss: 0.4139  Acc: 0.8197 
val Loss: 0.2570  Acc: 0.9216 

Epoch 10/19
----------
train Loss: 0.4058  Acc: 0.8402 
val Loss: 0.2794  Acc: 0.9085 

Epoch 11/19
----------
train Loss: 0.4000  Acc: 0.8238 
val Loss: 0.2611  Acc: 0.9346 

Epoch 12/19
----------
train Loss: 0.4265  Acc: 0.8156 
val Loss: 0.2510  Acc: 0.9346 

Epoch 13/19
----------
train Loss: 0.3911  Acc: 0.8648 
val Loss: 0.2729  Acc: 0.9085 

Epoch 14/19
----------
train Loss: 0.4748  Acc: 0.7787 
val Loss: 0.2668  Acc: 0.9216 

Epoch 15/19
----------
train Loss: 0.3790  Acc: 0.8443 
val Loss: 0.2865  Acc: 0.9020 

Epoch 16/19
----------
train Loss: 0.3815  Acc: 0.8730 
val Loss: 0.2611  Acc: 0.9412 

Epoch 17/19
----------
train Loss: 0.3946  Acc: 0.8197 
val Loss: 0.2722  Acc: 0.9020 

Epoch 18/19
----------
train Loss: 0.4166  Acc: 0.8525 
val Loss: 0.2828  Acc: 0.9085 

Epoch 19/19
----------
train Loss: 0.3790  Acc: 0.8484 
val Loss: 0.2649  Acc: 0.9216 

Training completato in 1  minuti 42s 
</code></pre></div></div>

<h1 id="tensorboard">Tensorboard</h1>

<p>https://pytorch.org/docs/stable/tensorboard.html</p>

<p>installazione, io ho usato conda: <code class="language-plaintext highlighter-rouge">conda install -c conda-forge/label/cf202003 tensorboard</code></p>

<p>Tensorboard e’ un metodo per visualizzare tramite delle dashboard visibili da browser:</p>
<ul>
  <li>il grafo computazionale</li>
  <li>l’evoluzione dei parametri (Loss, accuracy, ecc…)</li>
  <li>nota che e’ sviluppato per Tensorflow (ma funge anche con Pytorch)</li>
  <li>visualizzare istogrammi dei <strong>pesi</strong> e dei <strong>bias</strong></li>
  <li>visualizzare immagini, testi, audio (eh si lavora anche con quelli)</li>
  <li>fare un profiling dei programmi di TensorFlow</li>
  <li><code class="language-plaintext highlighter-rouge">Project embeddings in lower dimensional spaces</code>   ?????</li>
</ul>

<p>Viene usato il codice del tutorial numero <strong>13</strong> (il riconoscimento delle immagini dei numeri da 0 a 9). Da quel codice ho tolto quasi tutti i commenti in modo che i commenti rimanenti siano solo per 
l’utilizzo di tensorboard</p>

<ul>
  <li>
    <p>Quando si lancia Tensorboard bisogna specificare il path dove verranno salvati i logfile. Di default vengono messi nella directory chiamata <code class="language-plaintext highlighter-rouge">run</code> (suppongo che sia una sottodir dell’installazione di Tensorboard</p>
  </li>
  <li>
    <p>per fare partire Tensorboad (dalla dir dove si e’ lanciato il Python):  <br />
<code class="language-plaintext highlighter-rouge">tensorboard --logdir=runs</code> <br />
a questo punto si apre un browser alla pagina: <br />
<code class="language-plaintext highlighter-rouge">http://localhost:6006</code></p>
  </li>
</ul>

<p>(CTRL+C = quit)</p>

<ul>
  <li>La prima cosa che si fa e’ essenzialmente istanziare un <code class="language-plaintext highlighter-rouge">writer</code>: <code class="language-plaintext highlighter-rouge">writer=SummaryWriter('runs/mnist')</code> (riga 12).
Il writer  in pratica scrive dei valori in formato JSON (controllare) con specifiche che tensorboard va a leggere nella dir runs e, note le chiavi le mette nella dashboard.</li>
</ul>

<p>In pratica a quanto capisco, il writer scrive i dati in un formato particolare che viene poi
letto da tensorboard e mandato nella dashboard al localhost:6006. Questo e’ il motivo per cui l’oggetto fondamentale e’ un writer che ha dei vari metodi in grado di scrivere le informazioni dei vari oggetti in modo che Tensorboard le interpreti correttamente.</p>

<p><strong>I Esempio</strong> di utilizzo: visualizzo i dati nel browser, consiste di 3 passi (piu’ l’istanza del writer appena fatta):</p>

<ol>
  <li>Costruisco una griglia di immagini <code class="language-plaintext highlighter-rouge">img_grid  = torchvision.utils.make_grid(example_data)</code> (usando le utility di torchvision)</li>
  <li>Do in pasto la griglia al writer, tramite il metodo add_image:  <code class="language-plaintext highlighter-rouge">writer.add_image('Immagini di Mnist', img_grid)</code>,
il primo argomento una label per la image grid, il secondo e’ la griglia stessa</li>
  <li>uso il metodo <code class="language-plaintext highlighter-rouge">writer.close()</code> per assicurarmi che tutti i dati siano mandati</li>
</ol>

<p><strong>II Esempio</strong> di utilizzo: visualizzo il grafo computazionale. Consiste di 2 passi.</p>
<ol>
  <li>uso il metodo   <code class="language-plaintext highlighter-rouge">writer.add_graph(model, examples_data.reshape(-1,28*28).to(device))</code>,quindi il primo argomento e’ il modello e il secondo sono ancora gli esempio (che ho flattenato). Attento, dato che sto mettendo tutto sul device, devo farlo anche per quanto riguarda examples_data, nel video non viene fatto! (questo perche’ il notebook usato non ha una scheda grafica dedicata e quindi sono comunque tutti sull’host.</li>
  <li>chiudo il writer per assicurarmi che tutti i dati vengano passati: <code class="language-plaintext highlighter-rouge">writer.close()</code></li>
</ol>

<p>Nota che dopo avere scritto l’esempio II,  appare una seconda scelta in tensorboard (in alto), chiamata graph. Si vede quindi il grafo computazionale che viene visualizzato. Con dei doppi click si aprono nel dettaglio i grafi!</p>

<p><strong>III Esempio</strong> di utilizzo: mando la loss e la accuracy durante l’esecuzione. Consiste di 2 passi. Vogliamo la loss media durante il training, quindi aggiungo delle variabili al mdoello.</p>
<ol>
  <li>uso un nuovo metodo: <code class="language-plaintext highlighter-rouge">writer.add_scalar('Training loss', running_loss/100, epoch* n_total_steps + i)</code>
Nota che ho definito prima delle nuove quantita’ da mandare al writer tramite add_scalar</li>
</ol>

<p><strong>Attento</strong> se continuo a fare girare la rete neurale nel notebook, i dati in Tensorboard si accumulano a quelli dei run precedenti. Devo trovare un modo per azzerare i dati.
<code class="language-plaintext highlighter-rouge">Soluzione</code>:<br />
devi rinominare la dir  dove vengono salvati i dati per ogni run diverso. In questo modo si avranno delle righe di colore diverso per ogni run. Altrimenti provo ad andare a cancellare il contenuto della dir <code class="language-plaintext highlighter-rouge">runs/mnist</code> (occhio che e’ una cartella che viene creata nella stessa dir di dove gira il python!) (magari esiste un metodo migliore devo investigare). Ho cancellato tutto ma non funge… ok funge, li teneva in memoria! ho riavviato tensorboard ed e’ sparito tutto. La cosa curiosa e’ che aprendo gli eventi sembrano vuoti… Solo il primo contiene molti valori, il resto sono gli incrementi sul primo direi.</p>

<p><strong>Attento</strong> quando ci sono degli scalari (dei grafici) di default Tensorboard aggiunge una linea di smoothing (e’ nella modale(?) subito a sinistra).</p>

<p><strong>IV Eempio</strong> di utilizzo: aggiungere una precision e recall curve (riguarda le note sulla confusoin matrix per le definizioni esatte). Esiste un metodo apposta per aggiungere la precision. Guarda il link: pytorch.org/docs/stable/tensorboard.html</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>   <span class="c1">##############  TENSORBOARD
</span><span class="kn">import</span> <span class="nn">sys</span> 
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1">##### costruisco un writer ####################
</span><span class="n">writer</span>  <span class="o">=</span><span class="n">SummaryWriter</span><span class="p">(</span><span class="s">'runs/mnist'</span><span class="p">)</span> <span class="c1"># come argomento serve la dir dove vanno salvati i file
#####    fine writer   ########################
</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>    
<span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">100</span>    
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>        
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>      
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> 

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span> <span class="p">,</span> <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_dataset</span>  <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># 
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">examples</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">examples_data</span><span class="p">,</span> <span class="n">examples_targets</span> <span class="o">=</span> <span class="n">examples</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
<span class="c1">#for i in range(6):
#    plt.subplot(2,3,i+1)
#    plt.imshow(example_data[i][0], cmap='gray')
</span>
    
<span class="n">img_grid</span>  <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">examples_data</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s">'Immagini di Mnist'</span> <span class="p">,</span> <span class="n">img_grid</span><span class="p">)</span>    
<span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>     <span class="c1"># questo assicura che tutti gli output sono flushati
#sys.exit()    # per non dover fare tutto il training    
</span>
<span class="c1">############  MODELLO ####################
</span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
       
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>       
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>   
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>             

<span class="c1">############### ISTANZIO MODELLO, Back e Forw ########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>   <span class="c1"># non ha bisogno di parametri
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>        


<span class="c1">######  Aggiungo un altro grafo alla dashboard di Tensorboard,
###### ora uso il metodo     add_graph   (prima ho usato add_image)
</span><span class="n">writer</span><span class="o">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">examples_data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="c1">#sys.exit()
</span>
<span class="c1">############### TRAINING LOOP ############
</span><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>


<span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>      <span class="c1"># questo e' il valore aggiornato in tempo reale
</span><span class="n">running_correct</span> <span class="o">=</span> <span class="mi">0</span>     <span class="c1"># idem
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                           
   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>         
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
              
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">images</span><span class="p">)</span>                           
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                 
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>     
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>            
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>          
        
<span class="c1">################ da mandare a TENSORBOARD ####################        
</span>        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># aggiorno il totale
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span>  <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">running_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="c1">################ ##################### ########################        
</span>        
        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="c1">#print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}')        
</span>
<span class="c1">############## qui aggiungo alla dashboard un nuovo oggetto TENSORBOARD ######
</span>            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">'Training loss'</span><span class="p">,</span> <span class="n">running_loss</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="n">epoch</span><span class="o">*</span> <span class="n">n_total_steps</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">,</span> <span class="n">running_correct</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="n">epoch</span><span class="o">*</span> <span class="n">n_total_steps</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> 
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>     <span class="c1"># riazzero
</span>            <span class="n">running_correct</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># riazzero
</span>            <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="c1">######################################################            
</span>            
            

        
        
        
<span class="c1">################ per Tensorboard ########################
</span><span class="n">labels2</span> <span class="o">=</span> <span class="p">[]</span>   <span class="c1">#occhio che aveva gia' definito labels sotto, ho messo un 2 Tensorboard
</span><span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#########################################################
</span>        
<span class="c1">############ TEST LOOP #################
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>           
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>    
        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>  
        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>           
        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">class_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>  <span class="c1"># ho bisogno di probabilita', quindi serve softmax
</span>        <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_predictions</span><span class="p">)</span>
        <span class="n">labels2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>   <span class="c1"># per TENSORBOARD
</span>        
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span> <span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">preds</span> <span class="p">])</span> <span class="c1"># tensorboard 2D oggetto 1000, 1
</span>    <span class="n">labels2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">labels2</span><span class="p">)</span>   <span class="c1"># Tensorboard concateno le liste in un oggetto 1D        1000
</span>    
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>   <span class="c1"># tutte le possibili classi  0-9  Tensorboard
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
        <span class="n">labels_i</span> <span class="o">=</span> <span class="n">labels2</span> <span class="o">==</span> <span class="n">i</span>    <span class="c1"># Tensorboard    non chiaro cosa fa
</span>        <span class="n">preds_i</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>      <span class="c1"># Tensorboard 
</span>        <span class="n">writer</span><span class="o">.</span><span class="n">add_pr_curve</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">labels_i</span><span class="p">,</span> <span class="n">preds_i</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># TEnsorboard
</span>        <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>       <span class="c1"># chiudo il writer
</span>    
    
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span>       
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>            
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accuracy =95.03
</code></pre></div></div>

<h1 id="io-saving-and-loading-models">I/O Saving and Loading Models</h1>
<p>https://www.youtube.com/watch?v=9L9jEOwRrCg&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=17&amp;ab_channel=PythonEngineer</p>

<p>Un modello viene salvato come un dictionary.</p>

<p>i 3 metodi da ricordare:</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">torch.save(arg, PATH)</code> posso salvare tensor, model o dictiionary  (e posso usare Pickle)</li>
  <li><code class="language-plaintext highlighter-rouge">torch.load(PATH)</code></li>
  <li><code class="language-plaintext highlighter-rouge">model.load_state_dict(arg)</code></li>
</ol>

<p>Ci sono 2 modi per salvare un modello: il modo <code class="language-plaintext highlighter-rouge">lazy</code> e modo <code class="language-plaintext highlighter-rouge">raccomandato</code></p>

<ol>
  <li>lazy: usando <code class="language-plaintext highlighter-rouge">torch.save(arg, PATH)</code> e poi carico il modello con <code class="language-plaintext highlighter-rouge">model =torch.load(PATH)</code>
A questo punto si usa:<br />
<code class="language-plaintext highlighter-rouge">model.eval()</code> in questo modo si entra in modalita’ evaluation (da controllare)</li>
</ol>

<p>Il difetto del metodo lazy e’ che i dati “serializzati” (intende compressi con pickle) seguono esattamente la classe e la struttura di quando sono salvati.</p>

<ol>
  <li>modo raccomandato: basta salvare <strong>SOLO</strong> i parametri del modello stesso: <br />
<code class="language-plaintext highlighter-rouge">torch.save(model.state_dict(), PATH)</code></li>
</ol>

<p>A questo punto devo creare un nuovo modello, e poi importare i parametri: <br />
<code class="language-plaintext highlighter-rouge">model= Model(*args, **kwargs)</code><br />
<code class="language-plaintext highlighter-rouge">model.load_state_dict(torch.load(PATH))</code> <br />
<code class="language-plaintext highlighter-rouge">model.eval()</code><br /></p>

<ul>
  <li>i modelli vengono salvati in file che hanno come estensione <code class="language-plaintext highlighter-rouge">pth</code></li>
</ul>

<p>Salviamo anche i checkpoint:<br />
un check point e’ un dizionario di dizionari!</p>
<ul>
  <li>i parametri del modello sono un dizionario</li>
  <li>i parametri dell’optimizer sono un dizionario
 -….</li>
</ul>

<p>Quindi quello che facciamo e’ un dizionario in cui la prima key e’ “model” e associamo
 il dizionario del modello, poi la chiave “optimizer” e il dizionario dell’ottimizzatore.
 Alla fine quando abbiamo bisogno usiamo checkpoint[“model”] e lui mi restituisce il dizionario
 con i parametri del modello (o se ho scritto optimizer, quelli dell’ottimizzatore).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">######## LAZY method #################
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  <span class="c1"># eredita un oggetto nn.Module
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">n_input_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y_pred</span>
        
<span class="n">model</span> <span class="o">=</span>  <span class="n">Model</span><span class="p">(</span><span class="n">n_input_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>


<span class="c1">######## SALVO IL MODELLO ############
</span><span class="n">FILE</span> <span class="o">=</span> <span class="s">"model.pth"</span>       <span class="c1"># solitamente i modelli hanno come estensione pth ?
</span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">FILE</span><span class="p">)</span>     

<span class="c1">######## CARICO IL MODELLO ###########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">FILE</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter containing:
tensor([[-0.3695,  0.2621,  0.0619, -0.2925, -0.3088, -0.2284]],
       requires_grad=True)
Parameter containing:
tensor([-0.2680], requires_grad=True)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">########## Metodo raccomandato ###########
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  <span class="c1"># eredita un oggetto nn.Module
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">n_input_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y_pred</span>
        
<span class="n">model</span> <span class="o">=</span>  <span class="n">Model</span><span class="p">(</span><span class="n">n_input_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="c1">#for param in model.parameters():
#    print('Prima',param)
</span>
<span class="c1">######## SALVO IL MODELLO ############
</span><span class="n">FILE</span> <span class="o">=</span> <span class="s">"model-raccomandato.pth"</span>       <span class="c1"># solitamente i modelli hanno come estensione pth ?
</span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">FILE</span><span class="p">)</span>  <span class="c1"># salvo solo i parametri del modello   
</span>
<span class="c1">######## Prima devo definire un modello #####
</span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">n_input_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>        <span class="c1"># creo un modello con la stessa struttura
</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">FILE</span><span class="p">))</span>  <span class="c1"># carico i parametri
</span><span class="n">loaded_model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>                             <span class="c1"># setto il modello in eval mode.
</span>

<span class="c1">#for param in loaded_model.parameters():
#    print('Dopo', param)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (linear): Linear(in_features=6, out_features=1, bias=True)
)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">############# Check point  ####################
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  <span class="c1"># eredita un oggetto nn.Module
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">n_input_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y_pred</span>
        
<span class="n">model</span> <span class="o">=</span>  <span class="n">Model</span><span class="p">(</span><span class="n">n_input_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

<span class="c1">####### CHECK point ######
</span>
<span class="n">checkpoint</span>  <span class="o">=</span> <span class="p">{</span>         <span class="c1"># e' un dizionario
</span>    <span class="s">"epoch"</span><span class="p">:</span><span class="mi">90</span><span class="p">,</span>         <span class="c1"># per esempio siamo alla epoca 90
</span>    <span class="s">"model_state"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">"optim_state"</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s">"checkpoint.pth"</span><span class="p">)</span>

<span class="c1">#  a questo punto posso caricare:
</span>
<span class="n">loaded_checkpoint</span>  <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">"checkpoint.pth"</span><span class="p">)</span>
<span class="n">epoch</span>  <span class="o">=</span><span class="n">loaded_checkpoint</span><span class="p">[</span><span class="s">"epoch"</span><span class="p">]</span> 
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">n_input_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># mettiamo 0 e poi carichiamo la corretta lr
</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">"model_state"</span><span class="p">])</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">"optim_state"</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>




</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}
{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}
</code></pre></div></div>

<h2 id="salvare-modelli-da-gpu">Salvare modelli da GPU</h2>
<p>Minuto 16 circa.</p>

<p>Gli esempi sopra fungono se tutt il modello e’ tenuto sulla CPU (sia per load che train, validation).</p>

<p><strong>map_location</strong>
argomento di <code class="language-plaintext highlighter-rouge">load_state_dict(PATH, map_location=device)</code></p>

<p>non e’ chiaro cosa intenda per <em>save on GPU</em> (io salvo su disco! magari intende che il modello e’ sulla GPU e poi lo metto su disco)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># SAVE sulla GPU, load sulla CPU
</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>


<span class="n">device</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>

<span class="c1">#####################################
</span>
<span class="c1"># SAVE sulla GPU, load sulla GPU
</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>


<span class="n">device</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                      


<span class="c1">#####################################
</span>
<span class="c1"># SAVE sulla CPU, load sulla GPU
</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>


<span class="n">device</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">"cuda:0"</span><span class="p">)</span>  <span class="c1"># 0 e' per la GPU zerp
</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                        
                      
                      
                      
                      
</code></pre></div></div>

<h1 id="creare-e-deployare-un-modello-pytorch-con-flask">Creare e deployare un modello pytorch con Flask</h1>
<p>questo lo salto per ora.</p>

<h1 id="recurrent-neural-network">Recurrent Neural Network</h1>

<p>Url di riferimento: https://www.youtube.com/watch?v=WEV61GmmPrk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=19&amp;ab_channel=PythonEngineer</p>

<p>Note e Slide: https://github.com/python-engineer/pytorch-examples</p>

<p>Lezione interessante del MIT sulle RNN
https://www.youtube.com/watch?v=qjrad0V0uJE&amp;ab_channel=AlexanderAmini</p>

<p>© Alexander Amini and Ava Soleimany  <br />
MIT 6.S191: Introduction to Deep Learning <br />
IntroToDeepLearning.com <br /></p>

<p><code class="language-plaintext highlighter-rouge">Attenzione</code> alla notazione: il <strong>hidden-tensor</strong> non e’ un <strong>hidden layer</strong> del modello! E’ un tensore che viene usato per la predizione ma non e’ l’input!</p>

<p><code class="language-plaintext highlighter-rouge">Nota</code> descrivo con qualche immagine in piu’ le RNN, LSTM e RCU nel prossimo capitolo.</p>

<p><code class="language-plaintext highlighter-rouge">Scopo</code>: costruire una rete neurale ricorrente RNN, che prenda una dopo l’altra le singole lettere di un nome (ogni lettera sara’ un input) e alla fine dell’ultima lettera dica a quale lingua appartiene il nome</p>

<ul>
  <li>usiamo batch di dimensione 1  qui (1 lettera)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">LOGICA</code></p>
<ul>
  <li>un nome e’ una sequenza di lettere.</li>
  <li>ogni lettera vine trasformata in un tensore-lettera che diventa <code class="language-plaintext highlighter-rouge">parte</code> dell’input della rete neurale.</li>
  <li>Perche’ ho scritto solo <code class="language-plaintext highlighter-rouge">parte</code> dell’input? Perche’ l’input e’ una concatenazione di un tensore-lettera e un tensore-hidden!</li>
  <li>la rete neurale restituisce in output a quale lingua appartiene la lettera … ma dato che in input c’e’ anche il tensore-hidden modificato dallo strato lineare (<strong>hidden tensor</strong>), c’e’ memoria delle lettere precedenti, quindi ha senso dire la lingua di appartenenza di una sola lettera: c’e’ comunque la memoria derivante dalle altre lettere ottenuta tramite il tensore hidden!</li>
  <li>come secondo output la rete neurale emette anche un nuovo tensore-hidden (che verra’ usato al passo successivo, concatenandolo al  tensore-lettera successivo)</li>
  <li>alla fine della parola mi deve dire di che lingua stiamo parlando.</li>
</ul>

<p>Le parole sono spezzate in modo da diventare sequenze di tensori, secondo la logica <strong>one-hot encoderd</strong>. Supponiamo di avere un alfabeto di 6 lettere: <code class="language-plaintext highlighter-rouge">a, e, i, o, u, l.</code>
    La parola <code class="language-plaintext highlighter-rouge">aiuola</code> diventa la seguente sequenza di tensori 1D contennti 6 ingressi:</p>

<p><strong>a</strong>= 
$\left( \begin{array}{c}
   {\bf 1}  <br />
   0  <br />
   0  \ 
   0  <br />
   0  <br />
   0  <br />
   \end{array} \right)$
,$~<del>$<strong>i</strong>= 
$\left( \begin{array}{c}
   0  <br />
   0  <br />
   {\bf 1}  \ 
   0  <br />
   0  <br />
   0  <br />
   \end{array} \right)$
,$</del>~$<strong>u</strong>= 
$\left( \begin{array}{c}
   0  <br />
   0  <br />
   0  \ 
   0  <br />
   {\bf 1}  <br />
   0  <br />
   \end{array} \right)$ 
,$~<del>$<strong>o</strong>= 
$\left( \begin{array}{c}
   0  <br />
   0  <br />
   0  \ 
   {\bf 1}  <br />
   0  <br />
   0  <br />
   \end{array} \right)$ 
,$</del>~$<strong>l</strong>= 
$\left( \begin{array}{c}
   0  <br />
   0  <br />
   0  \ 
   0  <br />
   0  <br />
  {\bf 1}  <br />
   \end{array} \right)$ 
,$~~~$<strong>a</strong>= 
$\left( \begin{array}{c}
  {\bf 1}  <br />
   0  <br />
   0  \ 
   0  <br />
   0  <br />
   0  <br />
   \end{array} \right)$</p>

<p>Il tensore hidden invece ha una dimensione che fissiamo noi (nell’esempio 128), al passo 0 viene inizializzato con tutti 0, 
ma ai passi successivi si popola perche’ lo strato lineare che costruisce le versioni successive del tensore hidden prende in ingresso sia il tensore combinato che il tensore della lettera. Per esempio se la lettera passata e’ la $a$ e il tensore-hidden e’:<br />
<strong>hidden</strong> = 
$\left( \begin{array}{c}
  {\bf 0.3}  <br />
   0.23  <br />
   \dots  \ 
   0.29  <br />
   0.52  <br />
   0.11  <br />
   \end{array} \right)$ 
$\begin{array}{c}
   1  <br />
   2  <br />
   \dots  \ 
   126  <br />
   127  <br />
   128  <br />
   \end{array}$  <br />
Allora il <strong>tensore-combinato</strong> = 
$\left(\begin{array}{c}
     {\bf a}  <br />
    {\bf hidden}  <br />
   \end{array} \right)$ =
$\left( \begin{array}{c}
   {\bf 1}  <br />
   0  <br />
   0  \ 
   0  <br />
   0  <br />
   0  <br />
  {\bf 0.3}  <br />
   0.23  <br />
   \dots  \ 
   0.29  <br />
   0.52  <br />
   0.11  <br />
   \end{array} \right)$ 
(in questo esempio  il tensore combinato ha dimensione 6 + 128 (perche’ in questo esempio l’alfabeto per  l’one-hot encoring contiene solo <em>aeioul</em>, mentre la dimensione del tensore combinato nel video di Python Engineer e’ 57 +128, perche’ l’alfabeto da lui usato contiene tutte le maiuscole, le minuscole e alcuni segni di punteggiatura.</p>

<p><img src="/images/posts/pytorch/rnn-nomi-lingue.png" alt="cane" /></p>

<p><code class="language-plaintext highlighter-rouge">Osservazione</code>
Il tensore che viene dato in pasto alla fully connected layer e’ la versione concatenata di:</p>
<ul>
  <li>one-shot encoded  (che ha dim 57 nel nostro caso, le maiuscole, minuscole e qualche segno di punteggiatura</li>
  <li>hidden tensor (che ha dimensione 128, perche’ lo abbiamo scelto noi).</li>
</ul>

<p>Questo tensore concatenato viene dato in pasto anche ad un’altra fully connected layer in modo da mantenere la
memoria di quanto e’ successo.</p>

<p>Nel video del MIT si dice che per fare il training di una RNN si usano la cosiddetta: <code class="language-plaintext highlighter-rouge">BPTT</code> (Backpropagation through time)
occhio che nel nostro caso non mi pare che si faccia.  Nel dettaglio la backpropagation si fa sulla loss soltanto, e quindi l’hidden tensor e’ considerato come un input non come un peso. Per questo non mi e’ chiaro come venga aggiornato il tensore dei pesi che ho chiamato W2 nell’immagine.</p>

<p><code class="language-plaintext highlighter-rouge">PROBLEMI</code></p>

<ul>
  <li>
    <p>ho provato a mandare tutto sul device CUDA ma rallenta! sospetto che sia perche’ copio sul device di volta in volta.
Il tempo che impiega (su 5000 passi) e’ 23 s col device e 13 sulla cpu! Vediamo se riesco ad evitare di copiare le cose in GPU ogni passaggio per velocizzare il calcolo. Mettendo line_tensor e hidden prima ho guadagnao, ora sono 20 s (comunque piu’ lento che con la CPU)</p>
  </li>
  <li>
    <p>Che strano: rifacendolo andare qualche giorno successivo ci mette 307 secondi! (sulla CPU) e se provo a farlo andare sulla GPU dice che ci sono problemiperche’ alcune cose sono su CPU e altre su GPU.  Seguendo le indicazioni ho messo line_tensor.to(device) e ora sembra fungere linea 188.</p>
  </li>
  <li>
    <p>ho inserito tutto fino a quando fa “whole sequence/name” ma ottengo un errore non ben chiaro:
“IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)”. Errore trovato: avevo scritto 
<code class="language-plaintext highlighter-rouge">inout_tensor</code> invece che <code class="language-plaintext highlighter-rouge">input_tensor</code></p>
  </li>
  <li>
    <p>Il tensore in uscita ha dimensione 128 invece che 18 (il numero di lingue) e non capisco perche’! Chiaro avevo scritto: 
<code class="language-plaintext highlighter-rouge">self.i2o = nn.Linear(input_size + hidden_size , hidden_size)</code> che quindi mi dava come dimensione di uscita del layer lineare <code class="language-plaintext highlighter-rouge">hidden_size</code> invece che <code class="language-plaintext highlighter-rouge">output_size</code>!</p>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Note</code>:
Nella lezione si fa uso di funzioni di aiuto “helper functions” (io all’inizio capivo alpha-functions…). Python Engineer le mette in un modulo, mentre io le ho riscritte all’inizio del codice qui sotto</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dati https://download.pytorch.org/tutorial/data.zip
</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">glob</span>       <span class="c1"># ?
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="c1">#  Helper Functions
</span>
<span class="c1"># Alfabeto minuscolo e maiuscolo
</span><span class="n">ALL_LETTERS</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span> <span class="s">".,;''"</span>   <span class="c1"># insieme delle lettere e della punteggiatura usata
</span><span class="n">N_LETTERS</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ALL_LETTERS</span><span class="p">)</span>

<span class="c1"># converti un UNICODE in ASCII grazie a https://www.stackoverflow.com/a/518232/2809427
# in pratica trasforma le lettere accentate in lettere NON accentate
</span><span class="k">def</span> <span class="nf">unicode_to_ascii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span> <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s">'NFD'</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s">"Mn"</span> <span class="ow">and</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">ALL_LETTERS</span><span class="p">)</span>

<span class="c1"># costuisce un dizionario "category_lines" e una lista di nomi per le varie lingue
</span><span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
    <span class="n">category_lines</span> <span class="o">=</span> <span class="p">{}</span>     <span class="c1"># dizionario
</span>    <span class="n">all_categories</span> <span class="o">=</span> <span class="p">[]</span> 

    <span class="k">def</span> <span class="nf">find_files</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>    <span class="c1"># glob??? The glob module finds all the pathnames matching a specified pattern according to ...
</span>    
    <span class="c1"># leggi un file e spezzalo in linee
</span>    <span class="k">def</span> <span class="nf">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">unicode_to_ascii</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">find_files</span><span class="p">(</span><span class="s">'data/names/*.txt'</span><span class="p">):</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">all_categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
    
        <span class="n">lines</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
        <span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">]</span>  <span class="o">=</span><span class="n">lines</span>
        
    <span class="k">return</span> <span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span>
        
<span class="c1"># trova l'indice di posizione associato ad una lettera nalla parola
</span>
<span class="k">def</span> <span class="nf">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ALL_LETTERS</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">letter</span><span class="p">)</span>

<span class="c1"># trasforma una lettera in un tensore 1 x n letters (tensore riga)
</span>
<span class="k">def</span> <span class="nf">letter_to_tensor</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_LETTERS</span><span class="p">)</span>  
    <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="c1"># trasforma una linea in una &lt;line_length x 1 x n_letters&gt;
# o un array hon-hot letter
</span>
<span class="k">def</span> <span class="nf">line_to_tensor</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_LETTERS</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">letter</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
        <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">letter_to_index</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="k">def</span> <span class="nf">random_training_example</span><span class="p">(</span><span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">random_choice</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="n">random_idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
    
    <span class="n">category</span> <span class="o">=</span> <span class="n">random_choice</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
    <span class="n">line</span> <span class="o">=</span><span class="n">random_choice</span><span class="p">(</span><span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">])</span>
    <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">all_categories</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">category</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span>
    <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">line_to_tensor</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span>


<span class="c1">###############################################################################
###############################################################################
###############################################################################
#print(ALL_LETTERS)
#category_lines , all_categories = load_data()
#print(category_lines['Italian'][:5])
#print(letter_to_tensor('J'))                # trasforma la lettera J (maiuscola) in un tensore one-hot encoding (sono 1D con 57 ingressi)
#print(line_to_tensor('Jones').size())       # Jones contiene 5 lettere, ognuna e' trasformata in un tensore 1D con 57 ingressi 
</span>
<span class="c1">#test = line_to_tensor('Jones')
#print(test)
##################  ok sopra funge correttamente   ###################
</span>

<span class="c1"># ho gia'importato torch sopra
</span><span class="kn">import</span> <span class="nn">torch.nn</span>  <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># from utils import ALL_LETTERS, N_LETTERS # qui non serve perche' fanno gia' parte di questo listato
# from load_data, letter_to_tensor, ...  
</span>
<span class="c1">#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>


<span class="c1"># esiste gia' un RNN in Torch, qui pero' lo creiamo da zero.
</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span> <span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>   <span class="c1"># ricorda che servono solo le dimensioni: dim tensore combinato, dim uscita
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span> <span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>   <span class="c1"># input 2 output
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># input ha dimensione 1,57, quindi softmax sulle colonne
</span>        
    
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden_tensor</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden_tensor</span> <span class="p">),</span><span class="mi">1</span><span class="p">)</span>   <span class="c1">#concatena input tensor e hidden tensor: nuova dimensione= input_size+hidden_size
</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>      <span class="c1"># qui spara fuori l'oggetto hidden, i pesi di questo layer non sono l'oggetto hidden!
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>      <span class="c1"># qui indica la guess riguardo la nazione
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>    <span class="c1"># qui usa la softmax per ottenere valori di probabilita'
</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>            <span class="c1"># restituisce sia l'output che l'hidden per il prossimo passo
</span>    
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>                             <span class="c1"># inizializza lo hidden_tensor alla dimensione hidden_size
</span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>


<span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>  <span class="c1"># chiave valore, nazione chiave, nome valore, e poi tutte le nazioni
</span><span class="n">n_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
<span class="c1">#print(n_categories)
</span>
<span class="n">n_hidden</span>  <span class="o">=</span><span class="mi">128</span>   <span class="c1"># selto da me
</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">N_LETTERS</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># qui ho inizializzato il modello, servono i parametri del costruttore
</span>


<span class="c1"># likelyhood di ogni nazione, vogliamo l'indice della categoria massima
</span><span class="k">def</span> <span class="nf">category_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">category_idx</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">all_categories</span><span class="p">[</span><span class="n">category_idx</span><span class="p">]</span>
    
<span class="c1">#print(category_from_output(output))
#print(output)
</span>
<span class="c1">####### facciamo il training ######
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>      <span class="c1"># negative log likelihood loss
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>         <span class="c1">#
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1">#  funzione helper che fa il training metto il tensore e la sua label
</span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">):</span>    

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                    <span class="c1"># azzero l'hidden tensor iniziale
</span>    <span class="c1">#        hidden = hidden.to(device)
</span>    <span class="c1">#line_tensor = line_tensor.to(device)
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span> 
        <span class="c1"># lunghezza del nome
</span>        <span class="c1">#l2d = line_tensor[i].to(device)
</span>        
<span class="c1">#        output, hidden = rnn(l2d, hidden)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>  <span class="c1"># inserisco i 2 tensori di input: lettera one-shot e hidden
</span>     
    <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">category_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1">#output = output.to(device) # inventato da me ma da comunque errore
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>




<span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">all_losses</span>  <span class="o">=</span> <span class="p">[]</span>
<span class="n">plot_steps</span><span class="p">,</span> <span class="n">print_steps</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span> <span class="mi">100000</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">100000</span>


<span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>     <span class="c1"># il momento d'inizio
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">random_training_example</span><span class="p">(</span><span class="n">category_lines</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">)</span>
    
    <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">line_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">output</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="n">plot_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">all_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span> <span class="o">/</span><span class="n">plot_steps</span><span class="p">)</span>
        <span class="n">current_loss</span> <span class="o">=</span><span class="mi">0</span>
        
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="n">print_steps</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">category_from_output</span> <span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="s">"Corretto"</span> <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">category</span> <span class="k">else</span> <span class="n">f</span><span class="s">" Sbagliato ( {category})"</span>
        <span class="k">print</span> <span class="p">(</span><span class="n">f</span><span class="s">'{i} {i/n_iters*100} {loss:.4f} {line}/{guess}{correct}  '</span> <span class="p">)</span>
        
        
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span> <span class="mi">100000</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Tempo impiegato '</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span><span class="n">since</span><span class="p">)</span>   
        
        
<span class="c1">#plt.figure()
#plt.plot(all_losses)
</span>
<span class="c1">#############  data una stringa in ingresso 
</span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">input_line</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\n</span><span class="s">&gt; {input_line}'</span><span class="p">)</span>                        <span class="c1"># la riscrive 
</span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="n">line_tensor</span>  <span class="o">=</span> <span class="n">line_to_tensor</span><span class="p">(</span><span class="n">input_line</span><span class="p">)</span>     <span class="c1"># trasforma in un tensore
</span>        
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>                    <span class="c1"># crea lo stato iniziale vuoto da dare in pasto
</span>        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>       <span class="c1"># gira sui tensori one-hot encoded
</span>            <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>   <span class="c1"># usa la rete neurale, uno dietro l'altro, cosi' hidden si aggiorna
</span>            
        <span class="n">guess</span> <span class="o">=</span> <span class="n">category_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>          <span class="c1"># Ottiene dal numero il nome della nazione
</span>        <span class="k">print</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>                                  <span class="c1"># stampa il nome della nazione 
</span>    
            
<span class="c1">########## qui si imparano molte cose #######
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">"Inserisci un nome (quit per terminare)"</span><span class="p">)</span>      <span class="c1"># scrive a video
</span>    <span class="k">if</span> <span class="n">sentence</span>  <span class="o">==</span> <span class="s">'quit'</span><span class="p">:</span>                     <span class="c1"># esci dal ciclo se scrivi quit
</span>        <span class="k">break</span>
    <span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>                           <span class="c1"># usa la rete neurale e scrivi la predizione
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>99999 99.99900000000001 1.3037 Woo/Chinese Sbagliato ( Korean)  
Tempo impiegato  319.30727434158325
Inserisci un nome (quit per terminare)quit
</code></pre></div></div>

<h1 id="rnn-gru-e-lstm">RNN, GRU e LSTM</h1>

<p>url: https://www.youtube.com/watch?v=0_PgWWmauHk&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=20&amp;ab_channel=PythonEngineer</p>

<p>nn.RNN: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html</p>

<p>Stanford: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
Questa e’ probabilmente la guida piu’ precisa, la matematica usa delle notazioni diverse dalle altre due (per esempio l’hidden tensor viene indicato con a), masuppongo che sia la piu’ affidabile.</p>

<p>Una guida illustrata su LSTM e GRU: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 qui cerca di non mettere la matematica, ma ci sono delle gif animate.</p>

<p>Qui indica anche un po’ di matematica: http://dprogrammer.org/rnn-lstm-gru  ma sospetto che le formule non corrispondano alle immagini (ha preso le immagini da qualche parte e le formule altrove). Di buono mette anche le formule per la backpropagation!</p>

<p>Qui usiamo i moduli gia’ creati per Long Short Term Memory e per GRU. Si prende come punto di partenza il tutorial 13.</p>

<p>Invece che guardare a tutta una immagine per volta vogliamo prendere una <code class="language-plaintext highlighter-rouge">sequenza</code> di righe</p>

<p>Usiamo Architettura Many to 1 (molti input e un solo output)</p>

<p>Commentando e decommentando le parti con GRU e LSTM si ottengono tutte e 3 le architetture.
 In realta’ sono solo 2 le righe che vanno cambiate tra GRU e RNN e 3 per LSTM (si deve mettere anche la cella)!</p>

<p>Nel codice qui sotto si fa una sorta di estensione rispetto al lavoro fatto nel capitolo 13 (Feed Forward NN )</p>

<p><code class="language-plaintext highlighter-rouge">RNN</code> di torch.nn.RNN e’ una Elman: 
<script type="math/tex">\large
\displaystyle
h_t = \tanh\left( \frac{} {} 
  W_{ih} x_t +b_{ih}  + W_{hh} h_{t-1}+b_hh 
  \right)</script></p>
<ul>
  <li>$h_t$ hidden state al tempo $t$</li>
  <li>$h_{t-1}$ hidden state al tempo $t-1$ (eh… abbastanza ovvio)</li>
  <li>$x_t$ input al tempo $t$</li>
  <li>$W_{ih}$ sono i <strong>pesi</strong> che contribuiscono a $h_t$ dal tensore di input $x_t$</li>
  <li>$b_{ih}$ sono i <code class="language-plaintext highlighter-rouge">bias</code> che contribuiscono a $h_t$ dal tensore di input</li>
  <li>$W_{hh}$ sono i <strong>pesi</strong> che contribuiscono a</li>
  <li>$b_{hh}$ sono i <code class="language-plaintext highlighter-rouge">bias</code> (non chiaro perche’ vengano distinti rispetto a b_{ih}, alla fine sono delle costanti…</li>
  <li>$x_t$ vettore di input al tempo t</li>
  <li>$\hat{y}_t$ vettore di output al tempo t</li>
  <li>$y_t$ le label vere (ground truth) al tempo t</li>
</ul>

<script type="math/tex; mode=display">\large
\displaystyle
\hat y_t = 
  W_{hy} h_t +b_{hy}</script>

<p><code class="language-plaintext highlighter-rouge">Empirical Loss</code>:</p>
<ul>
  <li>Quando si ha una RNN si hanno diversi output per ognuno dei “tempi” t</li>
  <li>Si sommano i valori delle loss.</li>
</ul>

<p>ok a questo punto pero’ dovrebbero anche esserci i pesi per l’output, nella formula sopra vedo solo l’equazione per l’hidden state.</p>

<p>Vediamo un grafico per <strong>LSTM</strong>:
<img src="/images/posts/pytorch/LSTM.jpg" alt="LSTM" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="c1">#device = torch.device('cpu')
</span>
<span class="c1"># Hyper parametri 
#input_size = 28*28    #  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">128</span>   <span class="c1">#  scelto da me
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>        <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1">#  questo no so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#  piccolo
</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span>        <span class="c1"># singolo input e' la riga     RNN
</span><span class="n">sequence_length</span> <span class="o">=</span><span class="mi">28</span>    <span class="c1"># ci sono 28 righe             RNN
</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>         <span class="c1"># di default =1                RNN
</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
                                           <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                          <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># 
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span>  <span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">### nota che la dimensione dei sample e' la seguente
# 100  = numero di immagini nel batch (se non metti batch_size, di default vale 1)
#   1  = numero di canali solitamente i colori
#  28  =  numero di ingressi sull'asse delle x
#  28  = numero di ingressi sull'asse delle y
</span>        

<span class="c1">############  MODELLO ####################
</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>   
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>     <span class="c1"># RNN
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>   <span class="c1"># RNN
</span>        <span class="c1">#self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)  # RNN , l'ordine e' importante batch set first dimension RNN
</span>        <span class="c1">#self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # GRU 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># LSTM         
</span>        
        <span class="c1"># x -&gt; batch_size, seq, input_size
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="c1"># RNN questo e' per l'ultimo passo della sequenza per avere la classificazione
</span>       
    
    <span class="c1"># RNN da documentazione ora servono 2 input, uno e' lo stato e l'altro e' l'hidden state
</span>    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># RNN numero di layer, batch size, hidden size
</span>        
        <span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># LSTM initial cell  
</span>        
        
        <span class="c1">#out, _ = self.rnn(x, h0) # RNN restituisce 2 outputs, uno out e hidden state per step n
</span>        <span class="c1">#out, _ = self.gru(x, h0) # GRU restituisce 2 outputs, uno out e hidden state per step n       
</span>        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span><span class="n">c0</span><span class="p">))</span> <span class="c1"># LSTM restituisce 2 outputs, uno out e hidden state per step n       
</span>        
        <span class="c1"># RNN batch_size, sequence_length, hidden_size
</span>        
        <span class="c1"># RNN vogliamo l'hidden state dell'ultimo step
</span>        <span class="c1"># RNN out (N, 28, 28)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># RNN serve solo l'ultimo time step quindi metto -1 e tutte le feature dell'hidden size
</span>        <span class="c1"># RNN out(N, 128)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># RNN
</span>        <span class="k">return</span> <span class="n">out</span>
        
        

<span class="c1">############### ISTANZIO MODELLO, Loss, optimizer  ########
</span><span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>  <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>                                         <span class="c1"># 
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>       <span class="c1"># 
</span><span class="n">n_total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>



<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>                          <span class="c1">#   
</span>   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span> <span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>   <span class="c1">#      
</span>        <span class="c1"># 100, 1, 28, 28  (batch, canali, x, y) forma del tensore images
</span>        <span class="c1"># 100, 28x28=784  forma voluta dall'hidden layer
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>     <span class="c1"># Ora vogliamo solo righe e tante.
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
              
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">images</span><span class="p">)</span>        <span class="c1">#  non chiama il metodo forward: perche'?
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                 
        <span class="c1"># backward pass
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>    <span class="c1"># 
</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1"># 
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         <span class="c1"># 
</span>        
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}'</span><span class="p">)</span>        

            
<span class="c1">############ TEST LOOP e' identico a RNN e FeedForward! #################
</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>    
    <span class="n">n_correct</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># numero di predizioni azzeccate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># ? 
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>               <span class="c1"># qui il modello e' gia' trainato!
</span>        
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># prendo la classe che ha il valore massimo
</span>        <span class="n">n_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># numero di samples nel batch corrente (nell'ultimo sono diversi spesso)
</span>        <span class="n">n_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="o">*</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_samples</span> <span class="c1"># accuratezza in percentuale
</span>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'accuracy ={acc}'</span><span class="p">)</span>            
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 1 / 2, step 100/600, loss= 0.9440
epoch 1 / 2, step 200/600, loss= 0.5394
epoch 1 / 2, step 300/600, loss= 0.3369
epoch 1 / 2, step 400/600, loss= 0.1903
epoch 1 / 2, step 500/600, loss= 0.2440
epoch 1 / 2, step 600/600, loss= 0.1913
epoch 2 / 2, step 100/600, loss= 0.1267
epoch 2 / 2, step 200/600, loss= 0.1024
epoch 2 / 2, step 300/600, loss= 0.0285
epoch 2 / 2, step 400/600, loss= 0.1911
epoch 2 / 2, step 500/600, loss= 0.1058
epoch 2 / 2, step 600/600, loss= 0.1007
accuracy =97.46
</code></pre></div></div>

<h1 id="pytorch-lightning">PyTorch Lightning</h1>

<p>https://www.youtube.com/watch?v=Hgg8Xy6IRig&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=21&amp;ab_channel=PythonEngineer</p>

<p>PyTorch Lightning  e’ un wrapper per velocizzare la scrittura di reti neurali, con Pytorch.</p>

<p>Sito di Pytorch Lightning
https://www.pytorchlightning.ai/</p>

<p>Per istallarlo: <code class="language-plaintext highlighter-rouge">conda install pytorch-lightning -c conda-forge</code></p>

<p><strong>Non</strong> e’ piu’ necessario:</p>
<ul>
  <li>model.train() (ovvero settare il modello  in  training mode)</li>
  <li>model.eval()  (ovvero settare il modello  in  evaluation mode)</li>
  <li>definire una <code class="language-plaintext highlighter-rouge">device</code> e fare model.to(device)  si puo’ “sconnettere” la GPU facilmente</li>
  <li>optimizer.zero_grad()</li>
  <li>loss.backwards()</li>
  <li>optimizer.step()</li>
  <li>with torch.nograd()</li>
  <li>x= x.detach</li>
</ul>

<p><strong>Bonus</strong></p>
<ul>
  <li>stampa consigli e aiuti!</li>
  <li>supporto Tensorboard: viene costruito un folder chiamato <code class="language-plaintext highlighter-rouge">lightning_logs</code>.
Per usare tensorboard <code class="language-plaintext highlighter-rouge">tensorboard  --logdir lightning_logs</code>  (e’ il nome della dir creata in automatico, nel video fa un errore e scrive log_dir)</li>
</ul>

<p>A questo punto per fare inspecting del training, creiamo un altro dict.</p>

<p>Usiamo il codice del tutorial 13 e lo modifichiamo per PyTorch Lightning.</p>

<p><code class="language-plaintext highlighter-rouge">Suggerimenti</code>:
-suggerisce di usare il metodo <code class="language-plaintext highlighter-rouge">validation_epoch_end()</code> per accumulare statistiche. Nota che nel video Loeber copia i metodi dal sito e li modifica per l’occasione. Questo metodo viene poi piazzato all’interno del modello</p>

<p><code class="language-plaintext highlighter-rouge">Domande</code>:
mi viene GPU present = True, used False, devo accendere l’uso della GPU. Si va nel metodo <code class="language-plaintext highlighter-rouge">Trainer</code> e si mette: <code class="language-plaintext highlighter-rouge">trainer=Trainer(gpus=1, max_epochs=num_epochs, fast_dev_run =True)</code>. Ok ho controllato e ora la GPU e’ presente e usata… ma il tempo di esecuzione praticamente non cambia!</p>
<ul>
  <li>Si puo’ usare anche una TPU e anche un <code class="language-plaintext highlighter-rouge">distributed backend</code> una DDP</li>
  <li>Si puo’ passare a precisione 16 bit</li>
  <li>in Trainer si puo’ mettere anche il karg: <code class="language-plaintext highlighter-rouge">auto_lr_find =True</code> per il learning rate</li>
  <li>in Trainer si puo’ mettere anche il karg: <code class="language-plaintext highlighter-rouge">deterministic =True</code> per riprodurre esattamente i risultati</li>
  <li>in Trainer si puo’ mettere anche il karg: <code class="language-plaintext highlighter-rouge">gradient_clip_val =0.3</code> (un numero tra 0 e 1 per fare clipping dei gradienti)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Problemi</code>:<br /></p>
<ul>
  <li>nel video si vede la loss che scende in basso a dx mentre per me e’ un NAN. Ok il problema era che non facevo “ritornare” nulla dal metodo <code class="language-plaintext highlighter-rouge">training_step</code> che invece deve restituire un dict della forma {‘loss’:loss}, che viene preso direttamente dal PL e mostrato nella barra sotto</li>
  <li>non vedo apparire la fase di validazione con una barra che si riempie (nel video c’e’)</li>
  <li>mi da il seguente warning: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0
Please use self.log(…) inside the lightningModule instead.
#log on a step or aggregate epoch metric to the logger and/or progress bar (inside LightningModule)
self.log(‘train_loss’, loss, on_step=True, on_epoch=True, prog_bar=True)
warnings.warn(*args, **kwargs)</li>
  <li>In tensor borad non riesco a trovare train_loss (e’ uno scalare ma non lo trovo)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span> 

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="n">pl</span> <span class="c1"># PL
</span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span> 

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># device config
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>


<span class="c1"># Hyper parametri 
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>      <span class="c1">#  =784 sono le dimensioni delle immagini
</span><span class="n">hidden_size</span>  <span class="o">=</span> <span class="mi">500</span>      <span class="c1">#  scelto da me
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>        <span class="c1">#  devo classificare immagini di numeri
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>          <span class="c1">#  quanti giri completi vengono fatti
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1">#  questo non so come sia stato scelto
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>   <span class="c1">#  piccolo
</span>
<span class="c1">#train_dataset = torchvision.datasets.MNIST(root= './data/', train= True, transform =transforms.ToTensor(), download = True)
#test_dataset = torchvision.datasets.MNIST(root= './data/', train= False, transform =transforms.ToTensor(), download = False) # ho gia' scaricato tutto con il train_datast
</span>
<span class="c1">#train_loader = torch.utils.data.DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle=True)
#test_loader = torch.utils.data.DataLoader (dataset=  test_dataset, batch_size = batch_size, shuffle=False)
</span>

<span class="c1">############  MODELLO ####################
</span>

<span class="c1">#class NeuralNet(nn.Module):             #vecchi
</span><span class="k">class</span> <span class="nc">LitNeuralNet</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>  <span class="c1"># nome scelto da noi, pl.LightningModule e' la versione super di nn.Module    
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LitNeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
       
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>    <span class="c1"># x e' l'input.
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># 
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># 
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>            <span class="c1"># 
</span>
    
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>  <span class="c1"># PL non uso piu' i loop!!!! 
</span>        <span class="c1">#x,y = batch           # unpack
</span>        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span>  <span class="o">=</span><span class="n">batch</span> <span class="c1"># PL unpack
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>   <span class="c1"># non e' piu' necessairo to(device)
</span>        
        <span class="c1">#y_hats = self(x)     # ????
</span>        <span class="n">outputs</span>  <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>  <span class="c1"># PL questo e' il forward pass! usiamo self perche' usiamo direttamente questo modulo
</span>        
        <span class="c1">#loss = F.cross_entropy(y_hat, y)
</span>        <span class="n">loss</span> <span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="n">tensorboard_logs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'train_loss'</span><span class="p">:</span><span class="n">loss</span><span class="p">}</span>    <span class="c1"># devo metterlo anche nel validatio_epoch_end
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">'loss'</span><span class="p">:</span><span class="n">loss</span><span class="p">,</span> <span class="s">'log'</span><span class="p">:</span><span class="n">tensorboard_logs</span><span class="p">}</span>   <span class="c1"># PL i nomi delle chiavi sono FISSI: e' log non Log
</span>        <span class="c1">#return {'loss': loss}
</span>    
    
    
    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># PL ma il nome e' fissato da PL? penso di si'
</span>        <span class="c1">#optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate)  # non ho capito se serve o no...   
</span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>  <span class="c1">#  PL self e' l'istanza del modello
</span>    
    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#dataset = MNIST(os.getcwd(), train=True, download=True, transform =transforms.ToTensor())
</span>        <span class="c1">#loader = DataLoader (dataset, batch_size= 32, num_workers=1, shuffle=True)   #lui ha messo num_workers = 4
</span>        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">train_loader</span>
    
    <span class="c1"># il nome deve essere questo!
</span>    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="c1"># ho gia' scaricato tutto con il train_datast
</span>        <span class="n">val_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="p">(</span><span class="n">dataset</span><span class="o">=</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">val_loader</span>    
    
    
    <span class="c1"># questo viene eseguito dopo ogni epoch di validazione
</span>    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
     
        <span class="n">tensorboard_logs</span>  <span class="o">=</span> <span class="p">{</span><span class="s">'tavg_val_loss'</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">}</span> <span class="c1"># PL per ora non lo guardiamo
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">'tavg_val_loss'</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">,</span> <span class="s">'log'</span><span class="p">:</span><span class="n">tensorboard_logs</span><span class="p">}</span>
        <span class="c1">#return {'val_loss': avg_loss}    
</span>    

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1">#trainer = Trainer(max_epochs = num_epochs, fast_dev_run = False)  # PL Trainer e' importato da PL
</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">max_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">fast_dev_run</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>  <span class="c1"># PL Trainer e' importato da PL
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LitNeuralNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>        <span class="c1"># PL istanzio il modello  
</span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>                                                <span class="c1"># faccio il training sul modello
</span><span class="k">print</span><span class="p">(</span><span class="s">"Tempo training+test= "</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>    
    

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU available: True, used: True
TPU available: None, using: 0 TPU cores

  | Name | Type   | Params
--------------------------------
0 | l1   | Linear | 392 K 
1 | relu | ReLU   | 0     
2 | l2   | Linear | 5.0 K 
--------------------------------
397 K     Trainable params
0         Non-trainable params
397 K     Total params
1.590     Total estimated model params size (MB)



HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…



Tempo training+test=  12.651966094970703
</code></pre></div></div>

<h1 id="lr-scheduler">LR Scheduler</h1>
<p>url di riferimento: https://www.youtube.com/watch?v=81NJgoR5RfY&amp;list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&amp;index=22&amp;ab_channel=PythonEngineer</p>

<p>Vediamo come sfruttare le funzioni di Pytorch che automaticamente modificano il learning rate per ottenere dei risultati ottimali.</p>

<p>Spesso (intuitivamente) si vuole diminuire il <code class="language-plaintext highlighter-rouge">Learning Rate</code>. La logica a mio avviso e’ la seguente, quando mi avvicino al minimo se non diminuisco il LR rischio di saltare da una parte all’altra del minimo stesso.</p>

<p>Per questo si usa uno scheduler.
Onestamente non mi pare ultra necessario, uno puo’ costruire delle funzioni custom che facciano la cosa ogni volta.  Anzi se uso una funzione che non ho scritto io c’e’ la possibilita’ che io non controlli perfertamente.</p>

<p>Una interessante e’ che si riduce solo se una certa metrica ha raggiunto un plateau.</p>

<p><code class="language-plaintext highlighter-rouge">Osservazioni</code></p>
<ul>
  <li>in python // e’ floor division.</li>
</ul>

<h1 id="autoencoder">Autoencoder</h1>

<p>https://www.youtube.com/watch?v=zp8clK9yCro&amp;t=1075s&amp;ab_channel=PythonEngineer</p>

<p>Un autoencoder e’ una rete neurale che cerca di “riassumere i tratti principali dell’input. L’idea e’ molto sempice, 
Pensiamo a delle immagini. Si prende e si fanno vari strati che hanno via via meno parametri. A quel punto si fanno i passi inversi (letteralemente) in modo da riottenere un medesimo numero di valori finali. Come funzione di Loss si usa una MSE. E’ intuitivo. se ho una immagine in ingresso voglio vedere la STESSA immagine in uscita (o almeno avvicinarmi)</p>

<p>Risorsa da cui sono in pratica presi i codici:
https://www.cs.toronto.edu/~lczhang/360/lec/w05/autoencoder.html</p>

<p>Qui il corso da cui sono prese i codici particolari dell’autoencoder:
https://www.cs.toronto.edu/~lczhang/360/</p>

<h2 id="senza-convoluzioni">Senza convoluzioni</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>  
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span> <span class="o">=</span><span class="n">mnist_data</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nb">type</span><span class="p">(</span><span class="n">mnist_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">len</span><span class="p">(</span><span class="n">mnist_data</span><span class="p">[</span><span class="mi">59999</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">type</span><span class="p">(</span><span class="n">mnist_data</span><span class="p">[</span><span class="mi">59999</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">mnist_data</span><span class="p">[</span><span class="mi">59999</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">size</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>784
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span> 
<span class="n">dataiter</span>  <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Autoencoder</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># N= 784 (28x28)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span>           <span class="c1"># numero di punti in ingresso (28*28) e neuroni in uscita(128)
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span>              
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span>               
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span>                <span class="c1"># 
</span>        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span>           <span class="c1"># numero di punti in ingresso (28*28) e neuroni in uscita(128)
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span>              
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span>               
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>                <span class="c1"># 
</span>        <span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span> <span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span> <span class="mf">1e-3</span> <span class="p">,</span> <span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>  
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">img</span>   <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">recon</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">recon</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        
        
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch+1}, Loss:{loss.item():.4f}'</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">epoch</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">recon</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 1, Loss:0.0426
Epoch: 2, Loss:0.0405
Epoch: 3, Loss:0.0284
Epoch: 4, Loss:0.0297
Epoch: 5, Loss:0.0298
Epoch: 6, Loss:0.0283
Epoch: 7, Loss:0.0273
Epoch: 8, Loss:0.0265
Epoch: 9, Loss:0.0249
Epoch: 10, Loss:0.0265
</code></pre></div></div>

<p>ci sono dei problemi di visualizzazione rispetto al codice scritto da Python Engineer. Riguardando un po’ 
la struttura ho trovato la soluzione</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#print(len(outputs[0]) )
</span><span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">varie</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">u</span><span class="o">=</span><span class="n">varie</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
<span class="n">u</span><span class="o">.</span><span class="n">shape</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="c1">#plt.imshow(outputs[0][2].detach().reshape(-1,28*28).numpy())
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x1d0474e0730&gt;
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_105_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span> 
    <span class="n">imgs</span>  <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">recon</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">recon</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_106_0.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_106_1.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_106_2.png" alt="png" /></p>

<h2 id="con-convoluzioni">Con Convoluzioni</h2>
<p>qui sotto prendo un modello migliore, che sfrutti le convoluzioni e riesca a dare dei risultati piu’ precisi nella ricostruzione.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>  
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span> <span class="o">=</span><span class="n">mnist_data</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span> 
<span class="n">dataiter</span>  <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Autoencoder</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># N= 784 (28x28)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">),</span>    <span class="c1"># canali input, output, kernel, stride, padding | N 16 14 14
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">),</span>    <span class="c1"># N 32 7  7         
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span> <span class="mi">7</span> <span class="p">)</span>    <span class="c1"># N 64  1  1 (64 parametri in uscita)             
</span>        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>   <span class="c1"># N 32 7 7   
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>   <span class="c1"># N 16 14 14                 
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>   <span class="c1"># N 1 28 28                  
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>             
        <span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded</span>
    
<span class="c1"># nn.MaxPool2d  e  nn.MaxUnpool2d    
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span> <span class="p">()</span>   <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span> <span class="mf">1e-3</span> <span class="p">,</span> <span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>  
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">img</span>   <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>          <span class="c1">#.to(device)
</span>        <span class="c1">#recon = model(img)             #.to(device)
</span>        <span class="n">recon</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">recon</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        
        
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch+1}, Loss:{loss.item():.4f}'</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">epoch</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">recon</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 1, Loss:0.0097
Epoch: 2, Loss:0.0062
Epoch: 3, Loss:0.0047
Epoch: 4, Loss:0.0039
Epoch: 5, Loss:0.0036
Epoch: 6, Loss:0.0035
Epoch: 7, Loss:0.0028
Epoch: 8, Loss:0.0030
Epoch: 9, Loss:0.0031
Epoch: 10, Loss:0.0027
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span> 
    <span class="n">imgs</span>  <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">recon</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">recon</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_112_0.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_112_1.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_112_2.png" alt="png" /></p>

<h2 id="my-playground">My Playground</h2>

<ul>
  <li>nota che quando fai andare prima il modello lineare e poi quello convoluzionale due volte i parametri continuano ad aggiornarsi, quindi la seconda volta ottieni dei valori piu’ precisi!</li>
  <li>per esempio provo a mettere un parametro: min_par che indica il numero minimo di parametri (nell’esempio e’ 64, io provo 32, ecc</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>  
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span> <span class="o">=</span><span class="n">mnist_data</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span> 
<span class="n">dataiter</span>  <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_par</span><span class="o">=</span><span class="mi">4</span>

<span class="k">class</span> <span class="nc">Autoencoder</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># N= 784 (28x28)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">),</span>    <span class="c1"># canali input, output, kernel, stride, padding | N 16 14 14
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">),</span>    <span class="c1"># N 32 7  7         
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="n">min_par</span><span class="p">,</span> <span class="mi">7</span> <span class="p">)</span>    <span class="c1"># N 64  1  1 (64 parametri in uscita)             
</span>        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>      <span class="c1"># funzione di pytorch che fa andare uno dopo l'altro varie funzioni
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">min_par</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>   <span class="c1"># N 32 7 7   
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>   <span class="c1"># N 16 14 14                 
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(()),</span>             
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>   <span class="c1"># N 1 28 28                  
</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>             
        <span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded</span>
    
<span class="c1"># nn.MaxPool2d  e  nn.MaxUnpool2d    
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span> <span class="p">()</span>   <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span> <span class="mf">1e-3</span> <span class="p">,</span> <span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>  
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">img</span>   <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>          <span class="c1">#.to(device)
</span>        <span class="c1">#recon = model(img)             #.to(device)
</span>        <span class="n">recon</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">recon</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        
        
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch+1}, Loss:{loss.item():.4f}'</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">epoch</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">recon</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 1, Loss:0.0467
Epoch: 2, Loss:0.0362
Epoch: 3, Loss:0.0390
Epoch: 4, Loss:0.0337
Epoch: 5, Loss:0.0352
Epoch: 6, Loss:0.0327
Epoch: 7, Loss:0.0348
Epoch: 8, Loss:0.0359
Epoch: 9, Loss:0.0347
Epoch: 10, Loss:0.0356
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span> 
    <span class="n">imgs</span>  <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">recon</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">recon</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">9</span><span class="p">:</span><span class="k">break</span>               <span class="c1"># prendi solo le prime 9
</span>        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/images/posts/pytorch/output_118_0.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_118_1.png" alt="png" /></p>

<p><img src="/images/posts/pytorch/output_118_2.png" alt="png" /></p>

      </div>
    </div>

    <div class="tag-list">
      
      
      
      <a class="tag-chip" href="/tags#python_cap"><div class="chip z-depth-1">Python</div></a>
      
      
      
      <a class="tag-chip" href="/tags#deeplearning_cap"><div class="chip z-depth-1">DeepLearning</div></a>
      
      
      
      <a class="tag-chip" href="/tags#neural-nets_cap"><div class="chip z-depth-1">Neural Nets</div></a>
      
      
      
      <a class="tag-chip" href="/tags#cuda_cap"><div class="chip z-depth-1">Cuda</div></a>
      
    </div>
    
    		
			<script src="  https://unpkg.com/showdown/dist/showdown.min.js"></script>
<script>
const GH_API_URL = 'https://api.github.com/repos/4phycs/4phycs.github.io/issues/13/comments';

let request = new XMLHttpRequest();
request.open( 'GET', GH_API_URL, true );
request.onload = function() {
	if ( this.status >= 200 && this.status < 400 ) {
		let response = JSON.parse( this.response );

		for ( var i = 0; i < response.length; i++ ) {
			document.getElementById( 'gh-comments-list' ).appendChild( createCommentEl( response[ i ] ) );
		}

		if ( 0 === response.length ) {
			document.getElementById( 'no-comments-found' ).style.display = 'block';
		}
	} else {
		console.error( this );
	}
};

function createCommentEl( response ) {
	let user = document.createElement( 'a' );
	user.setAttribute( 'href', response.user.url.replace( 'api.github.com/users', 'github.com' ) );
	user.classList.add( 'user' );

	let userAvatar = document.createElement( 'img' );
	userAvatar.classList.add( 'avatar' );
	userAvatar.setAttribute( 'src', response.user.avatar_url );

	user.appendChild( userAvatar );

	let commentLink = document.createElement( 'a' );
	commentLink.setAttribute( 'href', response.html_url );
	commentLink.classList.add( 'comment-url' );
	commentLink.innerHTML = '#' + response.id + ' - ' + response.created_at;

	let commentContents = document.createElement( 'div' );
	commentContents.classList.add( 'comment-content' );
	commentContents.innerHTML = response.body;
	// Progressive enhancement.
	if ( window.showdown ) {
		let converter = new showdown.Converter();
		commentContents.innerHTML = converter.makeHtml( response.body );
	}

	let comment = document.createElement( 'li' );
	comment.setAttribute( 'data-created', response.created_at );
	comment.setAttribute( 'data-author-avatar', response.user.avatar_url );
	comment.setAttribute( 'data-user-url', response.user.url );

	comment.appendChild( user );
	comment.appendChild( commentContents );
	comment.appendChild( commentLink );

	return comment;
}
request.send();
</script>

<hr>

<div class="github-comments">
	<h2>Comments</h2>
	<ul id="gh-comments-list"></ul>
	<div class="buttonArea">
	  <a target="_blank" href="https://github.com/4phycs/4phycs.github.io/issues/13"class="button">Add comment (via Github)</a>
	</div>
</div>


		
 
  </div>
</main>

	<script src="/assets/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript">
  jQuery(document).ready(function($){
    // browser window scroll (in pixels) after which the "back to top" link is shown
    var offset = 300,
      //browser window scroll (in pixels) after which the "back to top" link opacity is reduced
      offset_opacity = 1200,
      //duration of the top scrolling animation (in ms)
      scroll_top_duration = 700,
      //grab the "back to top" link
      $back_to_top = $('.cd-top');

    //hide or show the "back to top" link
    $(window).scroll(function(){
      ( $(this).scrollTop() > offset ) ? $back_to_top.addClass('cd-is-visible') : $back_to_top.removeClass('cd-is-visible cd-fade-out');
      // if( $(this).scrollTop() > offset_opacity ) { 
      //  $back_to_top.addClass('cd-fade-out');
      // }
    });

    //smooth scroll to top
    $back_to_top.on('click', function(event){
      event.preventDefault();
      $('body,html').animate({
        scrollTop: 0 ,
        }, scroll_top_duration
      );
    });

  });
</script>
<style type="text/css">
.cd-top {
  display: inline-block;
  height: 50px;
  width: 50px;
  position: fixed;
  bottom: 2%;
  right: 2%;
  border-radius: 40px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
  /* image replacement properties */
  overflow: hidden;
  text-indent: 100%;
  white-space: nowrap;
  background: #bbb url(/images/cd-top-arrow.svg) no-repeat center 50%;
  visibility: hidden;
  opacity: 0;
  -webkit-transition: opacity .3s 0s, visibility 0s .3s;
  -moz-transition: opacity .3s 0s, visibility 0s .3s;
  transition: opacity .3s 0s, visibility 0s .3s;
}
.cd-top.cd-is-visible, .cd-top.cd-fade-out, .no-touch .cd-top:hover {
  -webkit-transition: opacity .3s 0s, visibility 0s 0s;
  -moz-transition: opacity .3s 0s, visibility 0s 0s;
  transition: opacity .3s 0s, visibility 0s 0s;
}
.cd-top.cd-is-visible {
  /* the button becomes visible */
  visibility: visible;
  opacity: 1;
}
.cd-top.cd-fade-out {
  /* if the user keeps scrolling down, the button is out of focus and becomes less visible */
  opacity: .5;
}
.no-touch .cd-top:hover {
  background-color: #e86256;
  opacity: 1;
}
</style>

<a href="#0" class="cd-top">Top</a>
	<footer class="page-footer light-blue accent-4">  
  <div class="footer-copyright">
    <div class="container text-white">
     <a href="">4Phycs</a> &#xA9; 2021 Inherited from <a href="https://shawnteoh.github.io/matjek/">MatJeck</a>.
    </div>
  </div>
</footer>

<script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>


  
    <script src="/assets/js/post.js"></script>
  





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
  (window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>

<script src="/assets/js/main.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158698892-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158698892-1');
</script>

</body>
</html>
