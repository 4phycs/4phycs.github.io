{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(introduzione tramite Hadoop streaming, notebook del CINECA Big DATA 2015 versione modificata da Paolo Avogadro)\n",
    "\n",
    "<center>\n",
    "<img src='http://www.opensourceforu.efytimes.com/wp-content/uploads/2012/03/hadoop-database-590x321.jpg'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** e' un paradigma completamente differente rispetto a MPI o openMP\n",
    "\n",
    "* MapReduce viene usato per risolvere un **sottoinsieme di problemi** parallelizzabili  \n",
    "    - funziona per aggirare il collo di bottiglia dovuto **all'ingestione dei dati da disco** (analisi grandi moli di dati) \n",
    "* Con il parallelismo tradizionale i **dati** vengono spostati verso il nodo computazionale \n",
    "    - Map/reduce fa il contrario, definisce quali nodi calcolano **in funzione della posizione** dei dati\n",
    "\n",
    "* i Dati vengono suddivisi (sharding) in piccoli pezzi (128 Mb per chunk per esempio) \n",
    "    - e sono stoccati sui vari nodi computazionali "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** e' un paradigma di programmazione che consente una scalabilita' **massiva** su migliaia di nodi \n",
    "\n",
    "La sua implementazione open source e' **Hadoop** (Hadoop 1.0 aveva un limite di 4000 nodi, ora questo limite e' stato  superato).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***HDFS*** e' una parte fondamentale per Hadoop (e per Spark, che ha scopi simili ma usa maggiormente la memoria invece che il disco)\n",
    "\n",
    "* ridistribuisce i pezzi di dato ( tra i vari nodi )\n",
    "* e' necessario per rendere efficenti le applicazioni di tipo mapreduce\n",
    "* gestisce i nodi\n",
    "* gestisce i dati in modo che siano sicuri (p.es. 3 repilche)\n",
    "* gestisce la comunicazione tra nodi (p.es. la fase di shuffle di Mapreduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# una guida di Hadoop: da Java verso Python\n",
    "\n",
    "1. un'occhiata veloce ad Hadoop Java\n",
    "2. Comprendere come lanciare una job Hadoop Streaming \n",
    "5. Simuliamo Hadoop streaming con le \"pipe\" | di bash\n",
    "6. Lanciamo Hadoop Streaming con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conteggio delle parole di un testo\n",
    "E' l'`Hello World`' di MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Questo e' l'esempio che e' stato portato anche nell'articolo originale di Mapreduce\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Come funziona\n",
    "* Il passo di **MAP**  prende il testo e lo converte in coppie **chiave/valore intermedie**\n",
    "    - Ogni parola del testo diventa una chiave\n",
    "    - TUTTE le chiavi (le parole del testo) hanno come valore 1\n",
    "\n",
    "\n",
    "* Il passo di **REDUCE** accorpera' le chiavi duplicate: \n",
    "    - Si sommano tutti i valori associati alla **medesima chiave** \n",
    "    - l'**Output** diventa qundi una lista di coppie chiavi valore, in cui tutte le chiavi sono diverse tra loro\n",
    "    - il valore corrispondente ad ogni chiave (parola) e' il numero di volte che la chiave stessa appare nel testo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='http://disco.readthedocs.org/en/latest/_images/map_shuffle_reduce.png' width=800>\n",
    "</center>\n",
    "\n",
    "### Funzione Map:\n",
    "processa i dati e genera un insieme (bag) di coppie **chiave/valore** intermedie.\n",
    "\n",
    "\n",
    "### Reduce function:\n",
    "unisce tutti i **valori intermedi** associati con la **medesima chiave intermedia**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nel dettaglio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supponiamo che il file sia: \n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La funzione di map legge ad una ad una le parole ed emette: \n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La fase di shuffle crea una lista di valori associati ad ogni chiave \n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione di reduce somma i numeri della lista per ogni chiave ed emette coppie (parola, conteggio)\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ecco come farlo in Java!\n",
    "(il linguaggio nativo di Hadoop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop ottenuto tramite le `pipe` di Unix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prendiamo il testo \"The prince\" di Machiavelli (in inglese, non perche' anglofoni,\n",
    "ma perche' ci sono meno problemi con gli accenti!).\n",
    " \n",
    "**Utile**: provate a fare la stessa cosa con il vostro notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/paoloD/Bicocca/CalcoloParalleloCorso/slidesHadoop/notebook2016\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 '\n",
      "      1 (OCT\n",
      "      1 *\n",
      "      1 *>\n",
      "      2 ;\n",
      "      1 ^943\n",
      "      1 1469\n",
      "      1 1527\n",
      "      1 1903,\n",
      "      1 1909\n",
      "      1 1921.\n",
      "      1 22,\n",
      "      1 3,\n",
      "      1 6\n",
      "      4 a\n",
      "      1 account\n",
      "      1 accuse\n",
      "      1 advantage\n",
      "      2 advice\n",
      "      1 aiid\n",
      "      2 all\n",
      "      1 always\n",
      "      1 an\n",
      "     12 and\n",
      "      1 AND\n",
      "      1 and,\n",
      "      1 appreciation\n",
      "      1 arts\n",
      "      3 as\n",
      "      1 Author's\n",
      "      1 avowed\n",
      "      2 been\n",
      "      1 BOMBAY\n",
      "      1 Born,\n",
      "      1 brief\n",
      "      1 but\n",
      "      4 by\n",
      "      3 BY\n",
      "      1 CALCUTTA\n",
      "      1 CAPETOWN\n",
      "      1 City\n",
      "      1 classical\n",
      "      1 Classics\n",
      "      1 Classics'\n",
      "      1 could\n",
      "      1 criticised\n",
      "      1 defamed\n",
      "      1 deliberate\n",
      "      1 did\n",
      "      1 Died,\n",
      "      1 EDINBURGH\n",
      "      1 edition\n",
      "      1 elsewhere\n",
      "      1 endeavour\n",
      "      1 England\n",
      "      2 English\n",
      "      1 ENGLISH\n",
      "      1 enslave\n",
      "      1 Essay\n",
      "      1 ever\n",
      "      1 fault.\n",
      "      1 first\n",
      "      3 Florence\n",
      "      1 Florentine\n",
      "      1 for\n",
      "      1 For\n",
      "      1 free\n",
      "      1 Garden\n",
      "      1 GLASGOW\n",
      "      1 good\n",
      "      1 government,\n",
      "      2 great\n",
      "      1 greatest\n",
      "      1 had\n",
      "      3 have\n",
      "      5 he\n",
      "      1 heeded,\n",
      "      1 helping\n",
      "      1 him\n",
      "      9 his\n",
      "      1 historian\n",
      "      1 how\n",
      "      1 HUMPHREY\n",
      "      2 I\n",
      "      1 ideas\n",
      "      1 imagined.\n",
      "      1 impossible\n",
      "     10 in\n",
      "      1 In\n",
      "      1 indefatigable\n",
      "      1 ingratitude\n",
      "      1 INTO\n",
      "      1 inventor\n",
      "      3 is\n",
      "      4 it\n",
      "      2 Italian\n",
      "      1 Italy\n",
      "      1 June\n",
      "      1 Letchworth.\n",
      "      1 liberal\n",
      "      1 life\n",
      "      1 likely\n",
      "      1 listened\n",
      "      1 LONDON\n",
      "      1 lost\n",
      "      1 LUIGI\n",
      "      1 Macaulay's\n",
      "      3 MACHIAVELLI\n",
      "      1 Machiavelli,\n",
      "      3 Machiavelli's\n",
      "      1 made\n",
      "      1 MADRAS\n",
      "      1 May\n",
      "      1 MELBOURNE\n",
      "      1 men\n",
      "      1 MILFORD\n",
      "      1 misinter-\n",
      "      1 most\n",
      "      1 must\n",
      "      1 name,\n",
      "      1 native\n",
      "      1 new\n",
      "      1 NEW\n",
      "      2 NICCOL6\n",
      "      1 NICCOLO\n",
      "      1 no\n",
      "      4 not\n",
      "     10 of\n",
      "      1 OF\n",
      "      1 office\n",
      "      2 on\n",
      "      1 only\n",
      "      1 opinions,\n",
      "      1 or\n",
      "      1 original.\n",
      "      1 our\n",
      "      1 OXFORD\n",
      "      1 people.\n",
      "      1 peoples\n",
      "      1 plainest\n",
      "      1 policy.\n",
      "      1 political\n",
      "      1 popular\n",
      "      1 PREFACE\n",
      "      1 Preface,\n",
      "      1 present\n",
      "      1 PRESS\n",
      "      1 Press,\n",
      "      1 preted\n",
      "      2 Prince\n",
      "      2 PRINCE\n",
      "      1 Printed\n",
      "      1 profiting\n",
      "      1 published\n",
      "      1 reader\n",
      "      1 reading\n",
      "      1 refer\n",
      "      1 remitting\n",
      "      1 reprinted\n",
      "      1 Republic\n",
      "      1 RICCI\n",
      "      1 ridiculous\n",
      "      1 scorned\n",
      "      1 Secretary\n",
      "      1 secure\n",
      "      1 see\n",
      "      1 show\n",
      "      1 slaves\n",
      "      1 spurned,\n",
      "      1 statesman\n",
      "      1 statesman.\n",
      "      1 successful\n",
      "      1 system\n",
      "      1 that\n",
      "      1 That\n",
      "     15 the\n",
      "      2 The\n",
      "      1 'The\n",
      "      2 THE\n",
      "      1 those\n",
      "      8 to\n",
      "      1 to,\n",
      "      1 TORONTO\n",
      "      1 TRANSLATED\n",
      "      1 translation\n",
      "      1 true\n",
      "      1 tyrant\n",
      "      1 tyrants\n",
      "      1 un-\n",
      "      1 undoubtedly\n",
      "      1 UNIVERSITY\n",
      "      1 unjust\n",
      "      1 very\n",
      "      5 was\n",
      "      1 way\n",
      "      1 welcome\n",
      "      2 were\n",
      "      1 What\n",
      "      1 which\n",
      "      1 while\n",
      "      1 who\n",
      "      1 who,\n",
      "      1 whole\n",
      "      1 wilfully\n",
      "      1 will\n",
      "      1 wisdom,\n",
      "      1 with\n",
      "      1 words\n",
      "      1 work\n",
      "      1 works\n",
      "      2 World's\n",
      "      1 worst\n",
      "      1 wrongfully\n",
      "      1 XLIII\n",
      "      1 Yet,\n",
      "      1 YORK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MapRedue sfruttando le  | di bash\n",
    " head -100 tmp/the_prince.txt  | awk ' {for (i=1;i<=NF;i++) {print $i}}' | sort | uniq -c\n",
    "# head -100 tmp/cancella.txt  | awk ' {for (i=1;i<=NF;i++) {print $i}}' | sort | uniq -c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosa abbiamo fatto?\n",
    "\n",
    "vediamo i vari comandi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1.  `head -100 tmp/the_prince.txt  `\n",
    "\n",
    "1.  `awk ' {for (i=1;i<NF;i++) {print $i}}' `\n",
    "\n",
    "1.  `sort` \n",
    "\n",
    "1.  `uniq -c`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**INPUT STREAM**\n",
    "`head -100 tmp/the_prince.txt`\n",
    "\n",
    "**MAPPER**\n",
    "`awk ' {for (i=1;i<NF;i++) {print $i}}'`\n",
    "\n",
    "**SHUFFLE**\n",
    "`sort`\n",
    "\n",
    "**REDUCER**\n",
    "`uniq -c`\n",
    "\n",
    "**OUTPUT STREAM**\n",
    "`<STDOUT>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "passo passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World's Classics \n",
      "\n",
      "\n",
      "\n",
      "XLIII \n",
      "THE PRINCE \n",
      "\n",
      "BY \n",
      "\n",
      "NICCOL6 MACHIAVELLI \n",
      "\n",
      "\n",
      "\n",
      "THE PRINCE \n",
      "\n",
      "\n",
      "\n",
      "BY \n",
      "\n",
      "\n",
      "\n",
      "NICCOL6 MACHIAVELLI \n",
      "\n",
      "\n",
      "\n",
      "TRANSLATED INTO ENGLISH BY \n",
      "\n",
      "LUIGI RICCI \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HUMPHREY MILFORD \n",
      "\n",
      "OXFORD UNIVERSITY PRESS \n",
      "\n",
      "LONDON EDINBURGH GLASGOW \n",
      "\n",
      "NEW YORK TORONTO MELBOURNE CAPETOWN \n",
      "\n",
      "BOMBAY CALCUTTA AND MADRAS \n",
      "\n",
      "\n",
      "\n",
      "NICCOLO MACHIAVELLI \n",
      "\n",
      "Born, Florence May 3, 1469 \n",
      "\n",
      "Died, Florence June 22, 1527 \n",
      "\n",
      "The present translation of Machiavelli's * Prince ' was \n",
      "first published in 'The World's Classics' in 1903, and \n",
      "reprinted in 1909 and 1921. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(OCT I 6 ^943 \n",
      "\n",
      "\n",
      "\n",
      "Printed in England by the Garden City Press, Letchworth. \n",
      "\n",
      "\n",
      "\n",
      "PREFACE \n",
      "\n",
      "*> \n",
      "\n",
      "OF all Machiavelli's works The Prince is undoubtedly \n",
      "the greatest ; aiid a new English edition of it is \n",
      "likely to he welcome to all those who have not the \n",
      "advantage of reading it in the classical Italian \n",
      "original. \n",
      "\n",
      "For a true appreciation of Machiavelli, impossible \n",
      "in a brief Preface, I must refer the English reader \n",
      "to Macaulay's Essay on the Italian historian and \n",
      "statesman. In it he will see how our Author's ideas \n",
      "and work were wrongfully and wilfully misinter- \n",
      "preted by the very men who, while profiting by his \n",
      "wisdom, have with great ingratitude criticised the \n",
      "statesman and defamed his name, as that of the \n",
      "inventor of the worst political system ever imagined. \n",
      "Yet, as his whole life was an indefatigable and un- \n",
      "remitting endeavour to secure for his native Florence \n",
      "a good and popular government, and as he lost his \n",
      "great office of Secretary to the Florentine Republic \n",
      "on account of his avowed liberal opinions, it is not \n",
      "only unjust but ridiculous to accuse him of helping \n",
      "tyrants to enslave the people. What he did was to \n",
      "show in the most deliberate and in the plainest way \n",
      "the arts by which free peoples were made slaves ; \n",
      "and, had his words of advice been always heeded, \n",
      "no tyrant in Italy or elsewhere could have been \n",
      "successful in his policy. That he was not listened \n",
      "to, and his advice scorned and spurned, was not \n",
      "Machiavelli's fault. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lo STREAMING del file  \n",
    "! head -100 tmp/the_prince.txt\n",
    "# prendo le prime 10 righe per vedere cosa succede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World's\n",
      "Classics\n",
      "XLIII\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MAPPING\n",
    " head -5 tmp/the_prince.txt  | awk ' {for (i=1;i<=NF;i++) {print $i}}' \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "(OCT\n",
      "*\n",
      "*>\n",
      ";\n",
      ";\n",
      "^943\n",
      "1469\n",
      "1527\n",
      "1903,\n",
      "1909\n",
      "1921.\n",
      "22,\n",
      "3,\n",
      "6\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "account\n",
      "accuse\n",
      "advantage\n",
      "advice\n",
      "advice\n",
      "aiid\n",
      "all\n",
      "all\n",
      "always\n",
      "an\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "AND\n",
      "and,\n",
      "appreciation\n",
      "arts\n",
      "as\n",
      "as\n",
      "as\n",
      "Author's\n",
      "avowed\n",
      "been\n",
      "been\n",
      "BOMBAY\n",
      "Born,\n",
      "brief\n",
      "but\n",
      "by\n",
      "by\n",
      "by\n",
      "by\n",
      "BY\n",
      "BY\n",
      "BY\n",
      "CALCUTTA\n",
      "CAPETOWN\n",
      "City\n",
      "classical\n",
      "Classics\n",
      "Classics'\n",
      "could\n",
      "criticised\n",
      "defamed\n",
      "deliberate\n",
      "did\n",
      "Died,\n",
      "EDINBURGH\n",
      "edition\n",
      "elsewhere\n",
      "endeavour\n",
      "England\n",
      "English\n",
      "English\n",
      "ENGLISH\n",
      "enslave\n",
      "Essay\n",
      "ever\n",
      "fault.\n",
      "first\n",
      "Florence\n",
      "Florence\n",
      "Florence\n",
      "Florentine\n",
      "for\n",
      "For\n",
      "free\n",
      "Garden\n",
      "GLASGOW\n",
      "good\n",
      "government,\n",
      "great\n",
      "great\n",
      "greatest\n",
      "had\n",
      "have\n",
      "have\n",
      "have\n",
      "he\n",
      "he\n",
      "he\n",
      "he\n",
      "he\n",
      "heeded,\n",
      "helping\n",
      "him\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "his\n",
      "historian\n",
      "how\n",
      "HUMPHREY\n",
      "I\n",
      "I\n",
      "ideas\n",
      "imagined.\n",
      "impossible\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "In\n",
      "indefatigable\n",
      "ingratitude\n",
      "INTO\n",
      "inventor\n",
      "is\n",
      "is\n",
      "is\n",
      "it\n",
      "it\n",
      "it\n",
      "it\n",
      "Italian\n",
      "Italian\n",
      "Italy\n",
      "June\n",
      "Letchworth.\n",
      "liberal\n",
      "life\n",
      "likely\n",
      "listened\n",
      "LONDON\n",
      "lost\n",
      "LUIGI\n",
      "Macaulay's\n",
      "MACHIAVELLI\n",
      "MACHIAVELLI\n",
      "MACHIAVELLI\n",
      "Machiavelli,\n",
      "Machiavelli's\n",
      "Machiavelli's\n",
      "Machiavelli's\n",
      "made\n",
      "MADRAS\n",
      "May\n",
      "MELBOURNE\n",
      "men\n",
      "MILFORD\n",
      "misinter-\n",
      "most\n",
      "must\n",
      "name,\n",
      "native\n",
      "new\n",
      "NEW\n",
      "NICCOL6\n",
      "NICCOL6\n",
      "NICCOLO\n",
      "no\n",
      "not\n",
      "not\n",
      "not\n",
      "not\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "OF\n",
      "office\n",
      "on\n",
      "on\n",
      "only\n",
      "opinions,\n",
      "or\n",
      "original.\n",
      "our\n",
      "OXFORD\n",
      "people.\n",
      "peoples\n",
      "plainest\n",
      "policy.\n",
      "political\n",
      "popular\n",
      "PREFACE\n",
      "Preface,\n",
      "present\n",
      "PRESS\n",
      "Press,\n",
      "preted\n",
      "Prince\n",
      "Prince\n",
      "PRINCE\n",
      "PRINCE\n",
      "Printed\n",
      "profiting\n",
      "published\n",
      "reader\n",
      "reading\n",
      "refer\n",
      "remitting\n",
      "reprinted\n",
      "Republic\n",
      "RICCI\n",
      "ridiculous\n",
      "scorned\n",
      "Secretary\n",
      "secure\n",
      "see\n",
      "show\n",
      "slaves\n",
      "spurned,\n",
      "statesman\n",
      "statesman.\n",
      "successful\n",
      "system\n",
      "that\n",
      "That\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "The\n",
      "The\n",
      "'The\n",
      "THE\n",
      "THE\n",
      "those\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to,\n",
      "TORONTO\n",
      "TRANSLATED\n",
      "translation\n",
      "true\n",
      "tyrant\n",
      "tyrants\n",
      "un-\n",
      "undoubtedly\n",
      "UNIVERSITY\n",
      "unjust\n",
      "very\n",
      "was\n",
      "was\n",
      "was\n",
      "was\n",
      "was\n",
      "way\n",
      "welcome\n",
      "were\n",
      "were\n",
      "What\n",
      "which\n",
      "while\n",
      "who\n",
      "who,\n",
      "whole\n",
      "wilfully\n",
      "will\n",
      "wisdom,\n",
      "with\n",
      "words\n",
      "work\n",
      "works\n",
      "World's\n",
      "World's\n",
      "worst\n",
      "wrongfully\n",
      "XLIII\n",
      "Yet,\n",
      "YORK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# SHUFFLING mettiamo le cose con lo stesso nome una dietro l'altra (in Mapreduce sono mandate allo stesso reducer)\n",
    "head -100 tmp/the_prince.txt  | awk ' {for (i=1;i<=NF;i++) {print $i}}' | sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 '\n",
      "      1 (OCT\n",
      "      1 *\n",
      "      1 *>\n",
      "      1 ^943\n",
      "      1 1469\n",
      "      1 1527\n",
      "      1 1903,\n",
      "      1 1909\n",
      "      1 1921.\n",
      "      1 22,\n",
      "      1 3,\n",
      "      1 6\n",
      "      1 account\n",
      "      1 accuse\n",
      "      1 advantage\n",
      "      1 aiid\n",
      "      1 always\n",
      "      1 an\n",
      "      1 AND\n",
      "      1 and,\n",
      "      1 appreciation\n",
      "      1 arts\n",
      "      1 Author's\n",
      "      1 avowed\n",
      "      1 BOMBAY\n",
      "      1 Born,\n",
      "      1 brief\n",
      "      1 but\n",
      "      1 CALCUTTA\n",
      "      1 CAPETOWN\n",
      "      1 City\n",
      "      1 classical\n",
      "      1 Classics\n",
      "      1 Classics'\n",
      "      1 could\n",
      "      1 criticised\n",
      "      1 defamed\n",
      "      1 deliberate\n",
      "      1 did\n",
      "      1 Died,\n",
      "      1 EDINBURGH\n",
      "      1 edition\n",
      "      1 elsewhere\n",
      "      1 endeavour\n",
      "      1 England\n",
      "      1 ENGLISH\n",
      "      1 enslave\n",
      "      1 Essay\n",
      "      1 ever\n",
      "      1 fault.\n",
      "      1 first\n",
      "      1 Florentine\n",
      "      1 for\n",
      "      1 For\n",
      "      1 free\n",
      "      1 Garden\n",
      "      1 GLASGOW\n",
      "      1 good\n",
      "      1 government,\n",
      "      1 greatest\n",
      "      1 had\n",
      "      1 heeded,\n",
      "      1 helping\n",
      "      1 him\n",
      "      1 historian\n",
      "      1 how\n",
      "      1 HUMPHREY\n",
      "      1 ideas\n",
      "      1 imagined.\n",
      "      1 impossible\n",
      "      1 In\n",
      "      1 indefatigable\n",
      "      1 ingratitude\n",
      "      1 INTO\n",
      "      1 inventor\n",
      "      1 Italy\n",
      "      1 June\n",
      "      1 Letchworth.\n",
      "      1 liberal\n",
      "      1 life\n",
      "      1 likely\n",
      "      1 listened\n",
      "      1 LONDON\n",
      "      1 lost\n",
      "      1 LUIGI\n",
      "      1 Macaulay's\n",
      "      1 Machiavelli,\n",
      "      1 made\n",
      "      1 MADRAS\n",
      "      1 May\n",
      "      1 MELBOURNE\n",
      "      1 men\n",
      "      1 MILFORD\n",
      "      1 misinter-\n",
      "      1 most\n",
      "      1 must\n",
      "      1 name,\n",
      "      1 native\n",
      "      1 new\n",
      "      1 NEW\n",
      "      1 NICCOLO\n",
      "      1 no\n",
      "      1 OF\n",
      "      1 office\n",
      "      1 only\n",
      "      1 opinions,\n",
      "      1 or\n",
      "      1 original.\n",
      "      1 our\n",
      "      1 OXFORD\n",
      "      1 people.\n",
      "      1 peoples\n",
      "      1 plainest\n",
      "      1 policy.\n",
      "      1 political\n",
      "      1 popular\n",
      "      1 PREFACE\n",
      "      1 Preface,\n",
      "      1 present\n",
      "      1 PRESS\n",
      "      1 Press,\n",
      "      1 preted\n",
      "      1 Printed\n",
      "      1 profiting\n",
      "      1 published\n",
      "      1 reader\n",
      "      1 reading\n",
      "      1 refer\n",
      "      1 remitting\n",
      "      1 reprinted\n",
      "      1 Republic\n",
      "      1 RICCI\n",
      "      1 ridiculous\n",
      "      1 scorned\n",
      "      1 Secretary\n",
      "      1 secure\n",
      "      1 see\n",
      "      1 show\n",
      "      1 slaves\n",
      "      1 spurned,\n",
      "      1 statesman\n",
      "      1 statesman.\n",
      "      1 successful\n",
      "      1 system\n",
      "      1 that\n",
      "      1 That\n",
      "      1 'The\n",
      "      1 those\n",
      "      1 to,\n",
      "      1 TORONTO\n",
      "      1 TRANSLATED\n",
      "      1 translation\n",
      "      1 true\n",
      "      1 tyrant\n",
      "      1 tyrants\n",
      "      1 un-\n",
      "      1 undoubtedly\n",
      "      1 UNIVERSITY\n",
      "      1 unjust\n",
      "      1 very\n",
      "      1 way\n",
      "      1 welcome\n",
      "      1 What\n",
      "      1 which\n",
      "      1 while\n",
      "      1 who\n",
      "      1 who,\n",
      "      1 whole\n",
      "      1 wilfully\n",
      "      1 will\n",
      "      1 wisdom,\n",
      "      1 with\n",
      "      1 words\n",
      "      1 work\n",
      "      1 works\n",
      "      1 worst\n",
      "      1 wrongfully\n",
      "      1 XLIII\n",
      "      1 Yet,\n",
      "      1 YORK\n",
      "      2 ;\n",
      "      2 advice\n",
      "      2 all\n",
      "      2 been\n",
      "      2 English\n",
      "      2 great\n",
      "      2 I\n",
      "      2 Italian\n",
      "      2 NICCOL6\n",
      "      2 on\n",
      "      2 Prince\n",
      "      2 PRINCE\n",
      "      2 The\n",
      "      2 THE\n",
      "      2 were\n",
      "      2 World's\n",
      "      3 as\n",
      "      3 BY\n",
      "      3 Florence\n",
      "      3 have\n",
      "      3 is\n",
      "      3 MACHIAVELLI\n",
      "      3 Machiavelli's\n",
      "      4 a\n",
      "      4 by\n",
      "      4 it\n",
      "      4 not\n",
      "      5 he\n",
      "      5 was\n",
      "      8 to\n",
      "      9 his\n",
      "     10 in\n",
      "     10 of\n",
      "     12 and\n",
      "     15 the\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# REDUCER\n",
    "head -100 tmp/the_prince.txt  | awk ' {for (i=1;i<=NF;i++) {print $i}}' |sort| uniq -c |sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Considerazioni riguardo all'uso delle *pipe* rispetto a MapReduce\n",
    "\n",
    "* i passi sono seriali\n",
    "* il file non e' distribuito su vari nodi...\n",
    "* ...perche' c'e' un singolo nodo!\n",
    "* singolo mapper\n",
    "* singolo reducer\n",
    "* e' possibile aggiungere un Combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop streaming\n",
    "### Concetti e meccanismi\n",
    "\n",
    "Hadoop streaming e' una utility che: \n",
    "\n",
    "* e' fornita in bundle con la distribuzione di Hadoop\n",
    "* consente di creare e fare girare dei job Map/Reduce  \n",
    "    - con **QUALSIASI**  eseguibile o script come mapper e/o reducer (non devo piu' scrivere i job map/reduce in JAVA)\n",
    "\n",
    "I passi da eseguire: \n",
    "\n",
    "* Creare un job Map/Reduce \n",
    "* Lanciare il job in un cluster \n",
    "* Monitorare i progressi del job fino a completamento\n",
    "* prendere i vari risultati nella dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Perche'?\n",
    "\n",
    "Uno degli aspetti meno piacevoli di Hadoop (dal punto di vista degli utilizzatori di HPC) e' che e' scritto in *Java*.\n",
    "\n",
    "* Java non e' stato originariamente pensato per essere un linguaggio high-performance\n",
    "* Gli esperti di dominio fanno fatica ad imparare Java \n",
    "\n",
    "Questo e' il motivo per cui Hadoop consente di scrivere le funzioni map/reduce in qualunque linguaggio e utilizzarle tramite  \n",
    "Hadoop Streaming \n",
    "\n",
    "* Si puo' qundi trasformare uno script in Python, Bash, Perl, etc in un job Hadoop\n",
    "* non si deve imparare Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce streaming con binari ed eseguibili\n",
    "\n",
    "* Gli eseguibili sono specificati per i mapper e i reducer (combiner)!\n",
    "    - ogni compito di un mapper gira come un processo indipendente  \n",
    "* L'input e' convertito in linee e passato allo `STDIN` del processo\n",
    "* Il mapper riceve lo  `STDOUT` del processo (Hadoop Streaming) \n",
    "    - ogni linea e' una coppia dove **chiave e valore** sono separate da un **TAB** e terminano con una **newline** \\n\n",
    "    - per esempio ”this is the key\\tvalue is the rest\\n”\n",
    "\n",
    "### ATTENZIONE!!! Se non c'e' il TAB all'interno della linea passata al mapper, allora l'intera linea e' considerata la chiave e il valore e' nullo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<big> \n",
    "Vediamo come usare:\n",
    "</big> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un esempio di chiamata ad Hadoop Streaming:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer /bin/wc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se usassi dei codici **python** inventati da me:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -files mapper.py,reducer.py\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "prima di lanciare un job Hadoop Streaming e' bene:\n",
    "\n",
    "* controllare che non ci siano errori negli script\n",
    "* controllare che mapper e reducer facciano esattamente il lavoro voluto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per esempio controllando il tutto con una piccola quanita' di dati,\n",
    "\n",
    "come con i comandi `cat` o `head`, con le **pipe**, visti precedentemente:\n",
    "\n",
    "```\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo un job che prende parti di un file biologico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@HD\tVN:1.4\tGO:none\tSO:coordinate\n",
      "@SQ\tSN:chrM\tLN:16571\n",
      "@SQ\tSN:chr1\tLN:249250621\n",
      "@SQ\tSN:chr2\tLN:243199373\n",
      "@SQ\tSN:chr3\tLN:198022430\n",
      "@SQ\tSN:chr4\tLN:191154276\n",
      "@SQ\tSN:chr5\tLN:180915260\n",
      "@SQ\tSN:chr6\tLN:171115067\n",
      "@SQ\tSN:chr7\tLN:159138663\n",
      "@SQ\tSN:chr8\tLN:146364022\n",
      "@SQ\tSN:chr9\tLN:141213431\n",
      "@SQ\tSN:chr10\tLN:135534747\n",
      "@SQ\tSN:chr11\tLN:135006516\n",
      "@SQ\tSN:chr12\tLN:133851895\n",
      "@SQ\tSN:chr13\tLN:115169878\n",
      "@SQ\tSN:chr14\tLN:107349540\n",
      "@SQ\tSN:chr15\tLN:102531392\n",
      "@SQ\tSN:chr16\tLN:90354753\n",
      "@SQ\tSN:chr17\tLN:81195210\n",
      "@SQ\tSN:chr18\tLN:78077248\n",
      "@SQ\tSN:chr19\tLN:59128983\n",
      "@SQ\tSN:chr20\tLN:63025520\n",
      "@SQ\tSN:chr21\tLN:48129895\n",
      "@SQ\tSN:chr22\tLN:51304566\n",
      "@SQ\tSN:chrX\tLN:155270560\n",
      "@SQ\tSN:chrY\tLN:59373566\n",
      "@SQ\tSN:chr1_gl000191_random\tLN:106433\n",
      "@SQ\tSN:chr1_gl000192_random\tLN:547496\n",
      "@SQ\tSN:chr4_ctg9_hap1\tLN:590426\n",
      "@SQ\tSN:chr4_gl000193_random\tLN:189789\n",
      "@SQ\tSN:chr4_gl000194_random\tLN:191469\n",
      "@SQ\tSN:chr6_apd_hap1\tLN:4622290\n",
      "@SQ\tSN:chr6_cox_hap2\tLN:4795371\n",
      "@SQ\tSN:chr6_dbb_hap3\tLN:4610396\n",
      "@SQ\tSN:chr6_mann_hap4\tLN:4683263\n",
      "@SQ\tSN:chr6_mcf_hap5\tLN:4833398\n",
      "@SQ\tSN:chr6_qbl_hap6\tLN:4611984\n",
      "@SQ\tSN:chr6_ssto_hap7\tLN:4928567\n",
      "@SQ\tSN:chr7_gl000195_random\tLN:182896\n",
      "@SQ\tSN:chr8_gl000196_random\tLN:38914\n",
      "@SQ\tSN:chr8_gl000197_random\tLN:37175\n",
      "@SQ\tSN:chr9_gl000198_random\tLN:90085\n",
      "@SQ\tSN:chr9_gl000199_random\tLN:169874\n",
      "@SQ\tSN:chr9_gl000200_random\tLN:187035\n",
      "@SQ\tSN:chr9_gl000201_random\tLN:36148\n",
      "@SQ\tSN:chr11_gl000202_random\tLN:40103\n",
      "@SQ\tSN:chr17_ctg5_hap1\tLN:1680828\n",
      "@SQ\tSN:chr17_gl000203_random\tLN:37498\n",
      "@SQ\tSN:chr17_gl000204_random\tLN:81310\n",
      "@SQ\tSN:chr17_gl000205_random\tLN:174588\n",
      "@SQ\tSN:chr17_gl000206_random\tLN:41001\n",
      "@SQ\tSN:chr18_gl000207_random\tLN:4262\n",
      "@SQ\tSN:chr19_gl000208_random\tLN:92689\n",
      "@SQ\tSN:chr19_gl000209_random\tLN:159169\n",
      "@SQ\tSN:chr21_gl000210_random\tLN:27682\n",
      "@SQ\tSN:chrUn_gl000211\tLN:166566\n",
      "@SQ\tSN:chrUn_gl000212\tLN:186858\n",
      "@SQ\tSN:chrUn_gl000213\tLN:164239\n",
      "@SQ\tSN:chrUn_gl000214\tLN:137718\n",
      "@SQ\tSN:chrUn_gl000215\tLN:172545\n",
      "@SQ\tSN:chrUn_gl000216\tLN:172294\n",
      "@SQ\tSN:chrUn_gl000217\tLN:172149\n",
      "@SQ\tSN:chrUn_gl000218\tLN:161147\n",
      "@SQ\tSN:chrUn_gl000219\tLN:179198\n",
      "@SQ\tSN:chrUn_gl000220\tLN:161802\n",
      "@SQ\tSN:chrUn_gl000221\tLN:155397\n",
      "@SQ\tSN:chrUn_gl000222\tLN:186861\n",
      "@SQ\tSN:chrUn_gl000223\tLN:180455\n",
      "@SQ\tSN:chrUn_gl000224\tLN:179693\n",
      "@SQ\tSN:chrUn_gl000225\tLN:211173\n",
      "@SQ\tSN:chrUn_gl000226\tLN:15008\n",
      "@SQ\tSN:chrUn_gl000227\tLN:128374\n",
      "@SQ\tSN:chrUn_gl000228\tLN:129120\n",
      "@SQ\tSN:chrUn_gl000229\tLN:19913\n",
      "@SQ\tSN:chrUn_gl000230\tLN:43691\n",
      "@SQ\tSN:chrUn_gl000231\tLN:27386\n",
      "@SQ\tSN:chrUn_gl000232\tLN:40652\n",
      "@SQ\tSN:chrUn_gl000233\tLN:45941\n",
      "@SQ\tSN:chrUn_gl000234\tLN:40531\n",
      "@SQ\tSN:chrUn_gl000235\tLN:34474\n",
      "@SQ\tSN:chrUn_gl000236\tLN:41934\n",
      "@SQ\tSN:chrUn_gl000237\tLN:45867\n",
      "@SQ\tSN:chrUn_gl000238\tLN:39939\n",
      "@SQ\tSN:chrUn_gl000239\tLN:33824\n",
      "@SQ\tSN:chrUn_gl000240\tLN:41933\n",
      "@SQ\tSN:chrUn_gl000241\tLN:42152\n",
      "@SQ\tSN:chrUn_gl000242\tLN:43523\n",
      "@SQ\tSN:chrUn_gl000243\tLN:43341\n",
      "@SQ\tSN:chrUn_gl000244\tLN:39929\n",
      "@SQ\tSN:chrUn_gl000245\tLN:36651\n",
      "@SQ\tSN:chrUn_gl000246\tLN:38154\n",
      "@SQ\tSN:chrUn_gl000247\tLN:36422\n",
      "@SQ\tSN:chrUn_gl000248\tLN:39786\n",
      "@SQ\tSN:chrUn_gl000249\tLN:38502\n",
      "@RG\tID:1\tPL:illumina\tPU:unk_barcode\tLB:flowcell\tSM:file1\n",
      "@PG\tID:bwa\tPN:bwa\tVN:0.6.2-r126\n",
      "@PG\tID:GATK IndelRealigner\tVN:2.5-2-gf57256b\tCL:knownAlleles=[(RodBinding name=knownAlleles source=/gpfs/scratch/project/EPIGEN/db/35/1000G_phase1.indels.hg19.vcf), (RodBinding name=knownAlleles2 source=/gpfs/scratch/project/EPIGEN/db/35/dbsnp_135.hg19.vcf)] targetIntervals=/gpfs/scratch/project/EPIGEN/db/35/hg19.intervals LODThresholdForCleaning=0.4 consensusDeterminationModel=KNOWNS_ONLY entropyThreshold=0.15 maxReadsInMemory=300000 maxIsizeForMovement=3000 maxPositionalMoveAllowed=200 maxConsensuses=30 maxReadsForConsensuses=120 maxReadsForRealignment=20000 noOriginalAlignmentTags=false nWayOut=null generate_nWayOut_md5s=false check_early=false noPGTag=false keepPGTags=false indelsFileForDebugging=null statisticsFileForDebugging=null SNPsFileForDebugging=null\n",
      "HSCAN:421:C47DAACXX:4:1202:4115:156718\t121\tchrM\t14\t37\t75M\t=\t14\t0\tTCACCCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTTCGTCTGGGGGGTGTGCACGCGATAGCAT\t<?9?@DDDDDDCDCCDDDDDDECCDFCHHBFHEGIIIEGJJJJIGGJIJJJIJJJJJJJIIJHHHHHFFFFFCCC\tX0:i:1\tX1:i:0\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:37\tXM:i:0\tXO:i:0\tXT:A:U\n",
      "HSCAN:421:C47DAACXX:4:1202:4115:156718\t181\tchrM\t14\t0\t*\t=\t14\t0\tGATAGACCTGTGATCCATCGTGATGTCTTATTTAAGGGGAACGTGTGGGCTATTTAGGCTTTATGACCCTGAAGT\tDFFEC=HFIGHJIGGJIJJJIIGHHJIIIHCGJJJJJJJJGIJJJIIHF<HCIIIIIJHJIJHHHHHFFFFFCC@\tRG:Z:1\n",
      "HSCAN:421:C47DAACXX:1:1307:12098:142207\t163\tchrM\t19\t60\t75M\t=\t40\t96\tCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTTCGTCTGGGGGGTGTGCACGCGATAGCATTGCGA\tCCCFFFFFHHHHHJJJJJIJJJJJIJJJJIIIJJIJFHIJJJJIHJJJIJIJD:<:@CEDDDDBDBDDDEDDDDB\tX0:i:1\tX1:i:0\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:37\tNM:i:0\tSM:i:37\tXM:i:0\tXO:i:0\tMQ:i:60\tXT:A:U\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# vediamo come e' fatto il file, a noi interessa contare quante volte ogni chrM appare nel file, le linee con @ sono da saltare\n",
    " head -100 ngs.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:         # prende tutti gli stdin \n",
    "    line = line.strip()        # .strip e' un metodo che toglie gli spazi all'inizio e alla fine \n",
    "    pieces = line.split('\\t')  # crea una lista dove ogni ingresso sono i pezzi separati da TAB della linea\n",
    "    print(pieces)              # stampa la lista delle parole... occhio che lo fa per ogni input (ottenuto in streaming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo il mapper completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py   \n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "    print(mychr,mystart.__str__())\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM:14\t1\n",
      "chrM:15\t1\n",
      "chrM:16\t1\n",
      "chrM:17\t1\n",
      "chrM:18\t1\n",
      "chrM:19\t1\n",
      "chrM:20\t1\n",
      "chrM:21\t1\n",
      "chrM:22\t1\n",
      "chrM:23\t1\n",
      "chrM:24\t1\n",
      "chrM:25\t1\n",
      "chrM:26\t1\n",
      "chrM:27\t1\n",
      "chrM:28\t1\n",
      "chrM:29\t1\n",
      "chrM:30\t1\n",
      "chrM:31\t1\n",
      "chrM:32\t1\n",
      "chrM:33\t1\n",
      "chrM:34\t1\n",
      "chrM:35\t1\n",
      "chrM:36\t1\n",
      "chrM:37\t1\n",
      "chrM:38\t1\n",
      "chrM:39\t1\n",
      "chrM:40\t1\n",
      "chrM:41\t1\n",
      "chrM:42\t1\n",
      "chrM:43\t1\n",
      "chrM:44\t1\n",
      "chrM:45\t1\n",
      "chrM:46\t1\n",
      "chrM:47\t1\n",
      "chrM:48\t1\n",
      "chrM:49\t1\n",
      "chrM:50\t1\n",
      "chrM:51\t1\n",
      "chrM:52\t1\n",
      "chrM:53\t1\n",
      "chrM:54\t1\n",
      "chrM:55\t1\n",
      "chrM:56\t1\n",
      "chrM:57\t1\n",
      "chrM:58\t1\n",
      "chrM:59\t1\n",
      "chrM:60\t1\n",
      "chrM:61\t1\n",
      "chrM:62\t1\n",
      "chrM:63\t1\n",
      "chrM:64\t1\n",
      "chrM:65\t1\n",
      "chrM:66\t1\n",
      "chrM:67\t1\n",
      "chrM:68\t1\n",
      "chrM:69\t1\n",
      "chrM:70\t1\n",
      "chrM:71\t1\n",
      "chrM:72\t1\n",
      "chrM:73\t1\n",
      "chrM:74\t1\n",
      "chrM:75\t1\n",
      "chrM:76\t1\n",
      "chrM:77\t1\n",
      "chrM:78\t1\n",
      "chrM:79\t1\n",
      "chrM:80\t1\n",
      "chrM:81\t1\n",
      "chrM:82\t1\n",
      "chrM:83\t1\n",
      "chrM:84\t1\n",
      "chrM:85\t1\n",
      "chrM:86\t1\n",
      "chrM:87\t1\n",
      "chrM:88\t1\n",
      "chrM:14\t1\n",
      "chrM:15\t1\n",
      "chrM:16\t1\n",
      "chrM:17\t1\n",
      "chrM:18\t1\n",
      "chrM:19\t1\n",
      "chrM:20\t1\n",
      "chrM:21\t1\n",
      "chrM:22\t1\n",
      "chrM:23\t1\n",
      "chrM:24\t1\n",
      "chrM:25\t1\n",
      "chrM:26\t1\n",
      "chrM:27\t1\n",
      "chrM:28\t1\n",
      "chrM:29\t1\n",
      "chrM:30\t1\n",
      "chrM:31\t1\n",
      "chrM:32\t1\n",
      "chrM:33\t1\n",
      "chrM:34\t1\n",
      "chrM:35\t1\n",
      "chrM:36\t1\n",
      "chrM:37\t1\n",
      "chrM:38\t1\n",
      "chrM:39\t1\n",
      "chrM:40\t1\n",
      "chrM:41\t1\n",
      "chrM:42\t1\n",
      "chrM:43\t1\n",
      "chrM:44\t1\n",
      "chrM:45\t1\n",
      "chrM:46\t1\n",
      "chrM:47\t1\n",
      "chrM:48\t1\n",
      "chrM:49\t1\n",
      "chrM:50\t1\n",
      "chrM:51\t1\n",
      "chrM:52\t1\n",
      "chrM:53\t1\n",
      "chrM:54\t1\n",
      "chrM:55\t1\n",
      "chrM:56\t1\n",
      "chrM:57\t1\n",
      "chrM:58\t1\n",
      "chrM:59\t1\n",
      "chrM:60\t1\n",
      "chrM:61\t1\n",
      "chrM:62\t1\n",
      "chrM:63\t1\n",
      "chrM:64\t1\n",
      "chrM:65\t1\n",
      "chrM:66\t1\n",
      "chrM:67\t1\n",
      "chrM:68\t1\n",
      "chrM:69\t1\n",
      "chrM:70\t1\n",
      "chrM:71\t1\n",
      "chrM:72\t1\n",
      "chrM:73\t1\n",
      "chrM:74\t1\n",
      "chrM:75\t1\n",
      "chrM:76\t1\n",
      "chrM:77\t1\n",
      "chrM:78\t1\n",
      "chrM:79\t1\n",
      "chrM:80\t1\n",
      "chrM:81\t1\n",
      "chrM:82\t1\n",
      "chrM:83\t1\n",
      "chrM:84\t1\n",
      "chrM:85\t1\n",
      "chrM:86\t1\n",
      "chrM:87\t1\n",
      "chrM:88\t1\n",
      "chrM:19\t1\n",
      "chrM:20\t1\n",
      "chrM:21\t1\n",
      "chrM:22\t1\n",
      "chrM:23\t1\n",
      "chrM:24\t1\n",
      "chrM:25\t1\n",
      "chrM:26\t1\n",
      "chrM:27\t1\n",
      "chrM:28\t1\n",
      "chrM:29\t1\n",
      "chrM:30\t1\n",
      "chrM:31\t1\n",
      "chrM:32\t1\n",
      "chrM:33\t1\n",
      "chrM:34\t1\n",
      "chrM:35\t1\n",
      "chrM:36\t1\n",
      "chrM:37\t1\n",
      "chrM:38\t1\n",
      "chrM:39\t1\n",
      "chrM:40\t1\n",
      "chrM:41\t1\n",
      "chrM:42\t1\n",
      "chrM:43\t1\n",
      "chrM:44\t1\n",
      "chrM:45\t1\n",
      "chrM:46\t1\n",
      "chrM:47\t1\n",
      "chrM:48\t1\n",
      "chrM:49\t1\n",
      "chrM:50\t1\n",
      "chrM:51\t1\n",
      "chrM:52\t1\n",
      "chrM:53\t1\n",
      "chrM:54\t1\n",
      "chrM:55\t1\n",
      "chrM:56\t1\n",
      "chrM:57\t1\n",
      "chrM:58\t1\n",
      "chrM:59\t1\n",
      "chrM:60\t1\n",
      "chrM:61\t1\n",
      "chrM:62\t1\n",
      "chrM:63\t1\n",
      "chrM:64\t1\n",
      "chrM:65\t1\n",
      "chrM:66\t1\n",
      "chrM:67\t1\n",
      "chrM:68\t1\n",
      "chrM:69\t1\n",
      "chrM:70\t1\n",
      "chrM:71\t1\n",
      "chrM:72\t1\n",
      "chrM:73\t1\n",
      "chrM:74\t1\n",
      "chrM:75\t1\n",
      "chrM:76\t1\n",
      "chrM:77\t1\n",
      "chrM:78\t1\n",
      "chrM:79\t1\n",
      "chrM:80\t1\n",
      "chrM:81\t1\n",
      "chrM:82\t1\n",
      "chrM:83\t1\n",
      "chrM:84\t1\n",
      "chrM:85\t1\n",
      "chrM:86\t1\n",
      "chrM:87\t1\n",
      "chrM:88\t1\n",
      "chrM:89\t1\n",
      "chrM:90\t1\n",
      "chrM:91\t1\n",
      "chrM:92\t1\n",
      "chrM:93\t1\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 ngs.sam | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ora produciamo un mapper che emetta coppie formattate per il reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":          # evita queste linee\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)    # separa i pezzi\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM:84\t1\n",
      "chrM:85\t1\n",
      "chrM:86\t1\n",
      "chrM:87\t1\n",
      "chrM:88\t1\n",
      "chrM:89\t1\n",
      "chrM:90\t1\n",
      "chrM:91\t1\n",
      "chrM:92\t1\n",
      "chrM:93\t1\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 ngs.sam | python mapper.py | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shuffle \n",
    "\n",
    "<br><big>\n",
    "Il bello di Hadoop e' che ci sono molti lati del lavoro che sono completamente trasparenti al programmatore: HDFS si occupa di tutto. Infatti si vuole proprio evitare che il programmatore diventi matto a gestire la comunicazione tra nodi.\n",
    "</big>\n",
    "\n",
    "In questo esempio:\n",
    "\n",
    "* L'output dei mapper e' trasformato e distribuito ai reduce\n",
    "* Tutte le coppie key/value sono **ordinate correttamente** prima di essere mandate ai riduttori\n",
    "* Coppie con la stessa chiave vengono mandate allo stesso riduttore\n",
    "* Se si incontra una chiave che e' differente da quella precedente: \n",
    "    - *siamo sicuri che la precedente non apparira' piu' (perche' le abbiamo ordinate)\n",
    "* Se tutte le chiavi sono identiche\n",
    "    - un solo reducer viene usato, non si guadagna in parallelismo\n",
    "    - se questo succede e' importane costruire chiavi intermedie differenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"              # stringa vuota\n",
    "value_count = 1              # conteggio di valore = 1 \n",
    "for line in sys.stdin:       # prende tutte le linee dello standard input: in ogni riga ci sono \"key TAB value\\n\"\n",
    "    value, count = line.strip().split(TAB)       # il primo elemento e' value, il secondo e' count\n",
    "    # if this is the first iteration\n",
    "    if not last_value:            # ricorda che vuoto = falso\n",
    "        last_value = value        # definisci last value \n",
    "    # if they're the same, log it\n",
    "    if value == last_value:       # se il valore attuale e' uguale a quello vecchio \n",
    "        value_count += int(count) # aumenta il numero di conteggi di count\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))  # .join attacca last_value e value_cont tramite tab\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM:14\t3\n",
      "chrM:15\t2\n",
      "chrM:16\t2\n",
      "chrM:17\t2\n",
      "chrM:18\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m0.219s\n",
      "user\t0m0.076s\n",
      "sys\t0m0.356s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# needs ~ 5 seconds for running      # qui il comando sort fa il lavoro di shuffling\n",
    "time head -n 100 ngs.sam | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Esercizio\n",
    "\n",
    "Scrivere un mapper e un reducer in python per raggruppare le parole del \"Principe\" basate sulla\n",
    "loro lunghezza.\n",
    "\n",
    "Trovare quante parole ci sono piu' lunghe di 10 lettere o piu' corte di 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# verso il vero Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>\n",
    "Un codice python che funge con le pipe **dovrebbe fungere** anche con Hadoop Streaming\n",
    "</big>\n",
    "\n",
    "* Per fare si' che questo succeda abbiamo bisogno di usare dei file che sono all'interno dell' Hadoop File System\n",
    "* Nell' HDFS troveremo anche i log del job tracker \n",
    "* Proviamo ad usare bash scripting nel notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I comandi HDFS interagiscono con l'Hadoop file system con la sintassi:\n",
    "```\n",
    "hdfs dfs -command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`command` sono come i comandi della bash\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "hadoop dfs -mkdir hdfs:///dir\n",
    "hadoop dfs -put file_on_host hdfs:///path/to/file\n",
    "hadoop dfs -ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<big>\n",
    "Hadoop Streaming ha bisogno di “file binari” (o eseguibili)\n",
    "</big>\n",
    "\n",
    "Bisogna specificare l'interprete all'inizio dello script:\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "```\n",
    "\n",
    "E resi eseguibili con:\n",
    "```\n",
    "chmod +x hs*.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Environment does not have key: HADOOP_STREAMING\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Launch streaming\n",
    "hadoop jar $HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Preprocess with HDFS\n",
    "hdfs dfs -rm -r -f myinput\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "file=\"/tmp/ngs.sam\"\n",
    "hdfs dfs -put $file myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lancio finale tramite il comando bash per usare Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# A real Hadoop Streaming run\n",
    "time hadoop jar $HADOOP_STREAMING \\\n",
    "    -D mapreduce.job.mapper=12 -D mapreduce.job.reducers=4  \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input myinput -output myoutput \\\n",
    "    -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "**Nota 1**:\n",
    "\n",
    "Se il comando necessita di minuti per andare a completamento si puo' vedere cosa succede nel Hadoop JobTracker:\n",
    "http://localhost:8088/cluster\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Note 2**:\n",
    "\n",
    "Questo comando non puo' essere eseguito dalla versione cloud dei notebook, perche' la porta non e' aperta\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming e' **difficile da debuggare**.\n",
    "Proprio come il vero Java Hadoop.\n",
    "\n",
    "Quando si commettono degli errori  di setup, si possono ricevere degli errori che dipendono invece dalla Java virtual machine.\n",
    "\n",
    "Prima di googlare questo \"stacktrace\" e' meglio controllare che:\n",
    "\n",
    "* i file Python  (mapper e reducer) esistano\n",
    "* Che siano all'interno della main bash anche come una  **lista di file**\n",
    "* Che siano eseguibili e che contengano la prima linea come hasbang \n",
    "* Che la dir di input esista sull' HDFS\n",
    "* Che i file all'interno della directory di input non siano corrotti \n",
    "    - e.g. bad decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT: Check directory\n",
    "! hdfs dfs -ls myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT:  Copy file and go see it\n",
    "! rm -rf hs.*.txt && hdfs dfs -get myoutput/part-00000 hs.out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Esercizio\n",
    "\n",
    "Contare simboli (qualunque cosa che non sia una lettera dell'alfabeto) in un file di testo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercizio\n",
    "\n",
    "Fare girare i Mapper/Reducer di python con Hadoop Streaming.\n",
    "\n",
    "(*buona fortuna*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pensieri finali su Hadoop streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Consente di scrivere job MapReduce jobs in altri linguaggi\n",
    "* O persino degli eseguibili\n",
    "* Veloce\n",
    "* piu' semplice che in Java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quando e' particolarmente utile?\n",
    "\n",
    "* Quando lo sviluppatore non conosce Java \n",
    "* per scrivere Mapper/Reducer con linguaggi di scripting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Svantaggi\n",
    "\n",
    "* forza gli script in una Java VM\n",
    "    - Anche se quasi non ha overhead\n",
    "* I programmi/eseguibili devono prendere l'input dallo STDIN \n",
    "    - e produrre output nello STDOUT\n",
    "* Ci sono restrizioni riguardo ai formati di input/output \n",
    "    - non si occupa della preparazione dei file di input/output o delle directory \n",
    "    - l'utente deve quindi usare i propri comandi hdfs  per gestire i dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limiti\n",
    "\n",
    "* Non e' un modo di lavorare Pythonico\n",
    "\n",
    "(non e' stato scritto specificatamente per python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riassunto\n",
    "\n",
    "* Hadoop streaming fa girare Hadoop quasi nel modo classico \n",
    "* Wrappa qualsiadi eseguibile(e script)\n",
    "* Puo' girare su un cluster usando  non-interactive, all-encapsulated job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
