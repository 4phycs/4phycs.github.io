{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Un MapReduce Pythonico...\n",
    "(versione del Jupiter Notebook del corso di Bid Data Analysis del cineca riadattata da Paolo Avogadro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tramite Hadoop Streaming si puo' fare MapReduce con Python (o altri linguaggi), non si e' piu' legati a Java\n",
    "\n",
    "Possiamo migliorare ancora le cose?\n",
    "\n",
    "Che tipo di problemi possono esserci?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Con Hadoop Streaming dobbiamo lavorare direttamente con l'HDFS\n",
    "* spostare i file di input nell' HDFS\n",
    "* richiamare i file di output\n",
    "* questo aumenta la possibilita' di fare errori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il debugging puo' essere difficile\n",
    "\n",
    "* logs via jobtracker\n",
    "* gli errori sono spesso correlati con Java stacktrace, piuttosto che con il job MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' necessario scrivere (almeno) 2 file differenti\n",
    " * uno per il mapper\n",
    " * uno per il reducer\n",
    "\n",
    "tutto questo non e' un **modulo** di Python (che sono oggetti semplici e puliti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le **classi** di Python sono pero' uno strumento ideale che:\n",
    "\n",
    " * potrebbe rendere tutti questi passi piu' semplici \n",
    " * uniformare un job MapReduce in modo che sia chiaro e facilmente debuggabile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MRJob \n",
    "A more pythonic MapReduce library from Yelp\n",
    "\n",
    "<img src='https://avatars1.githubusercontent.com/u/49071?v=3&s=400' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> “Easiest route to Python programs that run on Hadoop”\n",
    "\n",
    "si installa facilmente (supponendo di avere Python) tramite: \n",
    "```bash\n",
    "pip install mrjob\n",
    "```\n",
    "\n",
    "**Running modes** ( sono le modalita' in cui si puo' usare MrJob)\n",
    "* Consentono di testare il codice in locale senza dover installare Hadoop \n",
    "* Oppure fare girare i job su un cluster a propria scelta, per esempio:\n",
    "    - Si integra con Amazon **Elastic MapReduce** (EMR)\n",
    "    - il codice e' lo stesso  in locale, su Hadoop o EMR\n",
    "    - rende sempice fare girare un job sul cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Come funziona MRJob?\n",
    "\n",
    "* E' un modulo di Python scritto **on top of Hadoop Streaming (HS)**\n",
    "    - HS jar apre un sottoprocesso del nostro codice \n",
    "    - gli manda l' input via stdin\n",
    "    - raccoglie i risultati dallo stdout\n",
    "* funge da wrapper per HDFS per il pre e post processing (se c'e' hadoop)\n",
    "* costruisce una interfaccia consistente tra tutti gli ambienti supportati \n",
    "* in modo automatico serializza/deserializza il flusso dei dati da ogni task \n",
    "    - e.g. JSON: json.loads() and json.dumps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Facciamo un po' di prove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un  ** job ** e' definito da una classe presa dal pacchetto MRJob \n",
    "\n",
    "* Contiene metodi che definscono i passi (**step**) di un job di Hadoop\n",
    "* Uno “step” consiste di un  **mapper**, un **combiner**,  un **reducer**. \n",
    "* Tutti questi step sono opzionali, ma ne serve almeno uno (altrimenti non stiamo facendo nulla...)\n",
    "\n",
    "Proviamo a vedere un template di questa classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class myjob(MRJob):                      # eredito la classe da MRjob\n",
    "    def mioMapper(self, _, line):        # l'argomento  _  e'perche' di spesso la chiave d'ingresso non serve\n",
    "        pass                             # senza il pass il template da errore\n",
    "    def mioCombiner(self, key, values):  # combiner esegue sul mapper, prima che lo shuffle sia stato effettuato\n",
    "        pass\n",
    "    def mioReducer(self, key, values):   # reducer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount\n",
    "contiamo quante parole ci sono in un testo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mapper\n",
    "\n",
    "Il metodo mapper() prende una chiave e un valore come argomenti\n",
    "\n",
    "```python\n",
    "    def mioMapper(self, _, line):  \n",
    "        pass\n",
    "```\n",
    "\n",
    "* In questo esempio la **key e' ignorata** (in quanto, per esempio, non ci serve sapere quale sia il numero di riga) mentre la riga stessa e' il valore, questo per la forma di default del protocollo\n",
    "\n",
    "* Questa  \"funzione\" restituisce (**yield**)  un numero di coppie chiave valore che dipende dall'input (perche' il termine \"funzione\" e' tra virgolette? vedremo poi)\n",
    "* ricordiamo quindi che un mapper associa ad **un** unico input, **molte** chiavi-valori come output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Yield?\n",
    "\n",
    "**Attenzione**: `yield` e' DIVERSO da `return`: \n",
    "\n",
    " `yield` trasforma una funzione in una **generator function**:\n",
    "\n",
    "- se chiamo una funzione piu' volte, ogni volta tutti i suoi argomenti saranno \"reinizializzati\" e non ci sara' \"memoria\" della chiamata precedente\n",
    "- Se e' presente uno **yield** l'oggetto creato e' un **generator**, non una *funzione* \n",
    "- **yield** a differenza di **return** \"congela\" il generator, la prossima volta che verra' chiamato ripartira' dal punto dove e' stato chiamato lo **yield**.\n",
    "- un **generator** ha, di default, varie proprieta', per esempio il metodo `next()`\n",
    "- Utile: https://docs.python.org/3/tutorial/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esempio di generatore che funziona come un iterator\n",
    "def mioGen():       #  e' praticamente identico ad una funzione.\n",
    "     i=0\n",
    "     while i<10:\n",
    "        i+=1\n",
    "        yield i     # qui invece e' diverso...\n",
    "        \n",
    "a=mioGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosa succede se cerco di chiamare un'altra volta 'a'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "a=mioGen()          \n",
    "i=0\n",
    "while i < 10:\n",
    "    i+=1\n",
    "    print(next(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(next(mioGen()))    # qui non e' ancora stato instanziato\n",
    "print(next(mioGen()))\n",
    "b=mioGen()               # questo crea una istanza del generatore\n",
    "c=b                      # questo crea un nuovo NOME dell'istanza del generatore  \n",
    "d=mioGen()               # questo crea una istanza differente del generatore\n",
    "print(next(b))\n",
    "print(next(b))\n",
    "print(next(c))\n",
    "print(next(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# quando uso un loop in Python su un oggetto-contenitore, uso la funzione iter() \n",
    "#e trasformo quel contenitore in un iteratore\n",
    "stringa = 'abcd' \n",
    "#print(next(stringa))\n",
    "Istringa = iter(stringa)    # crea istanza dell'iteratore\n",
    "print(next(Istringa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "print(next(Istringa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occhio, un generatore puo' esaurirsi! a quel punto non si puo' piu' chiamare il metodo next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3f6e2eea332d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Proviamo ora a creare un generatore che emetta coppie di oggetti,\n",
    "e chiamiamo tutti i valori possibili, come facciamo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 valore\n",
      "2 valore\n",
      "3 valore\n",
      "4 valore\n",
      "5 valore\n",
      "6 valore\n",
      "7 valore\n",
      "8 valore\n",
      "9 valore\n"
     ]
    }
   ],
   "source": [
    "def mygen():\n",
    "    for i in range(1,10):\n",
    "        yield i, 'valore'          # restituisce 2 oggetti\n",
    "\n",
    "for chiave, valore in mygen():     # chiamo ripetutamente la stessa 'funzione' \n",
    "    print (chiave, valore)         # che da risultati diversi ogni volta: e' un generatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [1,2,3]\n",
    "type(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list_iterator"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iter(lista))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ora a scrivere un generatore tale che, data una stringa con delle parole, separate da spazi, \"emetta\" coppie formate da: \n",
    " * primo oggetto emesso, la parola stessa\n",
    " * secondo oggetto emesso 1  (ho trovato, in questo punto la data parola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nel 1\n",
      "mezzo 1\n",
      "del 1\n",
      "cammin 1\n",
      "i 1\n",
      "i 1\n",
      "nel 1\n",
      "nel 1\n",
      "nel 1\n",
      "nel 1\n"
     ]
    }
   ],
   "source": [
    "linea = 'nel mezzo del cammin i i nel nel nel nel'\n",
    "def emetti(testo):\n",
    "    for parola in testo.split():\n",
    "        yield parola, 1\n",
    "\n",
    "for chiave, valore in emetti(linea):  # chiamo ripetutamente la stessa funzione\n",
    "    print (chiave, valore)            # che con yield e' un generatore e da risultati diversi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducer\n",
    "\n",
    "Il metodo usato come **riduttore()** prende come argomenti (a parte self):\n",
    "* una **chiave** \n",
    "* un **iteratore** di valori  (ricordiamo che solo oggetti con la medesima chiave intermedia arrivano ai rducer)\n",
    "\n",
    "```python\n",
    "    def mioRiduttore(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* Anche in questo caso il numero di coppie chiave-valore emesse dipende da come e' fatto il reducer stesso e da quali argomenti riceve\n",
    "\n",
    "* Per esempio somma i valori di ogni chiave    \n",
    "\n",
    "* anche il reducer puo' usare yield \n",
    "\n",
    "* un **iteratore** assomiglia ad un generatore ma e' un po' piu' generale: basta prendere un oggetto composto (p.es. una lista) e usare ```iter(miaLista) ```. A questo punto potremo chiamare \"chiamare\" i vari componenti tramite il metodo ```next()``` come nel caso dei generatori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scriviamo il nostro primo job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: mydir=.\n",
      "env: myinput=testo.txt\n",
      "env: myscript=./wordcount.py\n",
      "env: myoutput=./out.txt\n",
      "env: mylog=./out.log\n"
     ]
    }
   ],
   "source": [
    "# Configuriamo alcune variabili d'ambiente di  LINUX\n",
    "\n",
    "mydir = \".\"                \n",
    "%env mydir = $mydir\n",
    "myinput = \"testo.txt\"                # nome del testo che vogliamo processare\n",
    "%env myinput = $myinput    \n",
    "myscript = mydir + \"/wordcount.py\"   # script che voglio usare\n",
    "%env myscript = $myscript\n",
    "\n",
    "%system mkdir -p $mydir           \n",
    "%env myoutput $mydir/out.txt\n",
    "%env mylog $mydir/out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scriviamo il file con il job **wordcount**, in modo che:\n",
    "* prenda un file di testo in ingresso\n",
    "* emetta come risultato il numero di occorrenze di ciascuna parola (considerando parole, per cui l'unica differenza e' la maiuscola come parole identiche) \n",
    "* conti il numero di volte che una parola appare all'interno del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mioScript.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mioScript.py             \n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class MRWordCount(MRJob):             # eredita classe da MRJob \n",
    "    \"\"\" Wordcount with MapReduce in a pythonic way\"\"\"\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split():    # prende la linea, la divide in parole separate da spazi e ne crea una lista\n",
    "             yield word.lower(), 1      # prende le parole della lista e con .lower rende tutto lowercase; restituisce nello stdout una parola e l' 1\n",
    "\n",
    "                # Qui MRjob sta facendo del lavoro: prende tutte i valori intermedi e li mette insieme\n",
    "                # in un iteratore che viene passato al reducer!\n",
    "                \n",
    "                \n",
    "    def reducer(self, word, occurrences):  # prende lo stdin, le parole emesse dal mapper\n",
    "        yield word, sum(occurrences)       # ricorda che il modo in cui e' ricevuto e'  (parola, (1,1,1,1,1)) \n",
    "\n",
    "if __name__ == '__main__':                 # queste due linee sono fondamentali \n",
    "    MRWordCount.run()                      # senza non succede nulla, e' solo una classe: cosi' esegue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nota\n",
    "tramite MRJob, e il fatto che si usino dei generatori,\n",
    "non dobbiamo controllare all'intero del **reducer** quando il **valore** sta cambiando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I/O\n",
    "Si pue' passare l'input da processare come STDIN, pero' in ogni caso mrjob lo mettera' in un *file* all'inizio.\n",
    "Esempio di come dare in pasto un file da processare:\n",
    "\n",
    "```bash\n",
    "$ python my_job.py < input.txt\n",
    "```\n",
    "Si possono passare molteplici file di input usando il carattere -, per esempio\n",
    "\n",
    "```bash\n",
    "$ python my_job.py input1.txt input2.txt - < input3.txt\n",
    "```\n",
    "Di default l'output va nello stdout\n",
    "\n",
    "```bash\n",
    "$ python my_job.py input.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Eseguiamo il nostro primo job di MrJob\n",
    "! python $myscript $myinput 1> $myoutput 2> $mylog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat $myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "more $myoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a fare un singolo job che, dato un file di testo:\n",
    "    - calcoli il numero di parole\n",
    "    - calcoli il numero di righe\n",
    "    - calcoli il numero di lettere\n",
    "\n",
    "Tutto questo deve avvenire con un singolo mapper e un singolo reducer, come posso farlo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summerize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summerize.py  \n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class riassunto(MRJob):\n",
    "    def mapper(self,_,linea):\n",
    "        yield 'caratteri', len(linea)\n",
    "        yield 'parole', len(linea.split())\n",
    "        yield 'righe', 1\n",
    "        \n",
    "    def reducer(self, chiave, valore):\n",
    "        yield chiave, sum(valore)      \n",
    "\n",
    "if __name__ == '__main__':              \n",
    "    riassunto.run()                    # metodo .run() e' necessario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python summerize.py lungo.txt  1> riassunto.txt 2> errori.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Esempio di un **template** (pensato per essere eseguibile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" MapReduce easily with Python \"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def reducer(self, key, line):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [                    # versione ok, quella senza importare MRStep e' DEPRECATA\n",
    "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job.run()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "con MRStep si possono gestire i vari passi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Due parole sui combiner e i reducer\n",
    "* Un combiner, assomiglia ad un reducer, ma prende i dati direttamente da **un solo** mapper\n",
    "* puo' essere pensato come un modo per ridurre la quantita' di dati che deve poi circolare tra i nodi\n",
    "* solo alcuni tipi di funzioni possono essere usate in un combiner\n",
    "   - funzioni commutative e  associative\n",
    "   - deve emettere coppie chive valore che vadano bene per il reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio\n",
    "\n",
    "Costruiamo un job <Mapreduce> tale che \n",
    "- ci dica quante volte ognuna delle vocali appare in un testo\n",
    "- ci dica il numero TOTALE di vocali nel testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cercaVocali.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cercaVocali.py  \n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" MapReduce easily with Python \"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class riassuntoVocali(MRJob):\n",
    "    def mapper(self,_,linea):\n",
    "        vocali=('a','i','u','e','o')   # tupla di vocali\n",
    "        for lettera in range (0,len(linea)):  # gira sulle lettere della linea\n",
    "            if linea[lettera].lower() in vocali:\n",
    "                yield linea[lettera].lower(), 1\n",
    "                yield 'numero vocali',1\n",
    "                \n",
    "    def reducer(self, chiave, valore):\n",
    "        yield chiave, sum(valore)      \n",
    "\n",
    "if __name__ == '__main__':              \n",
    "    riassuntoVocali.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python cercaVocali.py testo.txt  1> riassunto.txt 2> errori.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running Modes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Di default MRJob gira in locale, in un singolo processo di Python.\n",
    "Il modo in cui gira puo' essere cambiato tramite i **running modes**,\n",
    "scelti attraverso l'opzione `-r/--runner`\n",
    "\n",
    "\n",
    "```bash\n",
    "-r inline, -r local, -r hadoop, or -r emr\n",
    "```\n",
    "\n",
    "E' inoltre possibile usare l'opzione `--verbose` per mostrare tutti i passi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "quindi basta usare  `-r hadoop` per fare girare il job su hadoop\n",
    "\n",
    "<small>Nota: La *magic* `capture` e' un altro modo che possiamo usare per catturare l'output.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Protocolli / Protocols\n",
    "\n",
    "http://mrjob.readthedocs.org/en/latest/guides/writing-mrjobs.html#protocols\n",
    "\n",
    "L'idea e' che si possano gestire in modo automatico molte caratteristiche,\n",
    "in particolare di formato (sia in ingresso che in uscita, ed eventualmente \n",
    "tra uno step e il successivo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MRJob ha molte utili opzioni\n",
    "\n",
    "anche se alcune di loro possono risultare costose (in termini di tempistiche) quando si fa un job pesante\n",
    "\n",
    "Per esempio e' possibile definire come sono i formati tra un passo e l'altro di MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MyMRJob(mrjob.job.MRJob):\n",
    "\n",
    "    # these are the defaults\n",
    "    INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol     # protocollo standard di input che legge le righe\n",
    "    INTERNAL_PROTOCOL = mrjob.protocol.JSONProtocol      # protocollo di comunicazione tra, per es. mapper e reducer \n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol        # protocollo di uscita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile protocol.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import PickleProtocol   # importiamo i protocolli pickle\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    # Optimization on internal protocols\n",
    "    INTERNAL_PROTOCOL = PickleProtocol      # i dati sono trasferiti compattati tra mapper e reducer\n",
    "    OUTPUT_PROTOCOL = PickleProtocol        # i dati in uscita sono comunque in formato pickle\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split( ):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "                \n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! python protocol.py /data/lectures/data/books/twolines.txt 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gestire i vari step di un processo MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "mrjob puo' essere configurato per fare diversi step \n",
    "\n",
    "Per ogni step e' necessario specificare quale parte deve essere eseguita \n",
    "e il metodo da usare all'interno della classe, per esempio: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def steps(self):\n",
    "    return [\n",
    "        MRStep(\n",
    "            mapper=self.mapper_get_words,          # indico quale e' il nome del mapper\n",
    "            combiner=self.combiner_count_words,    # combiner  \n",
    "            reducer=self.reducer_count_words),     # reducer\n",
    "        MRStep(reducer=self.reducer_find_max_word) # riduttore finale\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a vedere un processo mapreduce che ci fa vedere le parole piu' frequenti.\n",
    "Pensiamo ad un processo a piu' passi in cui il risultato finale sia la parola piu' frequente all'interno di un testo\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parolaFrequente.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parolaFrequente.py\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4262\t\"e\"\n"
     ]
    }
   ],
   "source": [
    "!python parolaFrequente.py divina.txt 2> problemi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Riassunto di MrJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " [documentazione della versione piu' recente](http://mrjob.readthedocs.org/en/latest/).\n",
    "\n",
    "(fatta bene senza diventare troppo complicata)\n",
    "\n",
    "Esistono molte opzioni e possibilita' avanzate da scoprire.\n",
    "\n",
    "<small>\n",
    "Nota : Gli sviluppatori possono aiutare: https://github.com/Yelp/mrjob/issues/1142\n",
    "</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ad majora:\n",
    "\n",
    "* con MrJob non e' possibile connettersi in **remoto** ad un cluster Hadoop. \n",
    "    - (questo perche' Hadoop non consente che si inviino dei job dall'esterno (classi o eseguibili).\n",
    "* d'altra parte ad EMR su Amazon si puo' accedere anche dal proprio laptop.\n",
    "    - Amazon ha creato il seguente [boto api](http://boto.readthedocs.org/en/latest/ref/emr.html) proprio per questo problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Vantaggi **\n",
    "* Piu' documentazione che qualunque altra libreria o framework\n",
    "* Basta un codice contenente una singola classe per ogni job MapReduce\n",
    "    * Map e Reduce sono dei singoli metodi\n",
    "    * Molto pulito e semplice\n",
    "* Configurazioni avanzate\n",
    "    * steps multipli\n",
    "    * i comandi da linea di comando possono essere gestiti all'interno del codice Python(vedere docs)\n",
    "* Facile la gestione di input/output \n",
    "    * Non e' necessario copiare i dati con HDFS\n",
    "* Gli errori e i warning vanno nell'outpur dello script \n",
    "* **Si puo' cambiare l'ambiente (HS, AWS,...)senza cambiare il codice changing the code...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Svantaggi**\n",
    "\n",
    "* il livello di accesso alle Hadoop APIs e' inferiore (rispetto HS per esmpio)\n",
    "    - Notevoli concorrenti Dumbo e Pydoop\n",
    "    - Altre librerie possono essere piu' veloci  can be faster if you use typedbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/features.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "da: http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*nota finale*: \n",
    "> Open source is a great thing\n",
    "\n",
    "I forum possono aiutare\n",
    "https://github.com/Yelp/mrjob/issues/1142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine Capitolo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
